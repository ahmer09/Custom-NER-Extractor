But will the current bans last? Dave Gershgorn Dec 18Â·4 min read OneZeroâ€™s General Intelligence is a roundup of the most important artificial intelligence and facial recognition news of the week. The facial recognition industry has been quietly working alongside law enforcement, military organizations, and private companies for years, leveraging 40-year old partnerships originally centered around fingerprint databases. But in 2020, the industry faced an unexpected reckoning. February brought an explosive New York Times report on Clearview AI, a facial recognition company that had scraped billions of images from social media to create an all-encompassing database, and quietly gave it to thousands of police departments and companies across the world. A ubiquitous facial recognition database weaponizing public social media profiles to create tools for law enforcement and private security was a splash of cold water for those who hadnâ€™t yet understood what the facial recognition industry had become. Now the technology has never been easier to implement by corporations and law enforcement, and the ramifications of the technology have never been more serious. Across the United States, bans on police use of facial recognition had started in 2019 with San Francisco. But in 2020 they proliferated, thanks to activist pressure from organizations like the ACLU, Fight For The Future, and AI Now. Portland, Oregon, instituted one of the strictest facial recognition laws in the world, and by far the most restrictive in the country, which prohibits private businesses from using the technology on their own premises. This means businesses like Walmart wouldnâ€™t be able to use facial recognition to track customers in their stores. â€œThis is really about making sure that we are prioritizing our most vulnerable community members and community members of color,â€ Portland City Council Commissioner Jo Ann Hardesty said in Kate Kayeâ€™s report earlier this year for OneZero. Portland, Maine, also banned police use of the technology this year, alongside bans and temporary moratoriums in Madison, Wisconsin; Springfield, Massachusetts; Cambridge, Massachusetts; Jackson, Mississippi, and Boston. Even Los Angeles placed a limited facial recognition ban on its police department, which prevented it from using facial recognition that sources images from social media, like Clearview AI. The LAPD will still use countywide facial recognition software that references booking images, according to BuzzFeed News. â€œThis is really about making sure that we are prioritizing our most vulnerable community members and community members of color.â€ While Massachusetts legislators passed a statewide ban on police use of the technology, Governor Charlie Baker has reportedly refused to sign the bill into law, claiming that facial recognition is a necessary tool for police. This comes after players of the Boston Celtics basketball team wrote an open letter to Baker, asking him to sign the bill. â€œBakerâ€™s rejection is deeply troubling because this technology supercharges racial profiling by police and has resulted in the wrongful arrests of innocent people,â€ the players wrote. The players are referring to a case in Detroit, where Robert Julian-Borchak Williams was wrongfully arrested due to an error in the policeâ€™s facial recognition system. The federal government has also taken notice of the rise of facial recognition, with renewed pressure from Congress to legislate the technology. Democrats introduced a bill that would ban federal agencies from using facial recognition, and withhold federal funds from municipalities unless they also enacted such bans. This bill came weeks before an updated report from the Government Accountability Office, which recommended facial recognition legislation to safeguard citizensâ€™ privacy. Even tech giants like IBM, Amazon, and Microsoft, which are some of the largest creators and proponents of the technology in the consumer space, have stopped selling the technology to police, and, in IBMâ€™s case, canceled facial recognition development altogether. Legal challenges filed in 2020 against Clearview AI and the use of facial recognition in schools are scheduled to be decided in 2021, according to a report from CNETâ€™s Alfred Ng. Clearview AI is facing lawsuits in Illinois, which has the countryâ€™s best protections against companies secretly collecting fingerprints, DNA, and facial images. These legal battles could set precedent for how facial recognition companies are allowed to collect data and act as a call to action for other states as well. All of this movement to regulate and restrict the use of facial recognition flies in the face of the enormous corporate and governmental interest in being able to identify people in public and private spaces. Marketers have long pitched facial recognition and other identifying technologies as the ultimate tool for understanding customers. And police departments often sing the praises of being able to pick out the identity of a suspect from a photo or video. That makes the activistsâ€™ progress in 2020 all the more impressive, and the stakes all the higher. In 2021, Amazonâ€™s self-imposed facial recognition moratorium is set to expire, opening up questions as to whether it will actually commit to stop its work with facial recognition or continue working with police, as it already has with its Ring smart doorbell. Microsoft and IBM have also sent letters to President-elect Biden, asking for national facial recognition laws. Those laws could curb the use of the technology â€” or add cover for the use of facial recognition by law enforcement in ways we can hardly imagine.
Ferris Argyle 9 hours agoÂ·3 min read AI Platform Notebooks provide managed JupyterLab notebook instances, a familiar tool to experiment, develop, and deploy models into production. The missing piece is training models for production; this typically runs much longer than the initial experimentation or the production prediction. The interactive Notebook in which you did your experimentation is probably not the right place for this: browser sessions time out, connections are lost, image rendering freezes, and the instance isnâ€™t right-sized for training. This article focuses on two approaches to running your training in a headless manner: AI Hub JupyterLab VM: Papermill AI Platform Training AI Hub JupyterLab VM: Papermill Papermill is a tool for parameterizing and executing Jupyter Notebooks in a number of different ways including by spinning up a VM. One of these is submitting the notebook from the command line of the Notebookâ€™s VM: this takes advantage of the custom VM configuration which youâ€™ve already configured with your AI libraries, while bypassing the potential freezing issues associated with rendering the results in the Notebook UI. The Notebook VM also includes Papermill pre-installed, so no additional configuration is required. If youâ€™d like to run on a separate VM with Papermill, this TPU-based Next 2019 talk walks through that scenario. You can access the command line either from the Cloud Console -> Compute Engine -> VM instances -> SSH option for the VM, or from within the Notebook itself; the former excludes the Notebook UI as a potential concern. You can open the SSH session in a browser window, or in a dedicated client for additional state persistence. Parameterization You can parameterize your Notebook to pass variable information such as the number of epochs into the notebook at run time. Set default parameters in a cell tagged with the â€œparameterâ€ keyword and refer to these in the training or other steps; these are then over-ridden by whatever you specify on the command line. Eg. Command Line for a Notebook with an epochs parameter: Execution When you run the command, you specify an output Notebook; all intermediate results, logging, etc. are recorded here. Include the log-output flag to write notebook output to stderr (ie. the terminal window) Note that if the terminal is closed, this terminates the SSH session, and by default any Notebooks running within it, including headless notebooks. Continue execution if the terminal is closed To continue execution even if the terminal session is closed, use a command sequence like one of the following: These commands show the process id (pid) and redirect stderr and stdout to nohup.out or ~/output.txt respectively. You can monitor the running process from from the output notebook, as well as from a new terminal session using the following command: Note that redirecting stdout and stderr throws the following error; this doesnâ€™t affect Notebook execution or logging: AttributeError: â€˜NoneTypeâ€™ object has no attribute â€˜send_multipartâ€™ More background here: Running commands which survive terminal close Difference between nohup, disown, and & AI Platform Training This approach is based on creating a Docker image with your libraries layered on top of a base Tensorflow image, then deploying that to AI Platform Training; you can also run the image locally for testing. This has the advantage of being able to size your training environment differently from your Notebook environment, and natively close the terminal session without affecting job execution; you can track job execution in the cloud console; logs are written to Stackdriver. Store the model weights in Google Cloud Storage as the link between the training and prediction steps; note that you can also save the whole model instead of just the weights; the chosen approach will depend on your use case. Parameterization As before, you can parameterize the execution, eg. with the number of epochs. Sample script Execution Step through the steps outlined above on your VM, or run it all as a bash script. You will need to create your own GCS bucket in the project in which youâ€™re running the VM; this GCS bucket can be on a different VM if you configure service accounts appropriately. If you choose to run the optional local training test step, your VM will need a GPU. Checkpoints â€‹To support better recovery from failures during a job, you can implementâ€‹ checkpoints; this Keras sample demonstrates checkpointing on epochs.
Ivana Kotorchevikj 5 days agoÂ·11 min read Artificial intelligence has shown great potential and a promising course in the past few years. But the unexpected developments in this turbulent year enforced rapid digital transformation that crammed years of innovation into several months. The disruption made companies transform and adopt new technologies â€” including Artificial Intelligence â€” in the blink of an eye, and AI experts predict that not only this trend wonâ€™t slow down, but it will accelerate even more. People well versed in the AI domain forecast that it will experience vast expansion and development in important and meaningful ways in 2021 and beyond. Compared to previous years when companies could experiment with Artificial Intelligence, ML, automation, 2020 proved to be the year to dive in headfirst, states Forbes. While 2020 saw the accelerated deployment of many unique platforms, researches and tools that leverage Artificial Intelligence to a great extent, 2021 is expected to deliver so much more and is fittingly named the golden year of AI implementation, ThinkML emphasises. All in all, 2021 will be marked by AI growth characterised by the launch of many AI applications, delivering insights, efficiency and cost-effectiveness in the digital era. And while itâ€™s going to continue permeating all aspects of our lives, these are areas where AI is predicted to have the biggest impact in 2021. Hyperautomation Hyperautomation found its place in Gartner Top Strategic Technology Trends for 2021. The idea behind it is that anything that can be automated in an organisation should be automated. Businesses will aim to automate as many business and IT processes as possible, relying on Artificial Intelligence. It is mainly driven by legacy business processes that are not streamlined, creating expensive and extensive issues. Yet, the pandemic accelerated the implementation of hyperautomation, which is also known as â€œdigital process automationâ€ and â€œintelligent process automationâ€, relates CRN. AI-powered solutions help to redefine business processing and functioning strategies providing real-time updates. Content intelligent technologies, coupled with AI-supported processes, will provide the digital workforce with exceptional capabilities. The trend emerges as the digital business need for efficiency, speed and democratisation is ever more growing. Organisations that donâ€™t focus on efficiency, efficacy and business agility will be left behind, warns Gartner. AIOps â€” AI Engineering A Gartner research finds that only 53% of AI projects successfully make it from prototype to full production. IT leaders and CIOs are struggling to deploy new Artificial Intelligence systems because they lack the tools to create and manage a production-grade AI pipeline and cannot manage system maintainability, scalability and governance. This is why AI initiatives are left somewhere on a shelf and fail to generate returns on investment. Itâ€™s becoming more evident that succeeding with AI production means turning to AI engineering, AIOps, a discipline focused on the governance and life cycle management of a wide range of operationalised AI and decision models, such as machine learning or knowledge graphs, relates Gartner. Companies are starting to grasp that a robust AI engineering strategy is the key to facilitate the performance, scalability, interpretability and reliability of Artificial Intelligence models while delivering the full value of AI investments. AI engineering- AIOps rests on three core pillars â€” DataOps, ModelOps and DevOps. Keeping up with the fast-paced IT Operations landscape, where AI is at the forefront, AIOps aims to increase productivity by adding automation to the workflow so people can focus their attention on solving problems that canâ€™t be automated. Convergence of AI and IoT into AIoT AIoT is not a new term, it has been a long time coming, but we will finally see seamless confluence between AI and IoT, AI experts state. In the past, IoT was able to monitor and collect data from smart devices. Adding AI into the equation will enable AIoT systems to take actions, complete tasks and learn based on the data without human involvement, like in locking doors, redirecting traffic, reducing in-home air temperatures, turning off lights, etc. The trend is already starting to dominate the tech industry and can be found in many valuable use cases. Research shows that 28% of all homes in the US could become smart homes by 2021. But it will not stop here. AIoT will also find its place in smart buildings, cities and retail environments, where data will be utilised to provide greater security, improved sustainability practices, improved customer experience, real-time offer optimisation, etc. AIoT will also redefine the future of industrial automation, and is set to lead the Industry 4.0 revolution. It will undoubtedly impact almost every industry verticals, including automotive, aviation, finance, healthcare, manufacturing and supply chain, reports Forbes. Focus on Total Cost of AI Ownership A few years ago, only the big tech companies by the likes of Google, Facebook and Amazon could offer to develop AI-based solutions. Today, technology is becoming more accessible to the vast majority of companies. As a result of developing various and affordable ML tools, frameworks and libraries, the number of Artificial Intelligence-based solutions and projects powered by Machine Learning is growing exponentially. Captivated by the promises and benefits of AI, enterprises are eager to implement Artificial Intelligence technologies to automate and optimise their business processes. More recently, companiesâ€™ focus started to shift from only seeing the benefits to how much would it cost in total to build and maintain an AI-solution. Despite being affordable, AI-based solutions are far from cheap. Several factors and aspects affect the overall cost and need to be considered before head-diving into an AI project. The question of the total AI ownership will be even more in the limelight in 2021, where Artificial Intelligence projects are concerned. Edge AI Weâ€™ve mentioned above how the convergence of AI and IoT is disrupting the tech industry and proving effective in various areas. However, thereâ€™s a persistent need to further speed up the decision-making process, securely analyse data, avoid uncontrolled latency, and control network connections. To solve these challenges, AI is moving to the edge, i.e. giving rise to Edge AI, which combines Edge Computing and Artificial Intelligence into one system. Edge AI enables data processing generated by a smart device locally, or on the server near the device using AI algorithms and edge computing. Edge AI devices apply to smart speakers, smartphones, laptops, robots, self-driven cars, drones, surveillance cameras that use video analytics, etc. The perk is that the device does not need to be connected to the Internet to process such data and makes mission-critical decisions in real-time, in a matter of milliseconds. Rising Ethics Questions around AI Technologies Several events in 2020 created a ripple effect in terms of the ethical questions with Artificial Intelligence. The U.S. protests against racial bias in the middle of the year sparked the debate about the unregulated use of facial recognition technology. As a response, leading tech companies like Microsoft, IBM and Amazon, announced they would limit the use of their AI-based facial recognition technology by police departments. The problem is complex and daunting. â€œItâ€™s just deeply chilling to think that engaging in protected activity, exercising your most fundamental rights, could end you up in a police databaseâ€, said Albert Fox Cahn, executive director of the Surveillance Technology Oversight Project for CBS News. Besides the controversial impact of facial recognition on human rights, numerous instances pointed at the searing ethical questions enveloping AI. One such example is Googleâ€™s forcing out one of its ethical AI co-lead, Timnit Gebru, after a protracted disagreement over the release of a research paper which lays out the risks of large language models. The paper Gebru co-authored summarises the environmental and financial costs, the massive undocumented data, inscrutable models, the research opportunity costs and the dangers of AI misinformation. Although many ethical questions are yet to be discussed and resolved, the Google example just opened the Pandora box of Artificial Intelligence ethics, and weâ€™ll see the effects of it well in 2021 and beyond. Explainable AI Explainable AI or XAI helps data scientists create trust across everyone from stakeholders, regulators, and end-users and better understand their models and offer confidence bounds on performance, relates KDNuggets. But although some experts are hopeful that model explainability will become mainstream in 2021, others suggest not to expect explainable AI to become mainstream in 2021. Most AI models are not designed for transparency, let alone explainability, and even designers of the models cannot explain how an AI arrives at a result, said Lion Jye Su, Principal Analyst at ABI Research for Digiconasia. Leading Artificial Intelligence vendors such as Google have offered development tools and frameworks around explainable AI, but Su states that AI built based on these solutions is not mature enough for mass commercialisation. Federated Learning The field of machine learning has had explosive advancements in the past few years. One example of this is the iteration of machine learning called â€œ federated learning â€œ introduced by Google AI in 2017. Federated learning enables edge devices to use machine learning without centralising data and privacy by default. Data generated at the edge with billions of phones and IoT devices can be harnessed to enable better products and smarter models. These locally trained models are then sent from the devices back to the central server where they are aggregated, i.e. averaging weights. And then a single consolidated and improved global model is sent back to the devices, explains ODSC â€” Open Data Science. In 2021, weâ€™ll see increasing development of Federated Learning with Differentiated Privacy as a solution to enable Artificial Intelligence to scale, especially in healthcare, insurance and banking services, experts predict. The War for AI Talent and the Growing AI Divide The war for talent has fiercely intensified in the last several years. Considering the AI and ML solutions are becoming widely adopted by companies, top-notch AI talent is running in short supply. Headhunting for the needed talent is that much challenging with the leading tech companies in the scene competing for the best AI capabilities. The war for Artificial Intelligence talent is ongoing and will become more persistent throughout 2021 and beyond. Peter Bendor-Samuel states in Forbes, that going into the new year, even if discretionary spending increases, most companies still have the challenge of a recession mindset and cutbacks. Still, they may face an emerging set of opportunities with a scarcity of talent in the market. He also adds that the companies that overly cut back on their capacity in 2020 will find themselves critically short of talent in 2021. Nonetheless, companies are really starting to grasp this reality, and they are designing strategies to attract much-needed talent on the market. Parallel to the war for Artificial Intelligence talent is the â€œAI divideâ€- the gap between those who can access computers and the internet and those who cannot, as Manish Bahl describes in Cognizant. He states that as Artificial Intelligence becomes more mainstream and develops, winners and losers will be determined by what level of access they have to AI technologies and how they leverage them. Manish outlines examples that will manifest the AI divide. Whether the AI divide will grow depends on how we bridge it in terms of infrastructure, skills, knowledge gaps, research capacities, and data availability. But one thing is certain; those companies unable to leverage AI will find themselves on the wrong side of the divide and have a significant disadvantage to those who can leverage Artificial Intelligence. Greater progress in NLP beyond GPT-3 OpenAI recently published a paper presenting an important upgrade to their well-established language model, GPT-3 (GPT stands for Generative Pre-trained Transformer). However, impressive it might be, does it really enable companies to practically use NLP on their data? GPT-3 has two limitations. The first is its limited context window. It cannot â€˜seeâ€™ beyond this window, which is roughly 500â€“1000 words long. The second limitation, which all the autoregressive language models trained using likelihood loss seem to share, is that they tend to eventually fall into repetitive loops of gibberish when generating free-form completions. However, there has been a breakthrough in the field of NLP about making predictions with unstructured data (text, audio, video), a trend that is expected to go strong in 2021 and beyond. Recently, Google launched the MT5 model, a multilingual transformer model, which Anders Arpteg, Head of Research at Peltarion, explained at the latest AIAW Podcast episode. The MT5 model enables the use of text for hundreds of languages at once, with state of the art performance. What the model can do besides text classification, question answering and text similarity, is that it can generate text, just like GPT-3. But in contrast, the companies can train the MT5 model on their own data and make it work for their proper use case â€” which is not possible with GPT-3. Minimal amount of annotated data for unsupervised learning Another breakthrough in ML is the focus of minimising the amount of annotated data in unsupervised learning, removing the need for thousands or millions of data points to make progress, stated Anders Arpteg. Another method for reducing the cost of data annotation is active learning, where the algorithm suggests which examples are worth annotating, and the user annotates only those selected samples. If only the minimal amount of data is annotated, every company can start using these techniques avoiding the slow, costly ways of annotating data. This breakthrough will enable more widely adopted use of NLP techniques in 2021, suggests Anders. AI as a Service As the demand for Artificial Intelligence capabilities is growing daily, and at the same time, talent is getting scarce, more data scientists are deciding to open their companies that offer AI as a service. AI as a Service combines the SaaS business model and AI services, helping bring Artificial Intelligence to the masses without a heavy price tag. Forbes already saw it taking off on a larger scale in 2020, and itâ€™s expected to accelerate well into 2021. Itâ€s not only because AI brings efficiency and optimisation to processes, but predominantly because the future of digital transformation itself depends on the democratisation of the latest technologies, relates Daniel Newman, a Forbes contributor. Some of the reasons for the proliferation of AI as a Service Newman outlined are the demand for AI is growing, the ecosystem needed to optimise AI is growing, companies are catching up quickly, and SMBs need the advanced infrastructure at minimal cost, which AI as a Service offers, to compete against tech giants. The most notable advantage of AI as a Service is that it allows companies to use AIâ€™s power without having to procure the expertise to manage it when thereâ€™s a shortage of AI experts and ever-increasing competition in the marketplace. â€¦ Do you see any other AI trend emerging in 2021? Share your thoughts in the comments below. Originally published at https://read.hyperight.com on December 21, 2020.
Data science trends in retail for the post-Covid world Fabrizio Fantini Dec 14Â·6 min read 2020 was a difficult year for every business. The coronavirus changed every aspect of our operations. One of the biggest trends in data science caused by the Covid-19 pandemic was an acceleration of the integration of data scientists and data-driven analysis into greater company operations. Data was vital to survive and thrive in the chaos caused by Covid. This trend was even more pronounced in the retail sector. As one of the hardest-hit sectors this year, retail turned to data science for solutions. Because of the increasing importance of data science â€” or business science as I think itâ€™s more appropriately considered â€” in retail, data-driven trends in the sector matter to every data scientist. Itâ€™s not just for those of us focused on the supply chain, pricing or other retail-adjacent applications. Data science trends in retail portend trends for the field as a whole in the post-Covid world. Here are five critical trends that I forecast will dominate data science in retail in 2021 â€” and that every data scientist must therefore be prepared to face head-on in the new year. 1. Automated analytics and analysis Almost every business uses Big Data. Itâ€™s become a basic necessity of modern retail. Next step: automation. Over the next year, more companies than ever will automate their analytics, both in terms of data collection and analysis. Data quality doesnâ€™t matter unless you can leverage its insights autonomously. Business science must be autonomous, not just data-driven, to be effective. Iâ€™ve been a tireless promoter of autonomous systems for analysis for years. Still, Iâ€™m not the only person who thinks that 2021 will be the tipping point for automation in the retail sector. McKinsey has noted it as a key trend in both consumer goods and fashion for 2021. Automation is the future for retail â€” which means that data scientists will need to shift their focus to automated models and analytics. 2. Maturation of AI With the rapid acceleration of AI adoption that weâ€™ve seen over the past year, 2021 is likely to become a watershed for AI maturation. That is, more models will deliver the optimal outcomes desired from their implementation. In terms of Gartnerâ€™s model for AI Maturity, many businesses will hit the final two stages. AI application in many organizations will finally tip into systematic or even transformational. For data scientists, AI maturation is exciting, as they can act like real business scientists, delivering accurate, actionable insights from their models. It also means an adjustment in the approach to model development. Optimization of a mature model is a new challenge that many of us will be facing over the next year. 3. Agile models â€” and supply chains More than anything else, 2020 has taught us to expect the unexpected. Supply chains are vulnerable to unanticipated disruptions, whether they come from pandemics, natural disasters, or whatever crisis we face next. A good AI model can digest any relevant information, map micro-patterns and micro-trends, learn from its own mistakes, and continuously adjust â€” all automatically. Just like a human would react, but more efficiently â€” and also more constrained by verifiable data. That means that once armed with new data and even limited observations, AI models learn and respond to changing conditions much better than any economically viable team of human managers armed with Excel and their gut feelings alone ever could. Data-driven decisions can better mitigate the impact of sudden deviations from historical patterns, giving companies a vital competitive advantage. In 2021, more agile supply chains are only going to increase in importance as retailers prepare for the next disaster. As such, companies will demand more agile forecasting models from data scientists. Data scientists must prioritize creating models that can more deftly respond to data imperfections, model drift and drastic deviations from historical patterns. More agile supply chains will be the priority of every retailer, and more agile models will be the ultimate goal of every data scientist serving them. 4. Consumer-driven products & time-lag elimination The coronavirus officially killed the traditional, top-down â€œpushâ€ model of retail. Successful retailers must now let customer preferences and demands drive product offerings. A faster response time is vital to meet demand. Product lifecycles will be shorter, requiring the supply chain to adjust accordingly. What does that mean for data scientists? Speed is more important than ever. Models must work faster, supply chains must be optimized down to the second, and systems must recognize and respond to even the slightest change in consumer demand. For each of these developments, data scientists must work ever harder to increase data velocity (and improve all of the 5 Vs of Big Data!). Every data scientist will be pushing their models ever-closer to real-time data and analysis in 2021. 5. Self-service AI optimization apps accessible to all During the pandemic, companies have rushed to implement AI, creating exponential growth in its adoption. Despite this, a report by Boston Consulting and the MIT Sloan Management Review reports that only 11% of companies implementing AI have seen sizeable returns on the investment. Why? Companies donâ€™t know how to use the technology effectively. For AI to fulfil its potential, AI must be easier to use. One of the biggest problems in AI that the pandemic exposed is the poor user experience of AI. The average retail executive struggles to use available tools designed for other data scientists. Itâ€™s why I champion business science: data science is useless unless what you learn can be applied for practical, applicable insights. What does that mean for 2021? Data scientists will design AI optimization tools and applications to be more user-friendly. More self-service, highly accessible apps will be released at higher rates over the next year. Simple optimization tools will no longer be a secondary goal; it will be the primary focus of every data scientist designing models to optimize any element of the supply chain. In fact, my own company Evo has made the release of our self-service portal a priority for 2021 for this exact reason. Even smaller retailers who canâ€™t afford personalized service and a model tailored by a data scientist deserve the opportunity to optimize their supply chain to compete in the post-Covid world. Unless these self-service offerings are accessible, AI successes will continue to lag behind adoption. Data scientists will have to change the way they design their applications to keep up in 2021. How to get ahead of data science trends for 2021 Each of these 5 trends will critically change the way data scientists operate over the next year. Itâ€™s time to adjust to these trends now before they accelerate â€” and you fall behind. You may be tempted to ignore these trends as isolated patterns in a single industry, but the reality is that they exist across the business world. They will affect your work sooner, rather than later. Iâ€™m looking forward to 2021 and a post-Covid world. But as much as the end of the pandemic, Iâ€™m looking forward to these trends becoming a reality for data scientists everywhere. Many data science trends for 2021 will help us all become better business scientists. PS I regularly write about Business Science. Recommended follow-up reading: Recovering from Bleak Friday How to use data to compete with Amazon post-Covid medium.com Supply Chain Optimization: Worth It or Not? The paradox of mathematical optimization towardsdatascience.com
18 hours agoÂ·13 min read An introduction to different families of multivariate time series and how this can impact your exploration and AI/ML modeling strategy. After an introduction to the different flavors of time series one can find in data science and machine learning projects, this article will focus on the ones that can be found in industrial settings. If youâ€™re looking for good strategies to explore, analyze and model univariate time series, you will probably find hundreds of them (including some great pieces on Medium): checking stationarity, building autoregressive models, uncovering seasonality, detecting and cleaning outliers, etc. Check out this series of articles from Mehul Gupta, starting with some basic terminologies: Getting Started with Time Series Time Series Analysis and Forecasting is one of the most important techniques in predictive analytics. Most dataâ€¦ medium.com Nowâ€¦ Letâ€™s say you want to learn more about multivariate timeseries? In this case, if youâ€™re like me, most of the articles and books you can find might not be of great help either from a theoretical or a practical standpointâ€¦ Even worse, if youâ€™re interested into dealing with industrial time series, most of the examples you find are about weather data, financial trading stocks, e-commerce or retail dataâ€¦ Most of them are data small enough that you can actually have a sneak peak in Excelâ€¦ You may have hundreds if not thousand of sensors data coming from industrial pieces of equipment at a high frequency (imagine 5 years of data coming from 5,000 sensors at a 10ms frequency)â€¦ This is where a sound framework for multivariate time series is desperately needed: in this article, we are going to look into what makes multivariate industrial data so different from most univariate time series we can find in usual data science challenges. In the conclusion, we will set the stage for future articles that will specifically deal with industrial time series. N.B.: in this article, you will see me use indifferently time series, signal or tag: these are quite common terminology when dealing with such type data. Call to action I intend to continue exploring industrial time series in future articles and even build Python packages and example Jupyter notebooks. If anyone has some industrial time series data they are ready to make publicly available so that we can blog about them (and even build benchmarks), leave me note! Happy to work together on ways to anonymize them and find a suitable way to host them! Univariate? Multivariate? Letâ€™s dive in the different flavors of time series signals one can find: Univariate time series data: a single series of a time-dependent variable (e.g. the energy output in kWh of a power plant). Multiple time series: multiple univariate time series about different phenomena, entities or events (e.g. the electricity consumption of all the arc furnaces a steel producer operates in Europe) Continuous multivariate time series: multiple time series applied to the same phenomenon, entity or event which is operating or happening continuously (e.g. the measurements taken by all the temperature, pressure and vibration sensors installed on a compressor) Discrete multivariate time series: multiple time series applied to a sequence of successive events (e.g. the measurements taken by all the sensors on a shampoo production lines that produces several batches of different products every days) Univariate time series A univariate time series is a series of a single time-dependent variable. For instance, this is what the maximum temperature around London looked like in early November 2011: Letâ€™s say you were asked to make a prediction for the temperature: you might look at past values and try to uncover a pattern. If you had several months of data, you might make the observation that temperature are usually lower in the morning than in the afternoon. With a year or a couple of years, you might even be able to extract seasonal trends, showing colder temperature in Winter and warmer in Summer. Note that all these observations could be done by looking at only one signal (the temperature in London). You might argue though, that this analysis is highly limited as it only considers that todayâ€™s temperature is dependent on past temperatures: however, we know that todayâ€™s temperature in a given location is actually a consequence of global atmospheric dynamic that is captured by numerous variables: temperatures around the globe, wind speed, pressure at all points of the atmosphere, solar activity, ocean dynamic like El NiÃ±oâ€¦ Multiple independent signals This first multiple time series category can be found in situation in which you have multiple (more or less) independent signals that can be seen as multiple univariate time series that you wish to analyze: the problem may seem to deal with multivariate timeseries, but they are independent most of the time. Historical daily price of trading stocks are good example of such signals: Trading stock of a given company can be influenced by the decisions of the company (Softbank selling its share in OSIsoft in August 2020) , major events from an industrial sector (Boeing 737MAX industrial accident impact on Boeing share), global undifferentiated event (Covid-19 impact onâ€¦ well, basically every sectors), tactical move from shareholders (activist buying a stake in a major competitor), chaotic human behavior based on local press coverageâ€¦ Depending on your scope of analysis, the independence of these signals can of course be weaker: indeed, you could try and forecast stock prices for similar companies from the same sector, operating in the same geography or having the same size. But all in all, if youâ€™d like to forecast the price of a given stock to inform buying and selling decisions, the relationship of a given stock with its peers may, most of the time, appear quite weak when compared to other exogenous parameters (shareholders reaction to press coverage) or endogenous (board decisions). There are however, already some challenges that you donâ€™t have to deal with when analyzing a single univariate time series: They can start and end at different dates (figure below): if you are unable to reconstruct the missing periods, this will force you to restrict your analysis to the common time ranges. You can have misaligned timestamps: one signal can be taken on the minute, while another 30 seconds later. Resampling and interpolating can help realign these timestamps for all your signals. The sampling rate can be different from on signal to another: depending on your use case and the phenomena you want to capture, you will have to compromise between under sampling the high frequency time series or over sampling the low frequency ones. Continuous multivariate timeseries event Critical industrial assets like a wind turbine are running 24/7 during their whole lifetime. Understanding the continuous time series emitted by these asset are key to deliver predictive and prescriptive maintenance capabilities that will allow running these assets at a higher utilization rate while ensuring the best performance. These industrial assets are instrumented with multiple sensors (sometime thousands) that generate continuous stream of data at a high frequency rate: Power generation assets (wind turbine, gas turbine, compressors, expandersâ€¦) Electrolysers transforming semi-finished mining materials to finished goods ready to ship Chemical reactors running continuously Key challenges associated to these kind of time series: Their sheer volume: logging 5,000 signals at 10 ms can produce 1 TB of data per year. Visualizing and exploring this volume of data without detrimental shortcuts: undersampling can restrain your capability to capture early onsets of anomalies you would like to predict, reducing the time period of analysis might generate bias in any training dataset you put together and analyzing a subselection of signals can prevent valuable interactions to be uncovered. Collect timestamped events of interest: most industrial keep historical maintenance operations database. This can help matching your time series with events you would like to better understand. However, these database and usually maintained manually, making them highly error-prone. Missing insights: if you want to perform anomaly prediction to prevent any undesirable event from happening (e.g. a potential part replacement) knowing when past replacements happened is not as useful as knowing when the industrial assets stop performing at its best. Identifying a signal that can be considered as the output of the process (e.g. energy output of a power plant or finished good quality) is key. Discrete multivariate timeseries Letâ€™s take the example of a paper mill: A simplified paper manufacturing process can look like this: At each process step, the production line collects many parameters: Some time series are coming from sensors linked to each piece of equipment: they are used to monitor each equipment themselves. Others might be linked to environmental conditions when they can impact the manufacturing process (e.g. atmosphere hygrometry or external temperature) Other time series will come from the process monitoring itself: for instance, the temperature of the paper pulp at the beginning of the process above can be monitored and measured. This is called a process variable. Each process variable is associated to a desired set point: at any given point of the process, the line operators applies a certain recipe to ensure the quality of the finished goods matches the desired requirements (e.g.: the desired temperature of the paper pulp) In addition, other data are needed to fully define the process behavior and the finished goods characteristics: raw material characteristics or supplier, quality inspection or lab results obtained at different quality gate of the processâ€¦ A paper mill such as the one operated by paper manufacturers can collect several thousands of time series at a high sampling rate (sub-second level). To ensure the highest throughput as possible, the production lines must have as high a utilization rate as possible. Data are continuously recorded across several batches of product (hence the â€œdiscreteâ€ qualifier used in the section title above) and each batch is more or less independent from the other: clean in place process might be needed between batches, the equipment or consumables have more operating hours, some pieces might have wore down a little bit more, some maintenance operations might have taken placeâ€¦ The context of a given batch can never be exactly reset to match the context of the previous one. Other example processes that produces similar multivariate time series can be: Different aircraft flights with different logged actions, different pilots, varying aircraft make and modelâ€¦ ECG from different patients of the same cohort or not Signals associated to successive cars assembled in an automotive factory To perform forecast analysis or anomaly detection on these multivariate time series, you need a model that will indeed try to find temporal correlation between all these signals. However, compared to the previous case (multiple independent univariate signals), correlation wonâ€™t be enough as the models also has to try and learn the physics nonlinear relationships these different signals may have in common. This is true both at the modelling phase and at the exploration phase. One of the key challenges you can have is to accurately slice your timeseries to obtain the right sequences: For a manufacturing production line, you can look for a discrete signals that could be used to record production batch numbers (a discrete signal with continuously increasing value might be a good hint) A rotating machine can have a signal with a unit of measure in RPM: when this value set to 0 or close enough, you can consider these periods as â€œoff-stateâ€ for the piece of equipment. You can also cluster the values of such signal to find different operating conditions. Another challenges is the presence of discrete (analog signals) that only take discrete values: they could be Boolean (0 or 1 given the state of a machine) or integer values (each value representing a given state of an actuator or parameter setpoints). You will also come upon other challenges linked to the way the data are stored and exported from local collection systems: Proprietary binary files format (ibaPDA is a well known data acquisition system that generates highly compressed binary files when exported: you will then need a specific wrapper running around a Windows DLL library to read you extracted data) For space saving purpose, only changing values are stored for each time series: when uncompressed to align every time series signals, and combined to binary formats as just mentioned, you sometime need to take care of how to deal with the massive amount of data generated: I once had 1 TB of data collected for a year, that yielded 30 TB of raw data on disk: at this point, Panda stops being your best friendâ€¦ Given that only changing values are stored in most industrial data acquisition system (like ibaPDA or any historian like GE Proficy Historian or OSIsoft PI System), the extraction function usually defaults to automatic resampling your data before the export. If you want to analyze the quality of your PID controllers or if you know how fast the phenomena you want to capture is, youâ€™ll need to make sure to extract actual raw data as they are recorded (otherwise, you risk smoothing out what youâ€™re precisely trying to uncover). Multivariate time series exploration challenges Depending on the family of multivariate time series you wish to analyze, you will have to devise a highly different strategy. Multiple independent signals Most of the time, the battery of packages, approaches, books and models that can be applicable to univariate time series can be successively applied to each available signal you have pertaining to your problem. With a low number of time series signals, you can perform covariance and cointegration analysis to understand the potential relationships between each signal. If you have a high number of signals at your disposal, you might also be able to extract new features from each signal and perform some unsupervised analysis (e.g. applying time series features engineering techniques and use clustering to identify families of time series). From a visualization and mining perspective, you might find some good insights with density line charts or multiscale temporal patterns mining with Pinus views. In addition, leveraging Markov transition fields and network graphs as I exposed in this article, is a great tool to objectively understand and classify time series behavior: Advanced visualization techniques for time series analysis Using Markov transition fields and network graphs to uncover time series behavior. towardsdatascience.com Multivariate timeseries event Whether your time series data are used to qualify continuous or discrete processes, some exploration challenges will be quite common: Ability to plot a high number of time series plots with the ability to zoom on different time ranges (for continuous data available over several years) or ability to highlight different sequences and links between them (for discrete processes like flight tests or production batches): using Bokeh, Highcharts or Plotly in a Jupyter Notebook will only get you so far without a realtime ingestion pipeline which will allow you to explore your data without any latency. Build an understanding on how to group different signals together for exploration purpose (analog vs. digital signals, clustering based on extracted features including the ones mentioned Labelled industrial data useful for anomaly prediction or predictive maintenance are very rare: some events are very difficult to catch and document in real time (if you have a production incident, you strive to solve it first, before recording the precise timestamp when itâ€™s happeningâ€¦). Having the ability to label your timeseries not only for classification but to dig right into them to label actual ranges when you see potential anomalous behavior is paramount to build high quality datasets that will later allow moving away from only unsupervised approaches. Conclusion and future works This article focused on a few definition pertaining to multivariate time series and pointed out some common challenges not encountered in univariate context. In the future, the work Iâ€™m interested into investigating is how to build a process or a system that will allow comfortable exploration of high dimensional time series data: from loading the right time range out of terabytes of data, visualizing hundreds of parallel time series, labelling and classifying them, regrouping them logically for uncovering potential correlationsâ€¦ There is a lot to explore and beginning with a repeatable time series profiling procedure looks like a good starting point. Once a suitable exploration framework is available I will move forward into building high quality datasets and benchmark datasets that will allow comparing and building time series oriented model zoos for classification, outlier detection, anomaly prediction, pattern learningâ€¦ Bonus: a few Python packages of interest! Beyond Pandas, you will find below some python packages I like to work with when dealing with univariate or multivariate time series data: tsfresh: this module can be used to extract characteristics from time series. It can compute up to 1,200 features for each time series provided. tsfuse: this package automatically constructs features from multiple time series. Instead of extracting univariate time series features, TSFuse generates new series by applying time series fusion operations. pyts: this is a Python package dedicated to time series classification. However, it also provides several preprocessing and utility tools for time series. tsia: a simple package I started to put together to help uncover patterns in time series thanks to imaging techniques like network graphs and Markov transition fields. This package is still in its infancy, feel free to send some suggestions my way to enrich it. fancyimpute: this package implements a variety of matrix completion and imputation algorithms (e.g. using KNN or matrix factorization).
Lina Faik 1 day agoÂ·11 min read Inequality is one of the greatest social and economic concerns of our time. It has dramatically increased over the last decades and has become deeply entrenched in our societies. Today, the figures are alarming: according to the OECD, the average income of the richest 10% of the population is about 9x that of the poorest 10% across the OECD. This is 7x higher than 25 years ago! The situation is even more worrying in the United State where the richest 1 percent concentrate more on national wealth and income than in any country. Faced with this situation, what actions should countries be taking to ensure a fair redistribution of wealth? As Amitabh Behar, the CEO of Oxfam India, puts it: â€œThe gap between rich and poor canâ€™t be resolved without deliberate inequality-busting policiesâ€. Therefore, despite the critics, tax policy is an indispensable tool for governments. It enables them to redistribute wealth through the services and the benefits they provide and thus reduce inequality. And yet, finding the optimal tax policy can be very challenging. Why? The main reason is that it has found a comprise of two opposing objectives: equality and productivity. In order words, tax policies must improve equality without undermining people's willingness to work, which would lead to lower productivity. Even with these two objectives in mind, it is still hard to determine the most effective tax policy, as it is difficult to carry out real-life experiments. Therefore, it is common to rely on economic theory. But the latter often makes simplifying assumptions that are hard to validate. So, how can Artificial Intelligence be used for tax policy design? AI and more precisely the field of Reinforcement Learning provides tools to simulate an artificial economy and observe and explore quantitatively the consequences of different policy scenarios. It consists of modeling multiple agents. These agents are immersed in a certain environment, which is here a simulated economy with a specific tax policy. For every action they choose to take, they receive rewards or penalties. After many iterations, they eventually learn the optimal decision-making strategy that will allow them to maximize the rewards accumulated over time. By observing the behaviors of these agents, it is possible to quantitative measure the efficiency of the tax policy both in terms of agentsâ€™ productivity and overall equality. How can it be implemented in practice? This is exactly what you will discover in my article. To go beyond the conceptual framework, I have simulated a simple world in which economic actors interact with each other according to the fiscal constraints set by a social planner. Both the economic actors and the government learn and adapt progressively. They do not use any economic modeling and rely only on observational data. In this article, I will describe the methodology I used, and I will share with you my key findings. You will learn: How to build an economic environment where multiple agents interact under market competitive pressures and dynamics? How to measure the effectiveness of a tax policy? What an AI-driven tax policy looks like and what improvement does it bring over common tax systems? This work is inspired by the research paper The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies written by S. Zheng & al. in April 2020[1]. Environment Description From the Agentsâ€™ Perspective The environment consists of a 2-dimensional grid word where agents are free to move, collect resources (stone or wood), and build houses at each step of an episode. They can also trade the resources they gather for a pre-defined price. Over the course of an episode, agentsâ€™ earnings evolve depending on the actions they take. More specifically, each action is associated with a labor cost while the action of building houses and trading is a source of income. Moreover, to account for the inequality of competences, their earnings depend on their skills: the higher the agentâ€™s skill is, the higher the cost of his work is. At the beginning of the simulation, agents are given the same initial wealth to ensure a sort of equality of opportunity. At the end of an episode, each agent receives a reward that reflects the utility he has acquired from the natural and financial resources he has earned, and from the work he has accomplished. The utility function used it the Constant Relative Risk Aversion (CRRA) utility function [2] defined as follows: Where: e_t and l_t are the earnings and labor costs accumulated over the episode Î· reflects the agentâ€™s degree of relative risk aversion. Higher values are associate with high-risk aversion levels. The figure synthesizes the interactions between the agents and the environment. From the Social Planner Perspective Each episode is split into 10 tax periods. At the beginning of each tax period, the social planner chooses the amount of taxes agents must pay depending on their income. These taxes are then evenly redistributed to agents at the end of the tax period. For comparison purposes, the tax policy is specified as a set of tax rates for 3 cut-off income levels. These levels are defined uniformly using on maximum and minimum earning of the agents during the episode. The objective of the social planner is to reach the optimal tax policy. But what is the optimal tax policy? The efficiency of a tax policy can be measured through the social welfare function swf which can be expressed as a trade-off between income equality eq and productivity prod. Where is the complement of the normalized Gini index on the distribution of agentsâ€™ income I Therefore, perfect equality is reached when eq = 1. Regarding productivity, it can be simply expressed as the sum of all incomes. It is analogous to GDP. The figure synthesizes the interactions between the Social Planner and the environment. Resulting Environment Dynamics The interactions between the environment agents can be depicted through the following graph: Results Analysis About the Simulation The simulation involves the interaction of 9 agents: 3 low skilled (earning 20% less than the average), 3 highly-skilled (earning 20% more than the average), and 3 medium-skilled. The risk aversion coefficient Î· chosen is 0.5. The learning algorithm chosen for both the agents and the social planner is a Deep Q-Networks (DQN) algorithm based on 3 fully connected layers. For more information, you can read this article. The simulation includes 150 episodes, each episode consisting of 100 steps. I compare 3 scenarios: Comparing the Different Tax Policies The training of the agents requires long computational times. Itâ€™s even longer in the AI-Driven tax policy case, as the Social Planner has also to learn and adapt to the behavior of the economic agents' of the environment. Therefore, the comparison below, which is based on the 50 last episodes, might be biased by the fact that agents in the Free Market scenario and Flat Tax scenario converge sooner. Overall, the AI-Driven leads to slightly higher equality than the other scenarios at the expense of productivity. Social welfare is the lowest among the 3 scenarios as the agents learn faster how to adapt to a non-changing tax schedule. Beyond the overall results obtained (which would be more telling with computation times longer than the 2 or 3 hours I used), what surprised me the most is the taxes paid by the middle class. As shown in the graph below, agents in this class have the highest rate and pay the most taxes. The research paper [1] also leads to this result (cf. next section). A way of boosting their productivity so that they can access the upper class while providing the lower class with subsidies to help them? Further Simulation and Comparison The environment I designed is intentionally very simplistic: the social planner chooses only 3 tax rates, prices of commodities are fixed, and the value of the house does not depend on the agent skills. This allows stakeholders to learn and adapt more quickly and effectively compared to an environment with more parameters and degrees of freedom. In this section, I will present to you the results obtained when the model is more complex. They are taken from the paper [1] which conducted more thorough experimentation and deeper analysis. More specifically, the paper compares the AI-driven tax policy (referred to as AI Economist) to the US federal tax schedule (US Federal). This involves cutting out the revenue intervals accordingly and keeping the same breakdown for all tax models. The paper also compares these scenarios to the free-market (no taxes) case (Free Market) and the Saez tax formula (Saez Formula). About Saez Tax Formula In the paper Using Elasticities to Derive Optimal Income Tax Rates (2001), Saez establishes a formula that aims at reallocating resources where they are needed most. More specially, he decomposes the marginal effects of a tax change into 3 effects: - Mechanical effect: it refers to the impact on government revenue if no individuals changed their behavior in response. If tax increases, this effect is positive. - Behavioral effect: it designates the impact on government revenue caused by a change in people behavioral followed by a tax modification. For instance, if tax increases, people will be discouraged to work harder, and tax revenue would drop. Thus, if tax increase, this effect is negative. - Welfare effect: it represents the impact of a tax change on the social welfare through peopleâ€™s utility. If tax increases, this effect is negative. At the optimum, the sum of these effect should be zero. This ultimately leads to the final formula. In order to get a sense of Saez tax formula without dwelling on mathematical calculation, letâ€™s suppose that the income distribution follows a Pareto distribution. The probability density of this distribution is represented in the graph below for various parameter Î±. The higher is, the thinner the tail of income becomes. In this case, Saez formula can be written as: Where: Ï„ is the tax rate Î¾_u and Î¾_c are respectively the uncompensated and compensated elasticities. The higher these elasticities are, the more sensitive individuals are to a change in tax. g is the ratio of social marginal utility for the top bracket taxpayers to the marginal value of public funds for the government. Small values of g mean that the government values less the welfare of individuals with high incomes. This formula has the advantage of being dependent not on an individualâ€™s utility function but on the income distribution. However, tax elasticities of income are difficult to estimate in the real world. Other differences between the paper implementation and my experimentation include the training algorithm (Proximal Policy Optimization, PPO vs. DQN), and the type of layers (LSTM vs. Layers), the number of agents (4 vs. 9), and the simulation parameters (in the paper, each episode contains 1000 steps and the simulation consists of 50M steps before convergence and 400M steps additional steps). Here are the 3 key findings of the paper: 1. The AI Economist leads to the best trade-off equality-productivity: Overall, the AI Economist achieves the best performance in terms of social welfare: it leads to a 16% improvement over the next best model which is the Saez Formula. This is achieved at the expense of productivity (11% decrease compared to Free Market) but for the benefit of equality (47% compared to the Free Market). These results are consistent over the training. The figure below shows that all model converges. 2. The AI Economist leads to a mix of progressive and regressive tax schedules. It fixes a higher top tax rate and a lower tax rate for medium incomes. This special structure enables disadvantaged agents to benefit from subsidies and thus ensure a higher equality level. As shown in the graph below, lower-skilled agents have higher net income under the AI Economist than under the other models. 3. The AI Economist leads to specializing where agents perform better in the respective activity. As under the Saez Formula, two kinds of agent profiles emerge from the set-up: high-skilled agents tend to buy resources and build houses whereas other agents are more likely to gather resources and sell them more frequently. However, under the AI-driven policy, agents are more productive in their specialization. Conclusion and Perspectives Beyond the results of these experiments, this study is exciting as it paves the way to new methods of testing social policies and analyzing their impacts. It removes the burden of the lack of data and ethical questions that weighs down more traditional approaches when it comes to experimentation. In the case of tax policies, the use of Reinforcement Learning enables to recreate specific environments and to put in interaction agents with different objectives. By observing how they adapt to each other, it becomes possible to measure the impact of predefined tax policies and even design one that maximizes the overall social welfare. There are two main advantages of such an approach: (1) it does not rely on economic modeling or any kind of prior assumptions and, (2) is very flexible (another social objective could have been chosen as well). Nevertheless, this kind of simulation is not without limitations: their ability to reproduce human behavior and interaction is not guaranteed, the model a relatively small economy and they require important computational time. However, I believe that merging Data Science with Economics has the potential to revolutionize the way policies are designed. This would ultimately lead to a significant positive social impact. So, get ready, Deep Reinforcement Learning will be the next tool of choice for economists! References [1] S. Zheng & al., The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies, Harvard University, April 2020 [2] Wikipedia, Isoelastic utility [3] A. Ratcliff, A. ThÃ©riault, Worldâ€™s billionaires have more wealth than 4.6 billion people, Oxfam, January 2020 [4] OECD, Inequality [5] E. Saez, Using Elasticities to Derive Optimal Income Tax Rates, Harvard University, May 2000
Vlad Alex (Merzmensch) 1 day agoÂ·16 min read The Digital Age reshaped everything. Even our conception of reality. In the XXth century, philosophers questioned the originality of an artwork (â€œThe Work of Art in the Age of Mechanical Reproductionâ€ by Walter Benjamin). In the XXIst century, we question if something is real at all (and if there is something like a reality). With Machine and Deep Learningâ€™s power, entirely new possibilities emerged: we can generate something that never existed before. And ontological discussions continue in new dimensionsâ€¦ The technical progress is going on at a rapid pace. For example, Generative Adversarial Networks. Here you can see the quality of GAN progress on face generation, presented in a tweet by Ian Goodfellow (creator and developer of GANs): And it was a 2018 tweet. The following year, StyleGAN came. And then, StyleGAN2. Based on the co-play of Generator and Discriminator this framework was able to generate photorealistic imagery. Not only the AI community was fascinated by new visual freshness. A website named â€œThisPersonDoesNotExist.comâ€ became viral and acted as a bridge between Data Science and a broad audience. It showcased AIâ€™s capability to create lifelike portraits of not existing personsâ€¦ or Deepfakes (a favorite media topic if itâ€™s time to speak about visual AI). DeepFake as a term means already delusion, fraud. But for me, ML/DL creations are full of unusual beauty. Forget â€œUncanny Valleyâ€. It became a cultural movement, a metaphor for AIâ€™s creative (and dangerous) potential, a meme. The â€œThis XY Does Not Existâ€ meme. In the following, I want to present the existing representations of non-existing things, crafted with Machine and Deep Learningâ€™s power. The list will be ongoing updated with new findings. Visual Non-Existant Things This Person Does Not Exist Presented images are coming from the Latent Space (the â€œmachineâ€™s subconsciousnessâ€, â€œrepresentation of compressed dataâ€). Every time you refresh the page, a new image appears. This was probably one of the first ways to demonstrate the power of StyleGAN2. I advise you to run this StyleGAN2 Colab Notebook, finetuned by Mikael Christensen. It even has the possibility of StyleGAN2 projection (e.g. for detecting DeepFakes). Meanwhile, there is an opposite page: ThisPersonExists, launched by Vincent Woo as a homage to thousands of real human faces being used for StyleGAN2 training. Beautiful and weird as well. This Cat Does Not Exist This page presents StyleGAN2-generated images of cats. Unlike faces, its resolution is rather humble (256x256), and cats donâ€™t always look realistic. But, as a side note, do cats ever look realistic? I mean, IRL? This Horse Does Not Exist The same story (see: cats). The low resolution doesnâ€™t reconstruct alternate horses. Sometimes you see weird glitches as if the Discriminator gave up on low quality and went fishing; however, the Generator still tried as well as possible. These pre-trained networks are enabled by NVidia with StyleGAN2. In the default collection were also â€œcarâ€ and â€œchurchâ€ datasets available. Alas, ThisChurchDoesNotExist doesnâ€™t exist â€” but, again, you can generate it using StyleGAN2 Notebook. This Automobile Does Not Exist For car lovers â€” a source of endless fascination. Interestingly, the number plates are anonymized, unintentionally â€” since this StyleGAN2 model is not trained on language. A nice side effect for Data Privacy. This Artwork Does Not Exist This is an excellent example of the reinterpretation of art by ML. Is it still mimicking human creativity, or something more? I also used this model for making some animated attract impressions, like this: Or this: Trained on Art Datasets, Neural Network can produce beautiful media, and itâ€™s up to us how to interpret it. This Waifu Does Not Exist With this project, the legendary Gwern Branwen brought joy not only for every otaku among us. In combination with GPT-3 generated text, you will get a review of non-existing anime shows every time you refresh your browser (or just after a couple of seconds of auto-refresh). The answer to the question in the project title â€œDo Electric Neural Nets Dream Of Anime Shows?â€ should be definitely: â€œã¯ã„ï¼â€ This Eye Does Not Exist You wonâ€™t find here Eye of Sauron, so donâ€™t worry â€” GonÃ§alo Abreu created a dataset of MakeUp faces and trained ProGAN on its eyes. Some of them are beautiful, some of them are creepy, some of them have definitely glitches. And you can vote for them as well. Why not? This Beetle Does Not Exist Imagine yourself being a coleopterist, discovering rare beetles, one after another. In the Latent Space. Thatâ€™s what a project by Bernat Cuni â€œConfusing Coleopteristsâ€ is all about. You see beautiful illustrations from scientific books under Public Domain license (it does mean they have some longer lives than many of us already). Look for your one exemplar! This Butterfly Does Not Exist Around 30.000 Butterflies, generated by StyleGAN2, flutter around on this website. Creator described pretty well the steps he did, so not only inspiring but also very instructive. This City Does Not Exist Ironically, for people living in Europe, such satellite images should be so familiar that one almost could identify a small town â€” besides, there are no such on the European ground or at the Earthâ€™s surface. Ron Hagensieker and Tomas Langer created an exciting model that lets you sneak into Alternate Europe. This Vessel Does Not Exist Ceramicist Derek Philip Au is multi-talent â€” not only he crafts ceramics, he also created Glazy, a free platform for sharing and analyzing ceramics recipes. But with ML, he also explores new frontiers of creativity. AI-generated new vessels can inspire new textures and patterns. Moreover, new combinations and metamorphoses are possible. For example, in combination with Beetle datasets: or with dresses: This Pony Does Not Exist Derpibooru is an image board for My Little Pony fans â€” and so are the contents. Thousands and thousands of fan images from the series â€” some of them SFW, some of them rather not. Luckily, Arfa trained the network on the safe images â€” and the page is a real Latent Space browser, where you can observe all kinds of My Little Ponies, generated by StyleGAN2. This Fursona Does Not Exist Another project by Arfa, similar to Ponies â€” and in this case, I would be even very grateful for using only SFW images for training the Network. Really. This Chemical Does Not Exist I know almost nothing about this project (please drop some info about it in the comments). The only thing is: this page is linked directly from ThisPersonDoesNotExist.com. Here you can examine a molecule in 3D. Without any background or further information. Call it Chem-ZEN. Mesmerizing. This Muppet Does Not Exist Jim Hanson would appreciate that his characters had run out of control and evolved to some weird, sometimes nightmarish, but all in all sympathetic Mega-Creatures like Doron did with StyleGAN2. Follow him on Twitter â€” he does many great experiments with the visual capabilities of Machine Learning. Textual Non-Existing Things This Word Does Not Exist This extraordinary project, powered by GPT-2, opens a door into the endless world of languages. Non-existing? Probably, till this moment â€” but after it emerges, the new word becomes manifested â€” and you can use it (if you want). This page features even a possibility to write your fantasy word â€” and the system will dress it up with semantics. Infinite Jest. These Lyrics Do Not Exist This lyric generator is a pretty powerful one. It creates lyrics to practically every topic you can imagine. Here is a song about toaster (you may know my affinity towards Toasters and Data Science): Or a rock song about â€œArtificial Intelligenceâ€: Even if they not only have a rhyme or coherent verse, nevertheless you cannot stop to generate â€” and to sing! This Snack Does Not Exist As project authors write, â€œWhile not exactly your ordinary GPT-2 modal, we do crawl the web to find unexpected combos and show them for you.â€ Interestingly, the page provides ML-based probability factors for every snack. â€œPad Thai Bubble Gumâ€ is 0.146, according to their Plausibility Index. This is the code of the project: looking for and scoring â€œthe plausibility of different types of associationsâ€. This inspirational art quote does not exist. This one is a beautiful approach to reimagine artist manifests and statements by AI. The creator of â€œVESSELâ€ (see above), Derek Philip Au is experimenting with GPT-3 as well, and the non-existing quotes are not only convincing. They represent existing opinions and views â€” even if the particular artists have never said it (or even if there arenâ€™t such particular artists at all). For example, even if I couldnâ€™t find any artist named â€œGiles Newtonâ€ â€” but art, criticizing art critics was almost the way of creating by Kurt Schwitters, German Dadaist, who mashed up all the criticism on his art into his new artworks. Transmedial Non-Existing Things Combining various visual and textual ML approaches can generate more unusual realms. Here are just some of them, which donâ€™t exist, and yet they do. This Rental Does Not Exist Nowadays, travels arenâ€™t in â€” Pandemics are causing our StayAtHome state of mind. But our fernweh can be healed by some crazy experiments, like this one. Trained on OpenDataSoftâ€™s Airbnb Listings and using â€œPredict Shakespeare With Cloud TPUsâ€ the system generates Rental Apartment descriptions. Images are produced by StyleGAN2 (on Bedroom Dataset). This is for sure not GPT-3 and the texts are on the fringe between insanity and reality, but this combination of visual and contextual data creates an eerie vibe of reality here. This Startup Does Not Exist Prepare for a long scroll: This is more than just an experiment. Itâ€™s a parody about the generic state of many StartUps, who try to disrupt but implode after several pitches. According to Forbes and Fortune, â€œNine of ten startups failâ€. There are manifold reasons for their ends, but one of the most crucial is: they often bear too perfect ideas without considering reality. In ThisStartupDoesNotExist we observe the web presences of unexisting startups as Zeitgeist: fancy stock image from Unsplash, hipster-ish looking staff, detailed pricing data, but you mostly have no clue, what does this StartUp really do. In the case of the current page, itâ€™s obvious (generated contents). But in the case of many existing StartUps, itâ€™s why they even by themselves not really know what do they actually do. This Question Does Not Exist Trained on Question&Answer conversations from StackExchange, this model generates realistically looking dialogues with experts. Stylistically and topographically correctly simulated forum couldnâ€™t probably solve the real problems but is refreshingly inspiring and generous for thinking out of the box. This Foot Does Not Exist Guerilla company MSCHF, which is (in)famous for various memes, virals, puns, and subversive fun, generated this page using StyleGAN. As training Data was used â€œWikiFeetâ€, a massive database of feet images (donâ€™t ask me, please). This Foot Does Not Exist is a parody of fetishism of old urges and new technologies, social media clashes, and messaging app crazes. Subversive as every other project by MSCHF. This Resume Does Not Exist If there are persons who do exist, even if they donâ€™t, why shouldnâ€™t they have a personal CV? Currently, the site seems not to work properly, as only one CV keeps to be generated repeatable. You can edit it within the browser and the texts are TextgenRNN generated contents, way better than lorem ipsum, but still not GPT-3. Anyway, the chic and simple design is pleasant. I hope to generate new CVs for a new person because this is an fantastic way of storytelling. This MP Does Not Exist In the case of Brexit, you probably need a new fresh perspective. Even a new Parliament. Like this one. In this case, â€œMP data was grabbed from Parliament.uk-pageâ€: images were used for training and generating new faces, and â€œnames were built using segments from the names of 2.097 real MPsâ€ (source). A bitter satire about generic politicians. This Meme Does Not Exist dylan wenzlau, founder of ImgFlip, trained Deep Conv Net on ~100M public meme captions by users of the Imgflip Meme Generator. Check out his article here on Towards Data Science: Meme Text Generation with a Deep Convolutional Network in Keras & Tensorflow The goal of this post is to describe end-to-end how to build a deep conv net for text generation, but in greater depthâ€¦ towardsdatascience.com The juxtaposition of memes as cultural phenomena and chaotic AI-generated captions is mind-blowing and hilarious at the same time. Itâ€™s a genius. Absurd. Metafictional. Other Sources about non-existing Stuff Even if I tried to include in this list everything I found so far, there are also other sources and lists covering Stuff that doesnâ€™t exist. Just to keep track. This X Does Not Exist This Startup Does Not Exist If your IP address is in San Francisco, this site actually just redirects to a randomâ€¦ thisxdoesnotexist.com Aggregate-Intellect/awesome-does-not-exist A collection of This * Does Not Exist websites. Contribute to Aggregate-Intellect/awesome-does-not-exist development byâ€¦ github.com In case youâ€™ve found something non-existing new, please write in the comments. The only rule is, the project should be called â€œThis (item) Does Not Existâ€ (even if I violated my own game rules in this article).
All incorporated in a single python notebook! Dhruvil Shah 1 day agoÂ·10 min read What is Instance Segmentation? Instance segmentation is the function of pixel-level recognition of object outlines. Itâ€™s one of the hardest possible vision tasks relative to equivalent computer vision tasks. Refer to the following terminologies: Classification: There is a horse/man in this image. Semantic Segmentation: These are all the horse/man pixels. Object Detection: There are a horse and a man in this image at these locations. Weâ€™re starting to account for objects that overlap. Instance Segmentation: There is a horse and a man at these locations and these are the pixels that belong to each one. You can learn the basics and how Mask RCNN actually works from here. We will implement Mask RCNN for a custom dataset in just one notebook. All you need to do is run all the cells in the notebook. We will perform simple Horse vs Man classification in this notebook. You can change this to your own dataset. I have shared the links at the end of the article. Letâ€™s begin. 1. Importing and cloning repositories. First, we will clone a repository that contains some of the building code blocks for implementing Mask RCNN. The function copytree() will get the necessary files for you. After cloning the repository we will import the dataset. I am importing the dataset from my drive. Letâ€™s talk about the dataset now. The dataset that I imported from my drive is structured as: The dataset that I have used, is created from VGG Image Annotator (VIA). The annotation file is in .json format which contains the coordinates of all the polygons which I have drawn on my images. The .json file will look something like this: Note: Although I have shown more than one shape in the above snippet, you should only use one shape while annotating the images. For instance, if you choose polygons as I have for the Horse vs Man classifier then you should annotate all the regions with polygons only. We will discuss how you can use multiple shapes for annotations later in this tutorial. 2. Selection of right versions of libraries Note: This is the most important step for implementing the Mask RCNN architecture without getting any errors. You will face many errors in TensorFlow version 2.x. Also, the Keras version 2.2.5 will save us from many errors. I will not go into detail regarding which kinds of error this resolves. 3. Configuration according to our dataset First, we will import a few libraries. Then we will give the path to the trained weights file. This could be the COCO weights file or your last saved weights file (checkpoint). The log directory is where all our data will be stored when training begins. The model weights at every epoch are saved in .h5 format in the directory so if the training gets hindered due to any reason you can always start from where you left off by specifying the path to the last saved model weights. For instance, if I am training my model for 10 epochs and at epoch 3 my training is obstructed then I will have 3 .h5 files stored in my logs directory. And now I do not have to start my training from the beginning. I can simply change my weights path to the last weights file e.g. â€˜mask_rcnn_object_0003.h5â€™. CustomConfig class contains our custom configurations for the model. We simply overwrite the data in the original Config class from the config.py file that was imported earlier. The number of classes is supposed to be total_classes + 1 (for background). Steps per epoch are set to 100 but if you want you can increase if you have access to higher computing resources. The detection threshold is 90% that means all the proposals with less than 0.9 confidence will be ignored. This threshold is different than the one while testing an image. Look at the two images below for clarification. 4. Setting up the CustomDataset class The class below contains 3 crucial methods for our custom dataset. This class inherits from â€œutils.Datasetâ€ which we imported in the 1st step. The â€˜load_customâ€™ method is for saving the annotations along with the image. Here we extract polygons and the respective classes. polygons = [r[â€˜shape_attributesâ€™] for r in a[â€˜regionsâ€™]] objects = [s[â€˜region_attributesâ€™][â€˜nameâ€™] for s in a[â€˜regionsâ€™]] Polygons variable contains the coordinates of the masks. Objects variable contains the names of the respective classes. The â€˜load_maskâ€™ method loads the masks as per the coordinates of polygons. The mask of an image is nothing but a list containing binary values. Skimage.draw.polygon() does the task for us and returns the indices for the coordinates of the mask. Earlier we discussed that you should not use more than one shape while annotating as it will get intricate when loading masks. Although If you want to use multiple shapes i.e. circle, ellipse, and polygon then you will need to change the load mask function as below. This is one way you can load masks with multiple shapes but this is just speculation and the model will not detect a circle or ellipse unless it captures a perfect circle or ellipse. 5. Creating Train() function The dataset that I imported from my drive is structured as: First, we will create an instance of CustomDataset class for the training dataset. Similarly, create another instance for the validation dataset. Then we will call the load_custom() method by passing in the name of the directory where our data is stored. â€˜layersâ€™ parameter is set to â€˜headsâ€™ here as I am not planning to train all the layers in the model. This will only train some of the top layers in the architecture. If you want you can set â€˜layersâ€™ to â€˜allâ€™ for training all the layers in the model. I am running the model for only 10 only as this tutorial is supposed to just guide you. 6. Setup before the training This step is for setting up the model for training and downloading and loading pre-trained weights. You can load the weights of COCO or your last saved model. The call â€˜modellib.MaskRCNN()â€™ is the step where you can get lots of errors if you have not chosen the right versions in the 2nd section. This method has a parameter â€˜modeâ€™ which decides whether we want to train the model or test the model. If you want to test set â€˜modeâ€™ to â€˜inferenceâ€™. â€˜model_dirâ€™ is for saving the data while training for backup. Then, we download the pre-trained COCO weights in the next step. Note: If you want to resume training from a saved point then you need to change â€˜weights_pathâ€™ to the path where your .h5 file is stored. 7. Start training This step should not throw any error if you have followed the steps above and training should start smoothly. Remember we need to have Keras version 2.2.5 for this step to run error-free. Note: If you get an error restart runtime and run all the cells again. It may be because of the version of TensorFlow or Keras loaded. Follow step 2 for choosing the right versions. Note: Ignore any warnings you get while training! 8. Testing You can find the instructions in the notebook on how to test our model once training is finished. Here, we define the path to our last saved weights file to run the inference on. Then, we define simple configuration terms. Here, confidence is specified again for testing. This is different from the one used in training. Now we will load the model to run inference. Now loading the weights in the model. Now, we are ready for testing our model on any image. 9. Color Splash For fun, you can try the below code which is present in the original implementation of Mask RCNN. This will convert everything grayscale except for the object mask areas. You can call this function as specified below. For video detection, call the second function. 10. Furtherâ€¦ You can learn how Region Proposals work from the latter cells of the notebook. Below are some of the interesting images that might catch your eye if you are curious about how the Region Proposal Networks work. You may want to see how Proposal classification works and how we get our final regions for segmentation. This portion is also covered in the latter part of the notebook. You can find the notebook comprising all of the code we saw above from GitHub here. You can connect with me on LinkedIn from here. References:
Joseph Rocca Sep 24, 2019Â·23 min read This post was co-written with Baptiste Rocca. Introduction In the last few years, deep learning based generative models have gained more and more interest due to (and implying) some amazing improvements in the field. Relying on huge amount of data, well-designed networks architectures and smart training techniques, deep generative models have shown an incredible ability to produce highly realistic pieces of content of various kind, such as images, texts and sounds. Among these deep generative models, two major families stand out and deserve a special attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). In a previous post, published in January of this year, we discussed in depth Generative Adversarial Networks (GANs) and showed, in particular, how adversarial training can oppose two networks, a generator and a discriminator, to push both of them to improve iteration after iteration. We introduce now, in this post, the other major kind of deep generative models: Variational Autoencoders (VAEs). In a nutshell, a VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term â€œvariationalâ€ comes from the close relation there is between the regularisation and the variational inference method in statistics. If the last two sentences summarise pretty well the notion of VAEs, they can also raise a lot of questions. What is an autoencoder? What is the latent space and why regularising it? How to generate new data from VAEs? What is the link between VAEs and variational inference? In order to describe VAEs as well as possible, we will try to answer all this questions (and many others!) and to provide the reader with as much insights as we can (ranging from basic intuitions to more advanced mathematical details). Thus, the purpose of this post is not only to discuss the fundamental notions Variational Autoencoders rely on but also to build step by step and starting from the very beginning the reasoning that leads to these notions. Without further ado, letâ€™s (re)discover VAEs together! Outline In the first section, we will review some important notions about dimensionality reduction and autoencoder that will be useful for the understanding of VAEs. Then, in the second section, we will show why autoencoders cannot be used to generate new data and will introduce Variational Autoencoders that are regularised versions of autoencoders making the generative process possible. Finally in the last section we will give a more mathematical presentation of VAEs, based on variational inference. Note. In the last section we have tried to make the mathematical derivation as complete and clear as possible to bridge the gap between intuitions and equations. However, the readers that doesnâ€™t want to dive into the mathematical details of VAEs can skip this section without hurting the understanding of the main concepts. Notice also that in this post we will make the following abuse of notation: for a random variable z, we will denote p(z) the distribution (or the density, depending on the context) of this random variable. Dimensionality reduction, PCA and autoencoders In this first section we will start by discussing some notions related to dimensionality reduction. In particular, we will review briefly principal component analysis (PCA) and autoencoders, showing how both ideas are related to each others. What is dimensionality reduction? In machine learning, dimensionality reduction is the process of reducing the number of features that describe some data. This reduction is done either by selection (only some existing features are conserved) or by extraction (a reduced number of new features are created based on the old features) and can be useful in many situations that require low dimensional data (data visualisation, data storage, heavy computationâ€¦). Although there exists many different methods of dimensionality reduction, we can set a global framework that is matched by most (if not any!) of these methods. First, letâ€™s call encoder the process that produce the â€œnew featuresâ€ representation from the â€œold featuresâ€ representation (by selection or by extraction) and decoder the reverse process. Dimensionality reduction can then be interpreted as data compression where the encoder compress the data (from the initial space to the encoded space, also called latent space) whereas the decoder decompress them. Of course, depending on the initial data distribution, the latent space dimension and the encoder definition, this compression can be lossy, meaning that a part of the information is lost during the encoding process and cannot be recovered when decoding. The main purpose of a dimensionality reduction method is to find the best encoder/decoder pair among a given family. In other words, for a given set of possible encoders and decoders, we are looking for the pair that keeps the maximum of information when encoding and, so, has the minimum of reconstruction error when decoding. If we denote respectively E and D the families of encoders and decoders we are considering, then the dimensionality reduction problem can be written where defines the reconstruction error measure between the input data x and the encoded-decoded data d(e(x)). Notice finally that in the following we will denote N the number of data, n_d the dimension of the initial (decoded) space and n_e the dimension of the reduced (encoded) space. Principal components analysis (PCA) One of the first methods that come in mind when speaking about dimensionality reduction is principal component analysis (PCA). In order to show how it fits the framework we just described and make the link towards autoencoders, letâ€™s give a very high overview of how PCA works, letting most of the details aside (notice that we plan to write a full post on the subject). The idea of PCA is to build n_e new independent features that are linear combinations of the n_d old features and so that the projections of the data on the subspace defined by these new features are as close as possible to the initial data (in term of euclidean distance). In other words, PCA is looking for the best linear subspace of the initial space (described by an orthogonal basis of new features) such that the error of approximating the data by their projections on this subspace is as small as possible. Translated in our global framework, we are looking for an encoder in the family E of the n_e by n_d matrices (linear transformation) whose rows are orthonormal (features independence) and for the associated decoder among the family D of n_d by n_e matrices. It can be shown that the unitary eigenvectors corresponding to the n_e greatest eigenvalues (in norm) of the covariance features matrix are orthogonal (or can be chosen to be so) and define the best subspace of dimension n_e to project data on with minimal error of approximation. Thus, these n_e eigenvectors can be chosen as our new features and, so, the problem of dimension reduction can then be expressed as an eigenvalue/eigenvector problem. Moreover, it can also be shown that, in such case, the decoder matrix is the transposed of the encoder matrix. Autoencoders Letâ€™s now discuss autoencoders and see how we can use neural networks for dimensionality reduction. The general idea of autoencoders is pretty simple and consists in setting an encoder and a decoder as neural networks and to learn the best encoding-decoding scheme using an iterative optimisation process. So, at each iteration we feed the autoencoder architecture (the encoder followed by the decoder) with some data, we compare the encoded-decoded output with the initial data and backpropagate the error through the architecture to update the weights of the networks. Thus, intuitively, the overall autoencoder architecture (encoder+decoder) creates a bottleneck for data that ensures only the main structured part of the information can go through and be reconstructed. Looking at our general framework, the family E of considered encoders is defined by the encoder network architecture, the family D of considered decoders is defined by the decoder network architecture and the search of encoder and decoder that minimise the reconstruction error is done by gradient descent over the parameters of these networks. Letâ€™s first suppose that both our encoder and decoder architectures have only one layer without non-linearity (linear autoencoder). Such encoder and decoder are then simple linear transformations that can be expressed as matrices. In such situation, we can see a clear link with PCA in the sense that, just like PCA does, we are looking for the best linear subspace to project data on with as few information loss as possible when doing so. Encoding and decoding matrices obtained with PCA define naturally one of the solutions we would be satisfied to reach by gradient descent, but we should outline that this is not the only one. Indeed, several basis can be chosen to describe the same optimal subspace and, so, several encoder/decoder pairs can give the optimal reconstruction error. Moreover, for linear autoencoders and contrarily to PCA, the new features we end up do not have to be independent (no orthogonality constraints in the neural networks). Now, letâ€™s assume that both the encoder and the decoder are deep and non-linear. In such case, the more complex the architecture is, the more the autoencoder can proceed to a high dimensionality reduction while keeping reconstruction loss low. Intuitively, if our encoder and our decoder have enough degrees of freedom, we can reduce any initial dimensionality to 1. Indeed, an encoder with â€œinfinite powerâ€ could theoretically takes our N initial data points and encodes them as 1, 2, 3, â€¦ up to N (or more generally, as N integer on the real axis) and the associated decoder could make the reverse transformation, with no loss during the process. Here, we should however keep two things in mind. First, an important dimensionality reduction with no reconstruction loss often comes with a price: the lack of interpretable and exploitable structures in the latent space (lack of regularity). Second, most of the time the final purpose of dimensionality reduction is not to only reduce the number of dimensions of the data but to reduce this number of dimensions while keeping the major part of the data structure information in the reduced representations. For these two reasons, the dimension of the latent space and the â€œdepthâ€ of autoencoders (that define degree and quality of compression) have to be carefully controlled and adjusted depending on the final purpose of the dimensionality reduction. Variational Autoencoders Up to now, we have discussed dimensionality reduction problem and introduce autoencoders that are encoder-decoder architectures that can be trained by gradient descent. Letâ€™s now make the link with the content generation problem, see the limitations of autoencoders in their current form for this problem and introduce Variational Autoencoders. Limitations of autoencoders for content generation At this point, a natural question that comes in mind is â€œwhat is the link between autoencoders and content generation?â€. Indeed, once the autoencoder has been trained, we have both an encoder and a decoder but still no real way to produce any new content. At first sight, we could be tempted to think that, if the latent space is regular enough (well â€œorganizedâ€ by the encoder during the training process), we could take a point randomly from that latent space and decode it to get a new content. The decoder would then act more or less like the generator of a Generative Adversarial Network. However, as we discussed in the previous section, the regularity of the latent space for autoencoders is a difficult point that depends on the distribution of the data in the initial space, the dimension of the latent space and the architecture of the encoder. So, it is pretty difficult (if not impossible) to ensure, a priori, that the encoder will organize the latent space in a smart way compatible with the generative process we just described. To illustrate this point, letâ€™s consider the example we gave previously in which we described an encoder and a decoder powerful enough to put any N initial training data onto the real axis (each data point being encoded as a real value) and decode them without any reconstruction loss. In such case, the high degree of freedom of the autoencoder that makes possible to encode and decode with no information loss (despite the low dimensionality of the latent space) leads to a severe overfitting implying that some points of the latent space will give meaningless content once decoded. If this one dimensional example has been voluntarily chosen to be quite extreme, we can notice that the problem of the autoencoders latent space regularity is much more general than that and deserve a special attention. When thinking about it for a minute, this lack of structure among the encoded data into the latent space is pretty normal. Indeed, nothing in the task the autoencoder is trained for enforce to get such organisation: the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised. Thus, if we are not careful about the definition of the architecture, it is natural that, during the training, the network takes advantage of any overfitting possibilities to achieve its task as well as it canâ€¦ unless we explicitly regularise it! Definition of variational autoencoders So, in order to be able to use the decoder of our autoencoder for generative purpose, we have to be sure that the latent space is regular enough. One possible solution to obtain such regularity is to introduce explicit regularisation during the training process. Thus, as we briefly mentioned in the introduction of this post, a variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process. Just as a standard autoencoder, a variational autoencoder is an architecture composed of both an encoder and a decoder and that is trained to minimise the reconstruction error between the encoded-decoded data and the initial data. However, in order to introduce some regularisation of the latent space, we proceed to a slight modification of the encoding-decoding process: instead of encoding an input as a single point, we encode it as a distribution over the latent space. The model is then trained as follows: first, the input is encoded as distribution over the latent space second, a point from the latent space is sampled from that distribution third, the sampled point is decoded and the reconstruction error can be computed finally, the reconstruction error is backpropagated through the network In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. The reason why an input is encoded as a distribution with some variance instead of a single point is that it makes possible to express very naturally the latent space regularisation: the distributions returned by the encoder are enforced to be close to a standard normal distribution. We will see in the next subsection that we ensure this way both a local and global regularisation of the latent space (local because of the variance control and global because of the mean control). Thus, the loss function that is minimised when training a VAE is composed of a â€œreconstruction termâ€ (on the final layer), that tends to make the encoding-decoding scheme as performant as possible, and a â€œregularisation termâ€ (on the latent layer), that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution. That regularisation term is expressed as the Kulback-Leibler divergence between the returned distribution and a standard Gaussian and will be further justified in the next section. We can notice that the Kullback-Leibler divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions. Intuitions about the regularisation The regularity that is expected from the latent space in order to make generative process possible can be expressed through two main properties: continuity (two close points in the latent space should not give two completely different contents once decoded) and completeness (for a chosen distribution, a point sampled from the latent space should give â€œmeaningfulâ€ content once decoded). The only fact that VAEs encode inputs as distributions instead of simple points is not sufficient to ensure continuity and completeness. Without a well defined regularisation term, the model can learn, in order to minimise its reconstruction error, to â€œignoreâ€ the fact that distributions are returned and behave almost like classic autoencoders (leading to overfitting). To do so, the encoder can either return distributions with tiny variances (that would tend to be punctual distributions) or return distributions with very different means (that would then be really far apart from each other in the latent space). In both cases, distributions are used the wrong way (cancelling the expected benefit) and continuity and/or completeness are not satisfied. So, in order to avoid these effects we have to regularise both the covariance matrix and the mean of the distributions returned by the encoder. In practice, this regularisation is done by enforcing distributions to be close to a standard normal distribution (centred and reduced). This way, we require the covariance matrices to be close to the identity, preventing punctual distributions, and the mean to be close to 0, preventing encoded distributions to be too far apart from each others. With this regularisation term, we prevent the model to encode data far apart in the latent space and encourage as much as possible returned distributions to â€œoverlapâ€, satisfying this way the expected continuity and completeness conditions. Naturally, as for any regularisation term, this comes at the price of a higher reconstruction error on the training data. The tradeoff between the reconstruction error and the KL divergence can however be adjusted and we will see in the next section how the expression of the balance naturally emerge from our formal derivation. To conclude this subsection, we can observe that continuity and completeness obtained with regularisation tend to create a â€œgradientâ€ over the information encoded in the latent space. For example, a point of the latent space that would be halfway between the means of two encoded distributions coming from different training data should be decoded in something that is somewhere between the data that gave the first distribution and the data that gave the second distribution as it may be sampled by the autoencoder in both cases. Note. As a side note, we can mention that the second potential problem we have mentioned (the network put distributions far from each others) is in fact almost equivalent to the first one (the network tends to return punctual distribution) up to a change of scale: in both case variances of distributions become small relatively to distance between their means. Mathematical details of VAEs In the previous section we gave the following intuitive overview: VAEs are autoencoders that encode inputs as distributions instead of points and whose latent space â€œorganisationâ€ is regularised by constraining distributions returned by the encoder to be close to a standard Gaussian. In this section we will give a more mathematical view of VAEs that will allow us to justify the regularisation term more rigorously. To do so, we will set a clear probabilistic framework and will use, in particular, variational inference technique. Probabilistic framework and assumptions Letâ€™s begin by defining a probabilistic graphical model to describe our data. We denote by x the variable that represents our data and assume that x is generated from a latent variable z (the encoded representation) that is not directly observed. Thus, for each data point, the following two steps generative process is assumed: first, a latent representation z is sampled from the prior distribution p(z) second, the data x is sampled from the conditional likelihood distribution p(x|z) With such a probabilistic model in mind, we can redefine our notions of encoder and decoder. Indeed, contrarily to a simple autoencoder that consider deterministic encoder and decoder, we are going to consider now probabilistic versions of these two objects. The â€œprobabilistic decoderâ€ is naturally defined by p(x|z), that describes the distribution of the decoded variable given the encoded one, whereas the â€œprobabilistic encoderâ€ is defined by p(z|x), that describes the distribution of the encoded variable given the decoded one. At this point, we can already notice that the regularisation of the latent space that we lacked in simple autoencoders naturally appears here in the definition of the data generation process: encoded representations z in the latent space are indeed assumed to follow the prior distribution p(z). Otherwise, we can also remind the the well-known Bayes theorem that makes the link between the prior p(z), the likelihood p(x|z), and the posterior p(z|x) Letâ€™s now make the assumption that p(z) is a standard Gaussian distribution and that p(x|z) is a Gaussian distribution whose mean is defined by a deterministic function f of the variable of z and whose covariance matrix has the form of a positive constant c that multiplies the identity matrix I. The function f is assumed to belong to a family of functions denoted F that is left unspecified for the moment and that will be chosen later. Thus, we have Letâ€™s consider, for now, that f is well defined and fixed. In theory, as we know p(z) and p(x|z), we can use the Bayes theorem to compute p(z|x): this is a classical Bayesian inference problem. However, as we discussed in our previous article, this kind of computation is often intractable (because of the integral at the denominator) and require the use of approximation techniques such as variational inference. Note. Here we can mention that p(z) and p(x|z) are both Gaussian distribution. So, if we had E(x|z) = f(z) = z, it would imply that p(z|x) should also follow a Gaussian distribution and, in theory, we could â€œonlyâ€ try to express the mean and the covariance matrix of p(z|x) with respect to the means and the covariance matrices of p(z) and p(x|z). However, in practice this condition is not met and we need to use of an approximation technique like variational inference that makes the approach pretty general and more robust to some changes in the hypothesis of the model. Variational inference formulation In statistics, variational inference (VI) is a technique to approximate complex distributions. The idea is to set a parametrised family of distribution (for example the family of Gaussians, whose parameters are the mean and the covariance) and to look for the best approximation of our target distribution among this family. The best element in the family is one that minimise a given approximation error measurement (most of the time the Kullback-Leibler divergence between approximation and target) and is found by gradient descent over the parameters that describe the family. For more details, we refer to our post on variational inference and references therein. Here we are going to approximate p(z|x) by a Gaussian distribution q_x(z) whose mean and covariance are defined by two functions, g and h, of the parameter x. These two functions are supposed to belong, respectively, to the families of functions G and H that will be specified later but that are supposed to be parametrised. Thus we can denote So, we have defined this way a family of candidates for variational inference and need now to find the best approximation among this family by optimising the functions g and h (in fact, their parameters) to minimise the Kullback-Leibler divergence between the approximation and the target p(z|x). In other words, we are looking for the optimal g* and h* such that In the second last equation, we can observe the tradeoff there exists â€” when approximating the posterior p(z|x) â€” between maximising the likelihood of the â€œobservationsâ€ (maximisation of the expected log-likelihood, for the first term) and staying close to the prior distribution (minimisation of the KL divergence between q_x(z) and p(z), for the second term). This tradeoff is natural for Bayesian inference problem and express the balance that needs to be found between the confidence we have in the data and the confidence we have in the prior. Up to know, we have assumed the function f known and fixed and we have showed that, under such assumptions, we can approximate the posterior p(z|x) using variational inference technique. However, in practice this function f, that defines the decoder, is not known and also need to be chosen. To do so, letâ€™s remind that our initial goal is to find a performant encoding-decoding scheme whose latent space is regular enough to be used for generative purpose. If the regularity is mostly ruled by the prior distribution assumed over the latent space, the performance of the overall encoding-decoding scheme highly depends on the choice of the function f. Indeed, as p(z|x) can be approximate (by variational inference) from p(z) and p(x|z) and as p(z) is a simple standard Gaussian, the only two levers we have at our disposal in our model to make optimisations are the parameter c (that defines the variance of the likelihood) and the function f (that defines the mean of the likelihood). So, letâ€™s consider that, as we discussed earlier, we can get for any function f in F (each defining a different probabilistic decoder p(x|z)) the best approximation of p(z|x), denoted q*_x(z). Despite its probabilistic nature, we are looking for an encoding-decoding scheme as efficient as possible and, then, we want to choose the function f that maximises the expected log-likelihood of x given z when z is sampled from q*_x(z). In other words, for a given input x, we want to maximise the probability to have xÌ‚ = x when we sample z from the distribution q*_x(z) and then sample xÌ‚ from the distribution p(x|z). Thus, we are looking for the optimal f* such that where q*_x(z) depends on the function f and is obtained as described before. Gathering all the pieces together, we are looking for optimal f*, g* and h* such that We can identify in this objective function the elements introduced in the intuitive description of VAEs given in the previous section: the reconstruction error between x and f(z) and the regularisation term given by the KL divergence between q_x(z) and p(z) (which is a standard Gaussian). We can also notice the constant c that rules the balance between the two previous terms. The higher c is the more we assume a high variance around f(z) for the probabilistic decoder in our model and, so, the more we favour the regularisation term over the reconstruction term (and the opposite stands if c is low). Bringing neural networks into the model Up to know, we have set a probabilistic model that depends on three functions, f, g and h, and express, using variational inference, the optimisation problem to solve in order to get f*, g* and h* that give the optimal encoding-decoding scheme with this model. As we canâ€™t easily optimise over the entire space of functions, we constrain the optimisation domain and decide to express f, g and h as neural networks. Thus, F, G and H correspond respectively to the families of functions defined by the networks architectures and the optimisation is done over the parameters of these networks. In practice, g and h are not defined by two completely independent networks but share a part of their architecture and their weights so that we have As it defines the covariance matrix of q_x(z), h(x) is supposed to be a square matrix. However, in order to simplify the computation and reduce the number of parameters, we make the additional assumption that our approximation of p(z|x), q_x(z), is a multidimensional Gaussian distribution with diagonal covariance matrix (variables independence assumption). With this assumption, h(x) is simply the vector of the diagonal elements of the covariance matrix and has then the same size as g(x). However, we reduce this way the family of distributions we consider for variational inference and, so, the approximation of p(z|x) obtained can be less accurate. Contrarily to the encoder part that models p(z|x) and for which we considered a Gaussian with both mean and covariance that are functions of x (g and h), our model assumes for p(x|z) a Gaussian with fixed covariance. The function f of the variable z defining the mean of that Gaussian is modelled by a neural network and can be represented as follows The overall architecture is then obtained by concatenating the encoder and the decoder parts. However we still need to be very careful about the way we sample from the distribution returned by the encoder during the training. The sampling process has to be expressed in a way that allows the error to be backpropagated through the network. A simple trick, called reparametrisation trick, is used to make the gradient descent possible despite the random sampling that occurs halfway of the architecture and consists in using the fact that if z is a random variable following a Gaussian distribution with mean g(x) and with covariance H(x)=h(x).h^t(x) then it can be expressed as Finally, the objective function of the variational autoencoder architecture obtained this way is given by the last equation of the previous subsection in which the theoretical expectancy is replaced by a more or less accurate Monte-Carlo approximation that consists, most of the time, into a single draw. So, considering this approximation and denoting C = 1/(2c), we recover the loss function derived intuitively in the previous section, composed of a reconstruction term, a regularisation term and a constant to define the relative weights of these two terms. Takeaways The main takeways of this article are: dimensionality reduction is the process of reducing the number of features that describe some data (either by selecting only a subset of the initial features or by combining them into a reduced number new features) and, so, can be seen as an encoding process autoencoders are neural networks architectures composed of both an encoder and a decoder that create a bottleneck to go through for data and that are trained to lose a minimal quantity of information during the encoding-decoding process (training by gradient descent iterations with the goal to reduce the reconstruction error) due to overfitting, the latent space of an autoencoder can be extremely irregular (close points in latent space can give very different decoded data, some point of the latent space can give meaningless content once decoded, â€¦) and, so, we canâ€™t really define a generative process that simply consists to sample a point from the latent space and make it go through the decoder to get a new data variational autoencoders (VAEs) are autoencoders that tackle the problem of the latent space irregularity by making the encoder return a distribution over the latent space instead of a single point and by adding in the loss function a regularisation term over that returned distribution in order to ensure a better organisation of the latent space assuming a simple underlying probabilistic model to describe our data, the pretty intuitive loss function of VAEs, composed of a reconstruction term and a regularisation term, can be carefully derived, using in particular the statistical technique of variational inference (hence the name â€œvariationalâ€ autoencoders) To conclude, we can outline that, during the last years, GANs have benefited from much more scientific contributions than VAEs. Among other reasons, the higher interest that has been shown by the community for GANs can be partly explained by the higher degree of complexity in VAEs theoretical basis (probabilistic model and variational inference) compared to the sim
Yash Prakash Dec 20Â·6 min read When I first heard about this powerful AI library that everyone seemed to be talking about, I was intrigued. FastAI â€” as its name stands, boasts to help coders deep dive into the vast and complicated world of deep learning in just a few lines of code and an extremely minimal setup too. Needless to say, I was pretty pumped to get my hands dirty and start experimenting with it a little. If youâ€™d like to get ahead and see all the code in one go, hereâ€™s the GitHub repo I put it all in: https://github.com/yashprakash13/RockPaperScissorsFastAI The installation on both Google Colaboratory as well as locally is very simple. It just takes one simple line of pip install fastai in the terminal/code block and youâ€™re done! Note: Do make sure youâ€™re inside a new virtual environment when you install fastai and open the jupyter notebook to start writing your model! Itâ€™ll help a lot in managing the dependencies and later, if you want, you can even package it as a production ready model with just a bit of extra work. Reading the book which is also provided freely in the convenience of Jupyter Notebooks was a great way to get started; I had already read the first three chapters before I got into writing my own model, utilising a pretty popular dataset from Kaggle for my convolutional neural network based image classification task. The dataset I wanted to try my hands on was the Rock, Paper, Scissors dataset. It has a reasonable amount of images categorised into three respective folders of train, test and validation which can easily be read by the powerful high level APIs provided by fastai. Believe me, I was pretty surprised to see how easy it was to import data through their DataBlock API â€” if youâ€™re coming from Keras, from my personal experience, this process feels much simpler than the Keras one. Let me now get into the mechanics of the steps I went through to make my first model and see for yourself how simple it all is! The steps to get started Hereâ€™s the data I used for this little project: https://www.kaggle.com/sanikamal/rock-paper-scissors-dataset Importing the vision modules from fastai is the first thing I did: The next step was to create something called a â€˜Pathâ€™ object to indicate where your data is. For example, I wrote it like this: This process is pretty intuitive, you can in fact view your folders of data through the ls function too: Now we get into supplying the data for our model through the DataBlock API I mentioned earlier. Yes, it is that simple! If your data is arranged into folders like this: Then importing the images with required labels and train + validation split is all done with just this single block of code! Breaking it down a little: fastai takes the names of the grandparent folders â€” the train and valid folders â€” as the train and validation split for our dataset through the argument â€˜splitterâ€™, and parent folders â€” as the x and y for our dataset, the images being the â€˜xâ€™ and the rock, paper, scissors folder names being the â€˜yâ€™. And thatâ€™s it! Our data is now ready to be consumed by a model. And not only that, the DataBlock API also gives access to a variety of data augmentation functions as well, which I will be exploring in my future articles. One of the first things Jeremy Howard, the co-founder of fastai, teaches in his book is to use the effectiveness of transfer learning for image classification. If youâ€™re new to the world of ML, donâ€™t worry about it too much. In the book, the introduction to using the power of transfer learning through the large pre-trained models is taught very well, and in simpler, non-mathematical terms too, so itâ€™s really convenient to learn from there instead of many many other online resources) and especially if you are as bad at learning ML math as me :P Loading a pre-trained model in fastai is even simpler. It looks like this: Here, as you can see, weâ€™re using the resnet34 pre-trained model for our purposes, and the accuracy after each evaluation on the validation set will be printed for our reference â€” to see how good or bad our model is performing. We can also see the model architecture in detail with one simple line: Learning can then be initiated with this line: One of the most useful methods of viewing how good your model is performing on the validation set via the Confusion Matrix. This is also very simple to do: This gives the confusion matrix for the three classes of images we have here. There are plenty of methods to customise the appearance of the matrix as well, display useful heatmaps for accuracy, etc. Concludingâ€¦ All of this seemed pretty surreal to me when I first read the chapters and tried to attempt it myself. The power of fastai seems unmatched in the current world of AI â€” the effortless execution of neural networks in this simple project of mine is a testament to that. In the book, they also explain the in-depth, under-the-hood details of most of the algorithms used very commonly in machine learning, for example, the stochastic gradient descent algorithm and the back propagation algorithm. As I continue on my journey to explore and learn more, it seems Iâ€™ll be doing some very interesting projects in the coming weeks, and if youâ€™re here, after having read all the way through, Iâ€™ll encourage you to embark on it with me! I donâ€™t remember the last time I was this excited about learning a new library, one that also encompasses all aspects of deep learning too! The resources Iâ€™ll recommend for you to look up if youâ€™re willing to learn more: I will be documenting quite frequently about my journey with fastai, so stay tuned if you like. I also publish python and mobile app development focused articles on my own Medium Blog â€” This Code. Do check it out! ðŸ˜ Connect with me on Twitter.
Matt Bartlett May 19Â·5 min read On what appeared to be an otherwise regular quarantine Friday night with a few Old-Fashioneds, J.K. Rowling opened the floodgates to Crypto Twitter with a single tweet: Rowling was quickly engulfed with the full force of Crypto Twitter â€” literally thousands of replies, including every significant person in the cryptocurrency space. Seemingly everyone even vaguely interested in crypto chipped in, from Elon Musk to the @Bitcoin Twitter account. The result was predictable: It only took a few hours for Rowling to conclude that bitcoin and cryptocurrency were too confusing for her to understand. The responses were so intense that she even felt she wouldnâ€™t be able to log into Twitter again. Rowling might have asked her original question as a tipsy joke â€” and the future of cryptocurrency certainly does not ride on whether celebrities can grasp the underlying concept â€” but this was still a pretty embarrassing indictment of how the crypto community engages with the outside world. Cryptocurrency is far from mass adoption partly because both its users and leaders are terrible at communicating the basics of what cryptocurrency is and why it matters. Even a brief look at the replies to Rowling illustrate some of these big issues: Itâ€™s genuinely very hard to explain what bitcoin is in a succinct and accessible way. When crypto people do try to explain those basic concepts, they almost always talk from â€œcrypto landâ€ and not â€œreal person land,â€ using talking points that donâ€™t mean anything to anyone not already in the crypto scene. There is a self-defeating tendency by many people in crypto to shamelessly plug or shill their own product, degrading the credibility of the whole space and reinforcing preconceptions that crypto is scammy. We still need a succinct explanation for bitcoin As Rowling points out, most of the early responders drew on metaphors that were pretty lackluster at explaining what bitcoin actually is. Seriously, fire insurance? Itâ€™s fire insurance for your economic system? The truth is that bitcoin is a tough thing to describe to the average person, particularly given that most people know next to nothing about how the existing financial system works. The very simplest explanation â€” â€œinternet moneyâ€ â€” vaguely points people toward bitcoinâ€™s fundamentals, but the next layer of explanation (cryptography, scarcity, and mining) tends to see eyes glaze over. At the same time, itâ€™s important to make a good-faith effort to explain the basics of bitcoin and cryptocurrency. The absolute worst thing to do is denigrate the personâ€™s intelligence or suggest that they wouldnâ€™t be able to understand it â€” a number of people took this elitist and pointless approach with Rowling: Bitcoin actually is really complicated, and Twitter is not the best medium for explaining it. We can always do better with coming up with succinct, compelling ways to communicate its fundamentals. Avoid crypto talking points that donâ€™t mean anything to people in the real world Some members of the community are so deeply steeped in crypto subculture that their language can seem totally alien to anyone on the outside. Even bitcoinâ€™s most basic meme, â€œdigital gold,â€ doesnâ€™t necessarily mean anything to the average person â€” sure, theyâ€™ll know that gold is valuable, but that doesnâ€™t explain what bitcoin is. It takes some extra context around scarcity and the idea of a â€œstore of valueâ€ for that metaphor to make sense. However, digital gold is nothing compared to the more obscure talking points that were endlessly pumped out in response to Rowlingâ€™s tweet. One respondent skipped complete sentences altogether to parrot a series of bitcoinâ€™s greatest hits: This was, of course, a waste of time â€” for better or for worse, the reality is that these terms mean absolutely nothing to 99% of the general public, as Rowlingâ€™s confused response to the above tweet demonstrates. Itâ€™s not clear to the average person what it means for something to be â€œpermissionlessâ€ or â€œscarce but divisible,â€ let alone whether it is good or bad for something to be owned by a â€œcentralized source of power.â€ These reflect ideas that are intuitive and understandable to most people in the crypto space but are gibberish to those on the outside. Mass adoption will require a concerted effort to explain what these mean in real, human terms. Stop desperately plugging your own product It was pretty pathetic to see just how many leading figures in crypto seized the (flimsy) opportunity to shill their own product. Brian Armstrong, CEO of Coinbase and one of the biggest leaders in crypto, didnâ€™t even manage to direct the above sign-up tweet to the right person â€” he tweeted a fake account masquerading as Rowling. CZâ€™s tweet was even worse: Just to be clear â€” these are the CEOs of the two biggest exchanges in crypto embarrassing themselves in an effort to personally sign up a celebrity to their business. Itâ€™s impossible to imagine this happening in a more established industry like gaming or social mediaâ€”seriously, imagine Zuckerberg begging J.K. Rowling to sign up for Facebookâ€”and it degrades the whole space. Seeing cryptoâ€™s leaders act in this juvenile way reinforces preconceptions that the public has about crypto: that itâ€™s an industry of scammers looking to take your money. Thatâ€™s inaccurate, but it would be hard to see the replies to Rowling and not believe it. For cryptocurrency to genuinely enter the mainstream and become accessible to and adopted by the public, the community needs to work on its messaging. Yes, the concepts can be tricky to explain, and the learning curve is high. But the communityâ€™s exclusive language and infighting make that work more difficult than it needs to be. For bitcoin and cryptocurrency to break out of its public niche as a confusing speculative investment, the community has to get a bit more mature in how it engages with the real world. Scaring influential public figures away from the space is not the way to achieve mass adoption.
2 days agoÂ·6 min read Introduction BSN (Blockchain-based Service Network) is the blockchain infrastructure-as-a-service for decentralized applications (smart contracts) deployment and consumption. In my previous article I showed how to deploy a sample chaincode (from Hyperledger Fabric) on the BSN Testnet for Permissioned Services. We also saw how to access the chaincode deployed in BSN through BSN specific API. Besides permissioned blockchain platforms, BSN also provides Permissionless Services. In short, BSN provides gateway services to a number of permissionless blockchain networks. Through the gateway we can access our accounts and invoke functions of smart contracts deployed in these permissionless blockchain networks. In this article I will show how to create BSN Permissionless Service, and how to access the services through the BSN gateway. BSN Permissionless Service In my previous work I have introduced a quick overview on BSN itself and BSN Permissioned Service. You can refer to it for a quick introduction about BSN. Here I only make an introduction to the BSN Permissionless Service. As we all know, there are quite many mature permissionless blockchain networks running today. BSN Permissionless Service provides gateway services to a large number of them, and gives us a single management point to access them. BSN keeps growing the list of permissionless blockchain networks supported. As of today (December, 2020), we have seen the following in the list Ethereum (Mainnet, Ropsten) EOSIO (Mainnet, Testnet, Mainnet-dfuse) Tezos (Mainnet, Carthagenet) Nervos (Mainnet, Aggron) NEO (Mainnet, Testnet) Irisnet (Mainnet, Nyancat) Algorand (Mainnet, Testnet) Solana (Mainnet, Testnet) ShareRing (Mainnet, Testnet) BitYuan (Mainnet, Testnet) Polkadot (Mainnet, Westend) Oasis (Mainnet, Testnet) The service is quite straightforward, and uniform across these networks. For each project we select the network we wish to access. Project ID and Project Key (optional) will be received. These are the important components when we access the selected network. BSN also provides the gateway detail when accessing the service. This gateway information will be incorporated into our client applications. Demonstration of BSN Permissionless Service Create a Project for Ethereum Mainnet Here we walk through how to create a BSN Permissionless Service, and perform a simple REST API access to this network. We are using https://bsnbase.io. Register an account if you have not yet. Choose a plan to fit your case. In my case I am using the free plan, with three projects and limited daily requests and TPS. By selecting Permissionless Services in left panel you can see this dashboard. As we saw in bsnbase.io, there are three cities providing permissionless services. Here we select HongKong PCN (Public City Node). We create a new project, providing a project name and select the blockchain network we are going to access. Here we select Ethereum Mainnet. And we see the project is successfully created. We see the Project ID for our Eth-Mainnet project, and BSN provides Access Addresses when we access through REST API or Web Socket. We see no Project Key by default. In the Action we can enable Project Key, which is used with the x-api-key header when we access the gateway. For demonstration purposes we enable it. The creation is complete and the gateway is ready for use. We will do a quick test with this REST API. Access Ethereum Mainnet through BSN Permissionless Service We use Postman to access the service we just created. We simply obtain the latest block in Ethereum Mainnet. Note that we are using the Access Address above as the target. And we need a header x-api-key holding the Project Key we just enabled in our project. And here we get the result (0xafc7f6 = 11519990). We see the result match the explorer. We can access Ethereum Mainnet through this access address (gateway). Access BSN Permissionless Service With the Access Address and optional Project Key, we can access the network with our friendly SDK. Here I have two projects created, one for Ethereum Ropsten and one for Algorand Testnet. I am using JavaScript SDK to access gateways. I have deployed an ERC20 token contract in Ethereum Ropsten. We specify the Access Address of BSN is placed as HTTP Service Provider (line 2). Here we are using web3.js library. And here is the result As expected, if we use another gateway service, say Infura for Ethereum, we will get back the same result. Note the HTTP Provider is updated with Infura gateway (line 2). Everything is kept unchanged. And we get the same result. It is an obvious result as both Infura and BSN permissionless service are same in nature, to access the same Ethereum network (Ropsten here) and therefore we get back the same result. Similar result is observed in Algorand Testnet. Here I simply check the balance of a given account (shown as amount in the result). Using BSN Gateway (line 3) And the result is, When using PureStake gateway service (line 3), We again get back the same result. Statistics of Permissionless Service BSN also provides the statistics for what we have demonstrated. Here we see the transactions we made for this demonstration on the projects. Summary BSN provides a single place to create and manage gateway services to multiple permissionless blockchain frameworks. We have demonstrated how to create a project of Permissionless Service in BSN, and how to check account balance (Algorand Testnet) and access a token contract (Ethereum Ropsten) through the BSN gateways. We see the same result whether we are using BSN gateway, or accessing the networks through other gateway services (PureStake in Algorand, Infura in Ethereum). Hope you have got more ideas about BSN services, both Permissioned Services in my previous article, and Permissionless Services here.
Aug 19Â·7 min read â€œAI allows processing of big data, whereas Blockchain offers security, immutability & decentralised data storage. The combo of these two tech giants makes smart contracts smarter, as they play their role in the 4th industrial revolutionâ€ Now, thanks to state-of-the-art tech, the implementation of AI-enabled, Blockchain-based smart contracts, is a reality. The disruptive melange of AI and Blockchain (the tech behind smart contracts), is a phenomenon which can miraculously streamline enterprise management and compliance way beyond our legacy systems. And while bad actors have tried to take advantage of AI and Blockchainâ€™s â€˜buzzwordâ€™ status, when we put a spotlight on their enormous capacity and potential, it is clear that they have significant transformative potential. Moreover, although each of these innovations can function autonomously: â€œAI and Blockchain also complement one another in such ways that their combination offers business solutions, not only the ability to build upon legacy enterprise systems, but also the power to eventually upend them in favour of next level solutions.â€ The complexities of smart contracts â€œSimple and easy to write contracts appear to be sufficient for many entirely digital transactions, but as these systems start to interact with the physical world, there is likely to be a need for greater intelligence and real world knowledge in making decisions.â€ Artificial Intelligence systems will be essential for translating data from a broad assortment of sensors, offering them in accurate terms which smart contracts can act on. Conversely, contracts which result in physical actions (e.g. delivering products), need to interface with robotic agents and human beings. For instance: operators and owners of critical energy infrastructure could require insurance contracts against harmful weather conditions and cyber-attacks; so to that end, a smart contract would have to ascertain when the payout event would be activated. The powerhouse of AI AI tech that can be utilised for smart contracts, extends from rule-based systems, for instance: expert systems engineered to make decisions based on input and rules, to more adaptive systems, such as logic, neural graphs and neural networks. Artificial Intelligence generates and executes intricate smart contracts by powering vital analysis; thereby rendering them more functional. And the more information that AI has, the better its predictions. To that end, when it comes to contract negotiation and finding the best path to secure an agreement: Artificial Intelligence has the means to examine previous negotiations in order to establish the way parties negotiated in the past. As a result, AI can propose the types of clause and language which would be the most successful for securing an agreement. Moreover, its examination of previous contracts allows it to pinpoint variables which were not previously considered, and then integrate them into future contracts. As a smart contract works as a self-executing document which directs workflow, Artificial Intelligence can also utilise former smart contracts in order to study the strength of past workflows and determine possible improvements that can be used later down the line. Watch â€œKEYNOTE + Q&A Blockchain, Smart Contracts and Artificial Intelligenceâ€ on YouTube at: Real market solutions â€œReal market solutions for these technologies have started to come online, although regulatory opaqueness hurdles aboundâ€ Miraculous real market solutions for Blockchain and AI, will take time, and naturally, there will be a cost attached to it. But, two words: â€œRegulatory frameworksâ€ stand in the way of getting things moving at full thrust. Yet, if this endeavour is successful, as predicted, then the pay-off promises to be extremely plentiful. To that end, addressing this constraint as soon as possible, is imperative. Therefore, at the present time: â€œas innovators seek to exploit the convergence of AI and Blockchain innovations, they must pay careful attention to overcome both technical and regulatory hurdles that accompany them.â€ Ying & yan AI and Blockchain tech seem to be diametric opposites, yet there is a vital connection â€” data. AI is an active form of tech: it carries out an analysis of what is around; and comes up with solutions based on the history what itâ€™s been exposed to. Conversely, Blockchain, which has become ever more popular over the last 6 years, is a largely passive bundle of tech: when anything is written into the network, its cryptographically-secured blocks act in a data-agnostic fashion. Due to this balance, each technology bumps up the strengths, and modifies the weaknesses of the other. In and of itself, Blockchain tech is not able to access the truth of the data which is written into its changeless network. Conversely, Artificial Intelligence can play the role of a knowledgeable gatekeeper, with regard what information comes on and off the network, and from whom. Looking to the future, it seems probable that the interaction between these two diverse capabilities will result in improvements across a vast spectrum of industries, many of which will have unique challenges, that this super technology duo could defeat. How Blockchain can transform Artificial Intelligence Many of the obvious disadvantages of AI and Blockchain can be effectively dealt with by merging both tech eco-systems. AI algorithms are reliant on information or data in order to study, deduce, and arrive at final decisions. â€œThe machine learning algorithms work better when data is collected from a data repository or a platform that is reliable, secure, trusted, and credible. Blockchain serves as a distributed ledger on which data can be stored and transacted in a way that is cryptographically signed, validated, and agreed on by all mining nodes.â€ Indeed, the Blockchain data is deposited with high level integrity and resilience, moreover, it cannot be interfered with. When smart contracts are utilised for machine learning algorithms to perform analytics and come to decisions, the consequences of such decisions cannot be disputed. To that end, this AI and Blockchain twosome can generate a decentralized, immutable and secure system for the extremely sensitive data that Artificial Intelligence-driven systems have to accumulate, deposit, and utilise. â€œThis concept results in significant improvements to secure the data and information in various fields, including: medical, personal, banking and financial, trading, and legal data.â€ Getting smart â€œWith a potentially larger pool of data to work from, the machine learning mechanisms of a widely distributed, Blockchain-enabled & AI-powered solution, could improve far faster than that of a private data AI counterpartâ€ Just as with Artificial Intelligence, if it is coded with particular automated processes, a Blockchain can rev up its record-keeping abilities. In fact, it could be said that these types of processes have helped bring about the successful rise of Blockchain. From the perspective of some, the latter started with the initiation of the Ethereum network, and its engineering around â€˜smart contracts.â€™ Note: a smart contract refers to a self-executing contract which has the terms of the agreement between buyer and seller directly written into lines of (computer) code. The agreements and code found therein, are prevalent across a distributed, decentralized Blockchain network. Moreover, the code executes the execution of the agreement in full or part, and transactions are irreversible and trackable. Of note, smart contracts are neither â€˜smart,â€™ in regard to employing AI applications; or â€˜contractsâ€™ in the sense of legally binding agreements. Instead, they comprise coded automated parameters which are responsive to anything that is recorded on a Blockchain. For instance: if the parties within a blockchain network have shown that by starting a transaction, particular parameters have been attained, then the computer code will execute the step/steps that have been set off by these coded parameters. â€œSmart contracts are gaining prominence because of the emergence of Blockchain tech. There is now more acceptance of the verification of transactions on public & private Blockchainsâ€ While smart contracts have been known for performing somewhat rudimentary tasks, this scenario is due to dramatically change. Indeed: â€œas developers figure out how to expand their networks, integrate them with enterprise-level technologies, and develop more responsive smart contracts, there is every reason to believe that smart contracts and their decentralized applications (dâ€™Apps) will see increased adoption.â€ The legal angle According to the 2019, book: â€˜A global perspective on recent industry, regulatory and legal developments in the Blockchain spaceâ€™: â€œThere is a broad consensus amongst law firms that next generation â€œLegal Techâ€ â€” particularly in the form of Blockchain-based technologies and Smart Contracts â€” will have a profound impact on the future operations of all legal service providers.â€ Indeed, the legal industry has already been revolutionized by Legal Tech startups due to revving up the efficiency and the speed of conventional legal services, or completely substituting them with advanced technologies. The impact, direction and scope of these cutting-edge technologies, in addition to their integration with legacy systems and existing practices, pose many uncertainties. Moreover, there is an increasing demand for easy-to-use contracting solutions, and to that end, a workable legal frameworks to protect the users of such solutions, is essential. Check out â€œHow Smart Contracts Will Change the World by Olga Mack at TEDxSanFrancisco Wrapping up By adding Artificial Intelligence to the already established Blockchain-enabled smart contracts, the efficiency of the latter is exponentially increased. When Blockchain and AI combine their strengths, this provides a more profound analysis of the effectiveness of the terms of the contract, and the workflows it supervises. Thus, the need for human analysis, intervention and verification, is greatly reduced. The cutting-edge dynamic duo of Artificial Intelligence and Blockchain, render the negotiation and execution procedure far simpler. Moreover, they also deliver more complex agreements â€” so the sum total of these benefits result in greater efficiency, making it a win-win situation all round. Further, it is important to be mindful of the fact that this inherent partnership propels the arena of smart contracts into an completely new sphere, and as such, businesses and legal professionals must up their game and be readyâ€¦
Dec 18Â·6 min read Introduction In the previous work we have demonstrated how to use JavaScript SDK to open accounts and transfer Algos between them in Algorand. Here we further demonstrate another important feature about digital assets in Algorand, known as Algorand Standard Asset (ASA), and see how to use JavaScript SDK to create, manage and delete digital assets. We will also use AlgoSigner, a browser plug-in wallet for Algorand recently released by PureStake, to perform transfer of digital assets between accounts. Quick Overview of ASA and AlgoSigner For those having experience in Ethereum, digital assets (always known as tokens) are implemented through (smart) contract, riding on top of an Ethereum network. This is called Layer-2 implementation. Algorand is taking a different approach, making digital assets as states of any given accounts. It is called Layer-1 implementation. When we inspect the account information of a given account, we can see the balance of Algos, the native currency in Algorand, and all the digital assets held in this account. This means that we do not need to deploy a token contract on Algorand, and can perform the lifecycle of digital assets directly on Algorand, including creation, transfer and destruction. We are showing this here with JavaScript SDK. ASA in fact comes with very comprehensive capabilities, good for both fungible and non-fungible tokens. This is not covered in this article. You can explore more about ASA here. Meanwhile, PureStake recently released AlgoSigner, a chrome plug-in as an Algorand wallet. For those familiarized with Metamask on Ethereum, this is the right way to understand AlgoSigner on Algorand. In this demonstration we will use AlgoSigner to perform digital asset transfer. Source This work is largely based on a complete example by Ryan Fox (link). His work can be found here. For better illustration, I broke it down into steps. Besides, when ASA transactions are signed and submitted, Ryanâ€™s code provides a decent way to wait for confirmation and show the asset details. I am not using this here, but it is highly recommended when we are developing a better application. Here I just use a separate script to inspect the account information. Demonstration We create two accounts, each of which comes with the 25-word mnemonic. For simplicity we specify them as Alice and Bob. Here are the addresses of both Alice and Bobâ€™s account. In our demonstration we will perform the following tasks Those scripts are provided in each demo step when applicable. Before using them, make sure you update 25-word mnemonic for your own accounts, and those account addresses. The asset-ID for the asset you have created. Over this demonstration, we keep checking ASA from both Alice and Bobâ€™s account. Here is the script asa_checkBoth.js to show the assets in both accounts. As seen at the beginning, both Alice and Bob do not have any assets defined. Step 1: Create ASA Here is the script asa_aliceCreate.js to let Alice create KCCOINs. We are using algosdk.makeAssetCreateTxnWithSuggestedParams to construct a transaction for asset creation (line 29). Note the Asset ID (here is 13300122). We will use this in other scripts. Step 2: Transfer of ASA with Receiver not Opt-in Here is the script asa_aliceSendToBob.js to perform a transfer of 200k KCCOINs from Alice to Bob. We are using algosdk.makeAssetTransferTxnWithSuggestedParams to construct a transaction for asset transfer (line 24). When we execute this script and see the error message: Bobâ€™s account (BDGâ€¦Y4M) does not opt-in yet (that asset is not in Bobâ€™s account yet). Step 3: Opt-in and Successful Transfer of ASA The script asa_bobOptIn.js makes Bob opt-in to KCCOIN. Note that the opt-in is done by sending a transaction to oneself with the asset ID and amount zero (line 15, 17, 18 and 22). After opt-in, we see the asset is now in Bobâ€™s account, with amount zero. Now we execute this script to transfer 200k KCCOINs from Alice to Bob (previous script). It is successful. And the balance is updated. Step 4: Observation in AlgoSigner I have imported both Alice and Bobâ€™s account into AlgoSigner. Here is the current status. We see Alice and Bob have one asset (ASA). And here are the details of both Alice and Bob. The right balance of KCCOIN is shown here in both accounts. Step 5: Alice sends 100k KCCOINs to Bob with AlgoSigner We choose Alice account, click Send, and specify Bobâ€™s address with the amount and the selected ASA (KCCOIN here). A wallet login is required as approval. Now we can check the balance again. The transfer is properly processed. Step 6: Destroy ASA with Outstanding ASAs Alice now destroys this asset with script asa_aliceDestroy.js. Note that as there are still outstanding KCCOINs in other accounts, the deletion fails. We are using algosdk.makeAssetDestroyTxnWithSuggestedParams to construct a transaction for asset destruction (line 19). We see the error here. Creator (Alice) does not have all the KCCOINs and therefore is unable to destroy this asset. Creator needs to hold all the amount issued before this asset can be destroyed. Step 7: Return all ASA to Creator We use AlgoSigner again to have Bob sending back 300k KCCOINs to Alice. And after transaction is processed, we can check the balance again for both accounts. Now all the ASAs are back to the creator. Creator (Alice) can destroy KCCOINs. Step 8: Destroy ASA Now Alice can destroy ASA. It is interesting to note that the ASA is removed from Alice the creator. However, this transaction has no way to remove opt-in from other accounts. That is the reason we see the ASA still in Bobâ€™s account despite the zero amount. Step 9: Remove Opt-in We use asa_bobRemoveOptIn.js to remove Bobâ€™s opt-in. We are using a transaction with closeRemainderTo set to the creator to construct a transaction for opt-in removal (line 22 and 25). With this transaction being processed, we see no more asset in Bobâ€™s account. This ends our demonstration. Summary We showed how to use JavaScript SDK to perform the basic lifecycle of digital assets in Algorand. We also used AlgoSigner, a chrome plug-in Algorand wallet, to make asset transfers. As mentioned before, ASA provides lots of options and feel free to explore more here. Besides, here is the complete code on JavaScript developed by Ryan Fox.
Bret Waters Nov 22Â·3 min read Gold has been used as a store of value for all of recorded history. It holds its value forever, and isnâ€™t subject to the fluctuations and political risk associated with fiat currency. Most of us know that a good balanced investment portfolio includes some gold (it has often outperformed the stock market), but the average person doesnâ€™t have an easy, efficient, and legal way to buy and store physical gold. So Ashraf Rizvi and his team decided to tackle this problem, and democratize physical gold ownership. Riz (as everyone calls him) is a Wharton grad who went on to be Global Head of Commodities Trading at UBS Investment Bank. He knows that wealthy people can easily buy and store gold as part of their diversified investment portfolio â€” one day he thought to himself, â€œWhy not bring that bring that access to all?â€ So Riz put together an all-star team and founded Digital Swiss Gold (DSG), a new fintech digital platform. Using their mobile application consumers can buy as little as a gram of gold (approx $60 USD). You actually own the gold, and it is stored for you in a vault in Switzerland. Blockchain technology is used to track, validate, and audit transactions. Itâ€™s as easy as a digital currency, except that itâ€™s actual physical gold ownership. Safe and secure. Digital Swiss Gold is headquartered in the US (and Jersey), but has chosen India as their first market. India is a market of 1.3 billion people, eager to access gold ownership. Just a month after launch, Digital Swiss Gold already has had over 30,000 app downloads. Riz and his team have also launched a US brand, Gilded, using the same safe-and-secure digital platform to provide gold ownership to American consumers and investors. Every transaction is recorded on a private permissioned blockchain and independently verified for audit and assurance. From a regulatory standpoint, one of the challenges of this business is that itâ€™s subject to Know Your Customer (KYC) and Anti-Money Laundering (AML) laws. Digital Swiss Gold has turned this into a competitive advantage, as they have developed and integrated a digital workflow that makes it 100% compliant with these regulations. Many companies make customers jump through all sorts of hoops in order to meet KYC and AML regulations before they can make a purchase. With Digital Swiss Gold itâ€™s a fast and efficient process, right within the app. Democratizing access to physical gold ownership isnâ€™t an easy issue to tackle. There are these regulatory compliance issues plus the logistical complexity of sourcing the gold, tracking it, and storing it in Swiss vaults. Then thereâ€™s the whole digital layer, with complexity from Blockchain to the mobile app. Turning all that into a seamless consumer user experience that is safe and secure takes a pretty amazing team of professionals. Which is exactly what Riz has assembled. Digital Swiss Gold was in the 4thly Global Startup Accelerator program.
Dec 2Â·6 min read Building Information Modeling (BIM) gives architecture, engineering, and construction (AEC) professionals insight and tools to more efficiently plan, design, construct, and manage buildings and infrastructure. However, payments are a challenge and there is a lot of paperwork involved. BIM software should not be limited to generating 3D models and could serve to compile all project information and documentation, such as change orders, invoices, and payment records. Blockchain can address issues surrounding secure access to the model and allow for a reliable audit of who made changes, when they were made, and what those changes were. Contractual processes that typically require human intervention and oversight can be partially or fully automated with smart contracts, originating from blockchain technology. In this article, I explore technological advancements that are disrupting the AEC industry and share my takeaways about the convergence of blockchain and the industry so far. What Is Blockchain? Blockchain is a decentralized universal ledger that runs off a network of computers that jointly manage a database. It also fosters smart contracts. Whenever an entry is made, it gets added on as a block in an already existing chain of entries after being authenticated by everyone in the blockchain. This chain is also protected by the best cryptography algorithms available, so itâ€™s very difficult to hack. All the blocks are immutably linked to the previous block. So, if a hacker wants to change one block, he/she has to change all the blocks in the chain, which is next to impossible. Also, as this is a chain, we can trace back any event block by block. So, super security comes with this technology. Letâ€™s say there are 10 people who decide to contribute $1,000 each to a lucky draw. The winner of the draw gets $10,000. For this draw, nobody is in higher authority and there is no bank. If you entered the draw, who do you trust? You have to trust every other participant equally and you are also aware if they want to defraud you, most or all would have to be convinced for an action like that. Therefore, the concept of centralized trust changes to distributed trust. This distributed trust is the heart and soul of blockchain. How Does Blockchain Relate to Construction? BIM over the last decade and a half has totally transformed the AEC industry. How the AEC industry works now is totally different from how it worked during the 1990s when one compares some aspects of designing, drawings generation, communication, collaboration, and information exchange. BIM gives AEC professionals insight and tools to more efficiently plan, design, construct, and manage buildings and infrastructure but payments are a challenge and there is a lot of paperwork involved. BIM software should not be limited to only generating 3D models but could also be served to compile within the software all project information and documentation, such as change orders and invoices and payment information. Blockchain can address issues surrounding secure access to a BIM model and allow for a reliable audit of who made changes, when they were made, and what those changes were. Contractual processes that typically require human intervention and oversight can be partially or fully automated with smart contracts, originated from blockchain technology. Smart contracts refer to computer protocols that set out the terms and conditions of a contract. A smart contract helps the contractor to get paid immediately once the work is completed, after being verified by all the parties, automatically without any invoicing process. Regularly all the information regarding status, changes, etc., is sequentially uploaded and compiled into immutable and time-stamped blocks. These blocks are linked together and the linked information is constantly verified on a peer-to-peer basis by other linked block users in the blockchain. This eliminates the need for a third party to verify the accuracy of linked information and serves as a single source of truth. Effectively, once a block of information is created in this chain it can never be altered without the knowledge of other users in the chain. It is also very simple to verify when a block was created and by whom, and when changes have been made in the blockchain. What if you never had to submit an invoice again, but still got paid, instantly, the minute you finished your next job? Thatâ€™s the kind of promise blockchain technology holds for the AEC industry. For example, if a steel fabricator is ready to ship the steel components to the job site, he would log this information in the BIM software. The smart contract is linked to the BIM model and the project account is funded by the owner. Once the components have been delivered to the job site, the project manager would confirm having received the component within BIM. Automatically funds would get transferred from the project account to the steel fabricatorâ€™s account. The exchange of invoices and supplementary documentation in support of a payment claim could also be completed automatically depending on the extent to which such functions are programmed within the smart contract. Where supply chain management is concerned, if your supply chain is not in sync, then your project is going to suffer. There will likely to be delays, which will lead to a lapse in productivity, cost overruns, and an unhappy client. Blockchain in this context can help trace physical items from origin to final destination. It can help improve transparency, which in turn would help involved parties stay on the same page and avoid potential pitfalls and oversights. Challenges to Blockchain Implementation Will blockchain be one of those technologies implemented overnight by construction companies? Most likely, no. In fact, thereâ€™s a long way to go, as the digital revolution in construction is just beginning. Thereâ€™s a long road ahead before blockchain becomes the norm in construction. Hereâ€™s a look at why blockchain in construction still has some obstacles to overcome: Skepticism-Even the most tech-savvy contractors might be hesitant to adopt blockchain into their day-to-day operations. Culture-Blockchain technology is a nontraditional approach to asset transactions. It isnâ€™t easy to adapt, even for the organizations that are now digitized. It takes time, effort, and outstanding knowledge transfer to accomplish it. Lack of resources-In order to implement blockchain into everyday operations, a variety of complex systems need to be created. This comes at a cost, for the systems and to hire people required to create and implement them. Market readiness-Is the market ready for blockchain in construction? Not right now. It needs time to mature to a point where itâ€™s more of a reality and less of a pipe dream. Cost and efficiency-Blockchain technology is quite competent in cost reduction. But it still faces specific challenges while implementing the legacy systems. Setting up the initial blockchain infrastructure is expensive. The Bottom Line Though the blockchain future holds significant potential, very little can happen until the challenges mentioned above are dealt with. With an uncertain future ahead, a united front among all the countries and a standard set of regulations will be crucial to implement and realize the potential of these technologies. What this boils down to is very simple: blockchain has value in construction because it introduces automation, reducing the burden of administrative and financial processes. These are known to slow construction projects down significantly. Blockchain would be the first technology of its kind to revolutionize these processes. And while the industry may not be ready yet for a full-court blockchain press, know that adoption of such technology is closer to a reality than a remote possibility. Looking towards the future, blockchain will be something that weâ€™ll be hearing a lot more about-and itâ€™s only a matter of time before it becomes a necessity in the AEC industry. Paras Taneja is passionate about the future of Virtual Design & Construction (VDC), utilizing gaming engines, BIM, and structural computational design. As a junior consultant at Ramboll, he is responsible for several areas which include technology development and applying the appropriate tools for the right job. References Z. Turk and R. Klinc. Potentials of Blockchain Technology for Construction Management. Procedia Engineering 196 ( 2017 ) 638â€“645. Accessed August 21, 2020. doi:10.1016/j.proeng.2017.08.052. How Blockchain Could Mould the Future of BIM and Construction. July 6, 2018. Accessed September 14, 2020. https://www.rib-international.com. Originally published at https://www.autodesk.com on December 2, 2020.
Alex Mucalov | JD/MBA, MSc Economics Dec 16Â·11 min read Last edited: December 15, 2020. [Please note: this is the last Part of an ambitious 3-part series, and it is a work in progress. It intends to invite discussion. If you have comments, feel free to post them. All contributions are welcome. Thanks!] Introduction Todayâ€™s private progress has coincided with growing public peril. Search â€œtop inventions in the 21st centuryâ€ on Google, and this is what youâ€™ll get. Nanobots. Flying drones. Birth control patches. Head-of-a-pin operating systems. Virtual reality. Particle accelerators. Digital currency. Gene editing. Artificial intelligence. 3D printing. Lab-grown pancreases. Reusable rockets to Marsâ€¦ and on and on, among the 6.33 million search results returned in 0.58 seconds. Search â€œtop issues of the 21st centuryâ€ on Google, and this is what youâ€™ll get. Global warming. Poverty. Nuclear proliferation. Fake news. Terrorism. Inequality. Famine. Obesity (?!). Deforestation. Population imbalances. Pandemics. Leadership crisesâ€¦ and on and on, among the 237 million search results returned in 0.70 seconds. So doing the math here, weâ€™re faced with nearly 40 times as many questions as answers â€” despite the fact that answers include the likes of â€œnanobotsâ€ and â€œlab-grown pancreases.â€ Indeed, we can do (dubious) math like this in mere seconds only thanks to the magic of equally mind-boggling internet search technology. Trite though observations like this may by now be â€” and pessimistic as some truly are â€” it still jars to juxtapose human progress with human perils. We can build rocketships for passenger service to Mars, yet we plunder what remains of our plant, air, water, and other scarce bounty back home on Earth. As private sector tech approaches escape velocity, some of our vital public needs seem to be left in the dust. We can break this phenomenon down. This series argues that the best approach to solving todayâ€™s big issues will be to couple individualsâ€™ self-interest with the global public good; and it proposes a way to do this. The series comes in three Parts. 1.The first paper (published previously) presented Part I, identifying the source of todayâ€™s biggest issues. We saw that government historically solved pressing public problems well â€” especially when partnering with the private sector â€” and that government couldnâ€™t have done this alone. But we also saw that todayâ€™s issues will be harder for governments to solve, largely because the private sector wonâ€™t be as helpful as it was in the past. We ultimately concluded that governments, firms, and individuals collectively express insufficient will and ability to address todayâ€™s biggest public issues â€” and especially those that require global coordination. 2. The second paper (published previously) presented Part II, introducing a framework to describe what yields collective will and ability to achieve desired outcomes in a given human system; and it used this framework to identify a class of solutions promising to provide greater focus in solving public problems. In Part II, we saw that focus can be improved by reorienting incentives, capabilities, and power relationships among societyâ€™s key actors. We were drawn to conclude that coupling individual incentives more tightly with the global public good would be the single most effective and principled way to bring greater focus to addressing global issues. And we saw further that creating individual incentives that align with the global good could also reduce firmsâ€™ problematic and growing power over government as well, addressing both issues in a single stroke. 3. Finally, this third paper presents Part III, evaluating a particular idea that lives within the class of solutions arrived at in Part II. Iâ€™ll argue that we can create individual incentives to pursue the global public good without jeopardizing the time-tested profit motive, by putting them together. And weâ€™ll see that addressing the root cause of the collective action problems outlined in Part I could solve many of its manifestations presenting today around the world. Part I: Defining the Problem Space Why Todayâ€™s Big Problems Keep Getting Worse, and What We Can Do About It [Part I of III] How to keep capitalism and democracy working for us all medium.com Part II: Framing the Solution Space Why Todayâ€™s Big Problems Keep Getting Worse, and What We Can Do About It [Part II of III] How to keep capitalism and democracy working for us all medium.com Part III: Exploring One Possible Part of a Solution Part I concluded that governments, firms, and individuals collectively express insufficient will and ability to address todayâ€™s biggest public issues â€” and especially those that require global coordination. Part II concluded that coupling individual incentives more tightly with the global public good is perhaps the single most effective and principled way to better address global public issues. This Part III addresses the question, â€œHow do we create incentives that couple oneâ€™s individual private good with the global public good?â€ And, â€œHow do we do this in a way that doesnâ€™t require (politically-challenging) government action at the outset?â€ We can create individual incentives to pursue the global public good without jeopardizing the time-tested profit motive, by putting them together. Doubtless there are many ways to more tightly couple private incentives with the global public good; but naturally, we might think first in terms of dollars and cents. And propitiously, the advent of blockchain and decentralized digital currency technologies today enable solutions here that in prior times wouldnâ€™t have been possible. For example, today, itâ€™s now possible to create blockchain-based â€˜smart contractsâ€™ that can be programmed to execute the terms of (digital) financial contracts, while denying anyone â€” even contract creators â€” the ability to renege or invalidate these contracts, which live permanently and immutably on the blockchains that host them. And immutable digital contracts like these could, for example, tie private digital currency payouts to the achievement of global public outcomes â€” directly coupling private incentives with the achievement of the global public good. To take a more concrete example of this, one could imagine, for example, a blockchain-based digital â€œcoinâ€ whose value tracks, say, food security in states across Africa, Central America, and Central Asia. This â€œcoinâ€ could be programmed so that the less that people go hungry in a given calendar quarter, the more the coin is worth the next quarter; the more people go hungry, the less the coin is worth. Such a coin as described would inherently couple the private financial interests of the coinâ€™s buyers with global public outcomes â€” in this case, food security â€” with all the benefits promised by this tighter coupling, as explored in Part II. And itâ€™s already possible to create such digital coins, whose values track (global) public outcomes â€” the class of which we might term â€œAtlas Coins.â€ In fact, beta-versions of 3 such Atlas Coins have already been been created and deployed on the Ethereum blockchain, including the coin described above. With a few digital DAI â€œstablecoinsâ€ (whose value is pegged to the U.S. dollar) and an account to access the Ethereum blockchain, one can exchange DAI for â€œPlenty Coins,â€ which themselves pay a quarterly dividend that tracks food security in Africa, Central America, and Central Asia. Precisely as described above, Plenty Coin buyers realize greater returns when fewer go hungry in a given quarter, and lesser returns when more do. Now the real promise of a digital coin like this, weâ€™ll recall from Part II, is that if enough people become sufficiently invested in, for example, Plenty Coins, this could give rise to entirely new private sector product and labour markets aimed at improving the public good. The thinking here is straightforward. Plenty Coin holders get greater returns when food security is better in Africa, Central America, and Central Asia. So Plenty Coin holders could improve their returns by paying firms to improve food security in these places. And if Plenty Coin returns grow more than the fees charged by these firms, then Plenty Coin holders make an incremental gain. An example of this could look as follows. Suppose, for the sake of argument, that 1M people each purchase $10K worth of Plenty Coins. This means that buyers would have collectively purchased $10B worth of Plenty (1M x $10K). At typical food security levels, Plenty Coins yield roughly 4% returns, say; but along comes a firm that makes an enticing offer to Plenty Coin holders. The offer is this: â€œIf you pay us $40M each year, then weâ€™ll improve food security to the point where you earn an annual 10% return. Weâ€™ll improve food security by, among other things, providing local farmers with todayâ€™s best farming technology, helping to set up local irrigation systems, and ensuring sufficient crop supply to sow in seasons following periods of drought. You pay us only if we hit food security targets that yield 10% returns; otherwise, you pay us nothing.â€ Now, to the holders of Plenty Coins, this would be a good offer! A 10% return would be an incremental gain of 6% over and above the 4% typical of this Coin. And that 6% gain is worth $60M when buyers collectively possess $10B of the Coin. So even after Plenty Coin holders pay $40M to this enterprising food security firm, Plenty Coin holders will have netted $20M, equivalent to an incremental 2% on their total owned stock of Plenty. The offer entails no downside risk, since the firm charges nothing if targets arenâ€™t hit. And all of these terms could theoretically be designed to be programmatically executed on a given blockchain. And since firms like the one described above could make money by selling food security solutions to Plenty Coin holders, firms would form to serve this new customer base, competing to devise and execute the most effective food security solutions at the lowest cost to win contracts with Plenty Coin holders. So in this way, Plenty Coins would ultimately give rise to entirely new private sector product/service and labour markets aimed at improving food security. And thus more generally, Atlas Coins, such as Plenty Coin, have the potential, I believe, to give rise to entirely new product and labour markets aimed at improving the public good. Addressing this class of collective action problems could solve its many manifestations. Our world today faces countless cross-cutting global issues â€” global warming, health crises, global inequality, failed states, terrorism, leadership deficits, and so on, already well-enumerated in Part I and many other sources besides. We have seen, though, that all of these myriad global issues share a common thread, in that they are â€œcollective actionâ€ problems â€” a class of problems that are notoriously difficult to solve, because individuals do not face sufficient individual incentive to contribute to their resolution. But, after some reflection through Parts I, II, and III â€” on the kinds of problems the world faces today, how they contrast with yesterdayâ€™s issues, why we canâ€™t expect todayâ€™s institutions to deliver as effectively as yesterdayâ€™s, and what kinds of new institutions it would take to resolve these issues â€” we arrive ultimately at the simple and intuitive proposition: Perhaps we donâ€™t need to solve all the worldâ€™s problems. Perhaps we need to solve only one â€” â€œHow do we get enough people with enough clout to care?â€ And a solution of the kind proposed above â€” of Atlas Coins â€” has, I believe, much potential to help answer this question. A single Atlas Coin aimed at a pressing global issue, such as food security, has the potential to give rise to an entire ecosystem of for-profit companies whose paid mission it is, for example, to help make places sustainably better fed. And if the history of capitalism over the past 100 years has taught us anything, itâ€™s that human ingenuity and initiative are unlimited, when properly motivated. But thereâ€™s no reason why we need be limited to a single Atlas Coin. To be sure, we can imagine a financial ecosystem arrayed with many Atlas Coins, each carefully-designed, and established as a set to address the most important collective action problems of our day at every level â€” global, multinational, domestic, municipal. To take this a little further, were the idea of something like Atlas Coins to take root, we could even imagine accountable government bodies calibrating rates of return vis-a-vis public outcomes for different Atlas Coins, to incentivize appropriate relative effort in achieving various (global) public aims. Accountable bodies could periodically raise return rates for those Atlas Coins aimed at issues that are becoming more pressing, thus encouraging greater commitment; and it could lower rates for Atlas Coins whose public outcomes may happen to be decreasing in relative importance. (These mechanics would be analogous to how central banks today calibrate interest rates to achieve given national inflation and growth targets.) And Atlas Coins established as a set could improve their overall cost-effectiveness, given their inherent diversification generating less-correlated returns, and thereby reducing overall balance sheet maintenance costs. But arguably most importantly, having something like Atlas Coins more entrenched alongside traditional stock markets could give rise to an entirely new for-profit sector of the economy as described above â€” this one aimed explicitly at pursuing the public good. And this could give rise to entirely new mission-driven labour markets, potentially energizing an entire generation of labour market participants desperately craving both means and meaning in our professional lives. Indeed, something like Atlas Coins might even represent something more poetic than prosaic economics: it might even represent the democratization of public service itself, and the corresponding revitalization of shared purpose. For no longer would pursuit of the public good be restricted to todayâ€™s privileged class of civil servants. Instead, wage-earners everywhere could have the opportunity to direct their private, paid efforts to the betterment of all, through the provision of their labour to private sector firms that profit directly by improving the public good. And this could liberalize an entire class of needs heretofore attended to exclusively by national government elites. In the final analysis, individual incentives to achieve the (global) public good have the potential to yield significant benefits â€” a number of which arenâ€™t even listed here, for the sake of brevity. Complementary solutions to todayâ€™s collective action problems may also include the likes of government-built economic mechanisms to e.g. routinely support the pace of lifestyle-enhancing innovation, to naturally counteract the deleterious side effects of productivity-enhancing innovation, as discussed in Part II. But if individuals around the world were properly motivated and financially supported to pursue the global public good, a torrent of innovative ideas â€” many of which would surely be better than those first thoughts offered here â€” would doubtless flow from the infinite spring of human ingenuity. Atlas Coins, or something like them, illustrate perhaps one way that we might more tightly couple private incentives with the global public good. And successfully achieving that is perhaps the first step forward on the path to a brighter, more sustainable future for us all. [END OF PART III] [Thanks for reading! As always, please feel free to post questions or comments inline or in the comments section below. And to see a first attempt at implementing the idea of â€œAtlas Coinsâ€ as outlined above, please feel free to reach out to me on LinkedIn about the (unstable, pre-release) alpha-version of â€œAtlas Coins,â€ an app currently under development for use on iPhone. Thanks!] Alex Mucalov is an observer of human social systems. He has enjoyed diverse and direct exposure to some of democratic societyâ€™s key economic and political institutions through varied strategy experience in financial services, government, and regulatory bodies. He holds a JD/MBA from the University of Toronto, a Masterâ€™s in Economics from the London School of Economics and Political Science, and a Bachelorâ€™s in Commerce from Queenâ€™s University.
Sep 27, 2017Â·33 min read Introduction Odds are youâ€™ve heard about the Ethereum blockchain, whether or not you know what it is. Itâ€™s been in the news a lot lately, including the cover of some major magazines, but reading those articles can be like gibberish if you donâ€™t have a foundation for what exactly Ethereum is. So what is it? In essence, a public database that keeps a permanent record of digital transactions. Importantly, this database doesnâ€™t require any central authority to maintain and secure it. Instead it operates as a â€œtrustlessâ€ transactional system â€” a framework in which individuals can make peer-to-peer transactions without needing to trust a third party OR one another. Still confused? Thatâ€™s where this post comes in. My aim is to explain how Ethereum functions at a technical level, without complex math or scary-looking formulas. Even if youâ€™re not a programmer, I hope youâ€™ll walk away with at least better grasp of the tech. If some parts are too technical and difficult to grok, thatâ€™s totally fine! Thereâ€™s really no need to understand every little detail. I recommend just focusing on understanding things at a broad level. Many of the topics covered in this post are a breakdown of the concepts discussed in the yellow paper. Iâ€™ve added my own explanations and diagrams to make understanding Ethereum easier. Those brave enough to take on the technical challenge can also read the Ethereum yellow paper. Letâ€™s get started! Blockchain definition A blockchain is a â€œcryptographically secure transactional singleton machine with shared-state.â€ [1] Thatâ€™s a mouthful, isnâ€™t it? Letâ€™s break it down. â€œCryptographically secureâ€ means that the creation of digital currency is secured by complex mathematical algorithms that are obscenely hard to break. Think of a firewall of sorts. They make it nearly impossible to cheat the system (e.g. create fake transactions, erase transactions, etc.) â€œTransactional singleton machineâ€ means that thereâ€™s a single canonical instance of the machine responsible for all the transactions being created in the system. In other words, thereâ€™s a single global truth that everyone believes in. â€œWith shared-stateâ€ means that the state stored on this machine is shared and open to everyone. Ethereum implements this blockchain paradigm. The Ethereum blockchain paradigm explained The Ethereum blockchain is essentially a transaction-based state machine. In computer science, a state machine refers to something that will read a series of inputs and, based on those inputs, will transition to a new state. With Ethereumâ€™s state machine, we begin with a â€œgenesis state.â€ This is analogous to a blank slate, before any transactions have happened on the network. When transactions are executed, this genesis state transitions into some final state. At any point in time, this final state represents the current state of Ethereum. The state of Ethereum has millions of transactions. These transactions are grouped into â€œblocks.â€ A block contains a series of transactions, and each block is chained together with its previous block. To cause a transition from one state to the next, a transaction must be valid. For a transaction to be considered valid, it must go through a validation process known as mining. Mining is when a group of nodes (i.e. computers) expend their compute resources to create a block of valid transactions. Any node on the network that declares itself as a miner can attempt to create and validate a block. Lots of miners from around the world try to create and validate blocks at the same time. Each miner provides a mathematical â€œproofâ€ when submitting a block to the blockchain, and this proof acts as a guarantee: if the proof exists, the block must be valid. For a block to be added to the main blockchain, the miner must prove it faster than any other competitor miner. The process of validating each block by having a miner provide a mathematical proof is known as a â€œproof of work.â€ A miner who validates a new block is rewarded with a certain amount of value for doing this work. What is that value? The Ethereum blockchain uses an intrinsic digital token called â€œEther.â€ Every time a miner proves a block, new Ether tokens are generated and awarded. You might wonder: what guarantees that everyone sticks to one chain of blocks? How can we be sure that there doesnâ€™t exist a subset of miners who will decide to create their own chain of blocks? Earlier, we defined a blockchain as a transactional singleton machine with shared-state. Using this definition, we can understand the correct current state is a single global truth, which everyone must accept. Having multiple states (or chains) would ruin the whole system, because it would be impossible to agree on which state was the correct one. If the chains were to diverge, you might own 10 coins on one chain, 20 on another, and 40 on another. In this scenario, there would be no way to determine which chain was the most â€œvalid.â€ Whenever multiple paths are generated, a â€œforkâ€ occurs. We typically want to avoid forks, because they disrupt the system and force people to choose which chain they â€œbelieveâ€ in. To determine which path is most valid and prevent multiple chains, Ethereum uses a mechanism called the â€œGHOST protocol.â€ â€œGHOSTâ€ = â€œGreedy Heaviest Observed Subtreeâ€ In simple terms, the GHOST protocol says we must pick the path that has had the most computation done upon it. One way to determine that path is to use the block number of the most recent block (the â€œleaf blockâ€), which represents the total number of blocks in the current path (not counting the genesis block). The higher the block number, the longer the path and the greater the mining effort that must have gone into arriving at the leaf. Using this reasoning allows us to agree on the canonical version of the current state. Now that youâ€™ve gotten the 10,000-foot overview of what a blockchain is, letâ€™s dive deeper into the main components that the Ethereum system is comprised of: accounts state gas and fees transactions blocks transaction execution mining proof of work One note before getting started: whenever I say â€œhashâ€ of X, I am referring to the KECCAK-256 hash, which Ethereum uses. Accounts The global â€œshared-stateâ€ of Ethereum is comprised of many small objects (â€œaccountsâ€) that are able to interact with one another through a message-passing framework. Each account has a state associated with it and a 20-byte address. An address in Ethereum is a 160-bit identifier that is used to identify any account. There are two types of accounts: Externally owned accounts, which are controlled by private keys and have no code associated with them. Contract accounts, which are controlled by their contract code and have code associated with them. Externally owned accounts vs. contract accounts Itâ€™s important to understand a fundamental difference between externally owned accounts and contract accounts. An externally owned account can send messages to other externally owned accounts OR to other contract accounts by creating and signing a transaction using its private key. A message between two externally owned accounts is simply a value transfer. But a message from an externally owned account to a contract account activates the contract accountâ€™s code, allowing it to perform various actions (e.g. transfer tokens, write to internal storage, mint new tokens, perform some calculation, create new contracts, etc.). Unlike externally owned accounts, contract accounts canâ€™t initiate new transactions on their own. Instead, contract accounts can only fire transactions in response to other transactions they have received (from an externally owned account or from another contract account). Weâ€™ll learn more about contract-to-contract calls in the â€œTransactions and Messagesâ€ section. Therefore, any action that occurs on the Ethereum blockchain is always set in motion by transactions fired from externally controlled accounts. Account state The account state consists of four components, which are present regardless of the type of account: nonce: If the account is an externally owned account, this number represents the number of transactions sent from the accountâ€™s address. If the account is a contract account, the nonce is the number of contracts created by the account. balance: The number of Wei owned by this address. There are 1e+18 Wei per Ether. storageRoot: A hash of the root node of a Merkle Patricia tree (weâ€™ll explain Merkle trees later on). This tree encodes the hash of the storage contents of this account, and is empty by default. codeHash: The hash of the EVM (Ethereum Virtual Machine â€” more on this later) code of this account. For contract accounts, this is the code that gets hashed and stored as the codeHash. For externally owned accounts, the codeHash field is the hash of the empty string. World state Okay, so we know that Ethereumâ€™s global state consists of a mapping between account addresses and the account states. This mapping is stored in a data structure known as a Merkle Patricia tree. A Merkle tree (or also referred as â€œMerkle trieâ€) is a type of binary tree composed of a set of nodes with: a large number of leaf nodes at the bottom of the tree that contain the underlying data a set of intermediate nodes, where each node is the hash of its two child nodes a single root node, also formed from the hash of its two child node, representing the top of the tree The data at the bottom of the tree is generated by splitting the data that we want to store into chunks, then splitting the chunks into buckets, and then taking the hash of each bucket and repeating the same process until the total number of hashes remaining becomes only one: the root hash. This tree is required to have a key for every value stored inside it. Beginning from the root node of the tree, the key should tell you which child node to follow to get to the corresponding value, which is stored in the leaf nodes. In Ethereumâ€™s case, the key/value mapping for the state tree is between addresses and their associated accounts, including the balance, nonce, codeHash, and storageRoot for each account (where the storageRoot is itself a tree). This same trie structure is used also to store transactions and receipts. More specifically, every block has a â€œheaderâ€ which stores the hash of the root node of three different Merkle trie structures, including: The ability to store all this information efficiently in Merkle tries is incredibly useful in Ethereum for what we call â€œlight clientsâ€ or â€œlight nodes.â€ Remember that a blockchain is maintained by a bunch of nodes. Broadly speaking, there are two types of nodes: full nodes and light nodes. A full archive node synchronizes the blockchain by downloading the full chain, from the genesis block to the current head block, executing all of the transactions contained within. Typically, miners store the full archive node, because they are required to do so for the mining process. It is also possible to download a full node without executing every transaction. Regardless, any full node contains the entire chain. But unless a node needs to execute every transaction or easily query historical data, thereâ€™s really no need to store the entire chain. This is where the concept of a light node comes in. Instead of downloading and storing the full chain and executing all of the transactions, light nodes download only the chain of headers, from the genesis block to the current head, without executing any transactions or retrieving any associated state. Because light nodes have access to block headers, which contain hashes of three tries, they can still easily generate and receive verifiable answers about transactions, events, balances, etc. The reason this works is because hashes in the Merkle tree propagate upward â€” if a malicious user attempts to swap a fake transaction into the bottom of a Merkle tree, this change will cause a change in the hash of the node above, which will change the hash of the node above that, and so on, until it eventually changes the root of the tree. Any node that wants to verify a piece of data can use something called a â€œMerkle proofâ€ to do so. A Merkle proof consists of: Anyone reading the proof can verify that the hashing for that branch is consistent all the way up the tree, and therefore that the given chunk is actually at that position in the tree. In summary, the benefit of using a Merkle Patricia tree is that the root node of this structure is cryptographically dependent on the data stored in the tree, and so the hash of the root node can be used as a secure identity for this data. Since the block header includes the root hash of the state, transactions, and receipts trees, any node can validate a small part of state of Ethereum without needing to store the entire state, which can be potentially unbounded in size. Gas and payment One very important concept in Ethereum is the concept of fees. Every computation that occurs as a result of a transaction on the Ethereum network incurs a fee â€” thereâ€™s no free lunch! This fee is paid in a denomination called â€œgas.â€ Gas is the unit used to measure the fees required for a particular computation. Gas price is the amount of Ether you are willing to spend on every unit of gas, and is measured in â€œgwei.â€ â€œWeiâ€ is the smallest unit of Ether, where 1â°Â¹â¸ Wei represents 1 Ether. One gwei is 1,000,000,000 Wei. With every transaction, a sender sets a gas limit and gas price. The product of gas price and gas limit represents the maximum amount of Wei that the sender is willing to pay for executing a transaction. For example, letâ€™s say the sender sets the gas limit to 50,000 and a gas price to 20 gwei. This implies that the sender is willing to spend at most 50,000 x 20 gwei = 1,000,000,000,000,000 Wei = 0.001 Ether to execute that transaction. Remember that the gas limit represents the maximum gas the sender is willing to spend money on. If they have enough Ether in their account balance to cover this maximum, theyâ€™re good to go. The sender is refunded for any unused gas at the end of the transaction, exchanged at the original rate. In the case that the sender does not provide the necessary gas to execute the transaction, the transaction runs â€œout of gasâ€ and is considered invalid. In this case, the transaction processing aborts and any state changes that occurred are reversed, such that we end up back at the state of Ethereum prior to the transaction. Additionally, a record of the transaction failing gets recorded, showing what transaction was attempted and where it failed. And since the machine already expended effort to run the calculations before running out of gas, logically, none of the gas is refunded to the sender. Where exactly does this gas money go? All the money spent on gas by the sender is sent to the â€œbeneficiaryâ€ address, which is typically the minerâ€™s address. Since miners are expending the effort to run computations and validate transactions, miners receive the gas fee as a reward. Typically, the higher the gas price the sender is willing to pay, the greater the value the miner derives from the transaction. Thus, the more likely miners will be to select it. In this way, miners are free to choose which transactions they want to validate or ignore. In order to guide senders on what gas price to set, miners have the option of advertising the minimum gas price for which they will execute transactions. There are fees for storage, too Not only is gas used to pay for computation steps, it is also used to pay for storage usage. The total fee for storage is proportional to the smallest multiple of 32 bytes used. Fees for storage have some nuanced aspects. For example, since increased storage increases the size of the Ethereum state database on all nodes, thereâ€™s an incentive to keep the amount of data stored small. For this reason, if a transaction has a step that clears an entry in the storage, the fee for executing that operation of is waived, AND a refund is given for freeing up storage space. Whatâ€™s the purpose of fees? One important aspect of the way the Ethereum works is that every single operation executed by the network is simultaneously effected by every full node. However, computational steps on the Ethereum Virtual Machine are very expensive. Therefore, Ethereum smart contracts are best used for simple tasks, like running simple business logic or verifying signatures and other cryptographic objects, rather than more complex uses, like file storage, email, or machine learning, which can put a strain on the network. Imposing fees prevents users from overtaxing the network. Ethereum is a Turing complete language. (In short, a Turing machine is a machine that can simulate any computer algorithm (for those not familiar with Turing machines, check out this and this). This allows for loops and makes Ethereum susceptible to the halting problem, a problem in which you cannot determine whether or not a program will run infinitely. If there were no fees, a malicious actor could easily try to disrupt the network by executing an infinite loop within a transaction, without any repercussions. Thus, fees protect the network from deliberate attacks. You might be thinking, â€œwhy do we also have to pay for storage?â€ Well, just like computation, storage on the Ethereum network is a cost that the entire network has to take the burden of. Transaction and messages We noted earlier that Ethereum is a transaction-based state machine. In other words, transactions occurring between different accounts are what move the global state of Ethereum from one state to the next. In the most basic sense, a transaction is a cryptographically signed piece of instruction that is generated by an externally owned account, serialized, and then submitted to the blockchain. There are two types of transactions: message calls and contract creations (i.e. transactions that create new Ethereum contracts).   All transactions contain the following components, regardless of their type: nonce: a count of the number of transactions sent by the sender. gasPrice: the number of Wei that the sender is willing to pay per unit of gas required to execute the transaction. gasLimit: the maximum amount of gas that the sender is willing to pay for executing this transaction. This amount is set and paid upfront, before any computation is done. to: the address of the recipient. In a contract-creating transaction, the contract account address does not yet exist, and so an empty value is used. value: the amount of Wei to be transferred from the sender to the recipient. In a contract-creating transaction, this value serves as the starting balance within the newly created contract account. v, r, s: used to generate the signature that identifies the sender of the transaction. init (only exists for contract-creating transactions): An EVM code fragment that is used to initialize the new contract account. init is run only once, and then is discarded. When init is first run, it returns the body of the account code, which is the piece of code that is permanently associated with the contract account. data (optional field that only exists for message calls): the input data (i.e. parameters) of the message call. For example, if a smart contract serves as a domain registration service, a call to that contract might expect input fields such as the domain and IP address. We learned in the â€œAccountsâ€ section that transactions â€” both message calls and contract-creating transactions â€” are always initiated by externally owned accounts and submitted to the blockchain. Another way to think about it is that transactions are what bridge the external world to the internal state of Ethereum. But this doesnâ€™t mean that contracts canâ€™t talk to other contracts. Contracts that exist within the global scope of Ethereumâ€™s state can talk to other contracts within that same scope. The way they do this is via â€œmessagesâ€ or â€œinternal transactionsâ€ to other contracts. We can think of messages or internal transactions as being similar to transactions, with the major difference that they are NOT generated by externally owned accounts. Instead, they are generated by contracts. They are virtual objects that, unlike transactions, are not serialized and only exist in the Ethereum execution environment. When one contract sends an internal transaction to another contract, the associated code that exists on the recipient contract account is executed. One important thing to note is that internal transactions or messages donâ€™t contain a gasLimit. This is because the gas limit is determined by the external creator of the original transaction (i.e. some externally owned account). The gas limit that the externally owned account sets must be high enough to carry out the transaction, including any sub-executions that occur as a result of that transaction, such as contract-to-contract messages. If, in the chain of transactions and messages, a particular message execution runs out of gas, then that messageâ€™s execution will revert, along with any subsequent messages triggered by the execution. However, the parent execution does not need to revert. Blocks All transactions are grouped together into â€œblocks.â€ A blockchain contains a series of such blocks that are chained together. In Ethereum, a block consists of: the block header information about the set of transactions included in that block a set of other block headers for the current blockâ€™s ommers. Ommers explained What the heck is an â€œommer?â€ An ommer is a block whose parent is equal to the current blockâ€™s parentâ€™s parent. Letâ€™s take a quick dive into what ommers are used for and why a block contains the block headers for ommers. Because of the way Ethereum is built, block times are much lower (~15 seconds) than those of other blockchains, like Bitcoin (~10 minutes). This enables faster transaction processing. However, one of the downsides of shorter block times is that more competing block solutions are found by miners. These competing blocks are also referred to as â€œorphaned blocksâ€ (i.e. mined blocks do not make it into the main chain). The purpose of ommers is to help reward miners for including these orphaned blocks. The ommers that miners include must be â€œvalid,â€ meaning within the sixth generation or smaller of the present block. After six children, stale orphaned blocks can no longer be referenced (because including older transactions would complicate things a bit). Ommer blocks receive a smaller reward than a full block. Nonetheless, thereâ€™s still some incentive for miners to include these orphaned blocks and reap a reward. Block header Letâ€™s get back to blocks for a moment. We mentioned previously that every block has a block â€œheader,â€ but what exactly is this?   A block header is a portion of the block consisting of: parentHash: a hash of the parent blockâ€™s header (this is what makes the block set a â€œchainâ€) ommersHash: a hash of the current blockâ€™s list of ommers beneficiary: the account address that receives the fees for mining this block stateRoot: the hash of the root node of the state trie (recall how we learned that the state trie is stored in the header and makes it easy for light clients to verify anything about the state) transactionsRoot: the hash of the root node of the trie that contains all transactions listed in this block receiptsRoot: the hash of the root node of the trie that contains the receipts of all transactions listed in this block logsBloom: a Bloom filter (data structure) that consists of log information difficulty: the difficulty level of this block number: the count of current block (the genesis block has a block number of zero; the block number increases by 1 for each each subsequent block) gasLimit: the current gas limit per block gasUsed: the sum of the total gas used by transactions in this block timestamp: the unix timestamp of this blockâ€™s inception extraData: extra data related to this block mixHash: a hash that, when combined with the nonce, proves that this block has carried out enough computation nonce: a hash that, when combined with the mixHash, proves that this block has carried out enough computation Notice how every block header contains three trie structures for: state (stateRoot) transactions (transactionsRoot) receipts (receiptsRoot) These trie structures are nothing but the Merkle Patricia tries we discussed earlier. Additionally, there are a few terms from the above description that are worth clarifying. Letâ€™s take a look. Logs Ethereum allows for logs to make it possible to track various transactions and messages. A contract can explicitly generate a log by defining â€œeventsâ€ that it wants to log. A log entry contains: the loggerâ€™s account address, a series of topics that represent various events carried out by this transaction, and any data associated with these events. Logs are stored in a bloom filter, which stores the endless log data in an efficient manner. Transaction receipt Logs stored in the header come from the log information contained in the transaction receipt. Just as you receive a receipt when you buy something at a store, Ethereum generates a receipt for every transaction. Like youâ€™d expect, each receipt contains certain information about the transaction. This receipt includes items like: the block number block hash transaction hash gas used by the current transaction cumulative gas used in the current block after the current transaction has executed logs created when executing the current transaction ..and so on Block difficulty The â€œdifficultyâ€ of a block is used to enforce consistency in the time it takes to validate blocks. The genesis block has a difficulty of 131,072, and a special formula is used to calculate the difficulty of every block thereafter. If a certain block is validated more quickly than the previous block, the Ethereum protocol increases that blockâ€™s difficulty. The difficulty of the block affects the nonce, which is a hash that must be calculated when mining a block, using the proof-of-work algorithm. The relationship between the blockâ€™s difficulty and nonce is mathematically formalized as: where Hd is the difficulty. The only way to find a nonce that meets a difficulty threshold is to use the proof-of-work algorithm to enumerate all of the possibilities. The expected time to find a solution is proportional to the difficulty â€” the higher the difficulty, the harder it becomes to find the nonce, and so the harder it is to validate the block, which in turn increases the time it takes to validate a new block. So, by adjusting the difficulty of a block, the protocol can adjust how long it takes to validate a block. If, on the other hand, validation time is getting slower, the protocol decreases the difficulty. In this way, the validation time self-adjusts to maintain a constant rate â€” on average, one block every 15 seconds. Transaction Execution Weâ€™ve come to one of the most complex parts of the Ethereum protocol: the execution of a transaction. Say you send a transaction off into the Ethereum network to be processed. What happens to transition the state of Ethereum to include your transaction? First, all transactions must meet an initial set of requirements in order to be executed. These include: The transaction must be a properly formatted RLP. â€œRLPâ€ stands for â€œRecursive Length Prefixâ€ and is a data format used to encode nested arrays of binary data. RLP is the format Ethereum uses to serialize objects. Valid transaction signature. Valid transaction nonce. Recall that the nonce of an account is the count of transactions sent from that account. To be valid, a transaction nonce must be equal to the sender accountâ€™s nonce. The transactionâ€™s gas limit must be equal to or greater than the intrinsic gas used by the transaction. The intrinsic gas includes: The senderâ€™s account balance must have enough Ether to cover the â€œupfrontâ€ gas costs that the sender must pay. The calculation for the upfront gas cost is simple: First, the transactionâ€™s gas limit is multiplied by the transactionâ€™s gas price to determine the maximum gas cost. Then, this maximum cost is added to the total value being transferred from the sender to the recipient. If the transaction meets all of the above requirements for validity, then we move onto the next step. First, we deduct the upfront cost of execution from the senderâ€™s balance, and increase the nonce of the senderâ€™s account by 1 to account for the current transaction. At this point, we can calculate the gas remaining as the total gas limit for the transaction minus the intrinsic gas used. Next, the transaction starts executing. Throughout the execution of a transaction, Ethereum keeps track of the â€œsubstate.â€ This substate is a way to record information accrued during the transaction that will be needed immediately after the transaction completes. Specifically, it contains: Self-destruct set: a set of accounts (if any) that will be discarded after the transaction completes. Log series: archived and indexable checkpoints of the virtual machineâ€™s code execution. Refund balance: the amount to be refunded to the sender account after the transaction. Remember how we mentioned that storage in Ethereum costs money, and that a sender is refunded for clearing up storage? Ethereum keeps track of this using a refund counter. The refund counter starts at zero and increments every time the contract deletes something in storage. Next, the various computations required by the transaction are processed. Once all the steps required by the transaction have been processed, and assuming there is no invalid state, the state is finalized by determining the amount of unused gas to be refunded to the sender. In addition to the unused gas, the sender is also refunded some allowance from the â€œrefund balanceâ€ that we described above. Once the sender is refunded: the Ether for the gas is given to the miner the gas used by the transaction is added to the block gas counter (which keeps track of the total gas used by all transactions in the block, and is useful when validating a block) all accounts in the self-destruct set (if any) are deleted Finally, weâ€™re left with the new state and a set of the logs created by the transaction. Now that weâ€™ve covered the basics of transaction execution, letâ€™s look at some of the differences between contract-creating transactions and message calls. Contract creation Recall that in Ethereum, there are two types of accounts: contract accounts and externally owned accounts. When we say a transaction is â€œcontract-creating,â€ we mean that the purpose of the transaction is to create a new contract account. In order to create a new contract account, we first declare the address of the new account using a special formula. Then we initialize the new account by: Setting the nonce to zero If the sender sent some amount of Ether as value with the transaction, setting the account balance to that value Deducting the value added to this new accountâ€™s balance from the senderâ€™s balance Setting the storage as empty Setting the contractâ€™s codeHash as the hash of an empty string Once we initialize the account, we can actually create the account, using the init code sent with the transaction (see the â€œTransaction and messagesâ€ section for a refresher on the init code). What happens during the execution of this init code is varied. Depending on the constructor of the contract, it might update the accountâ€™s storage, create other contract accounts, make other message calls, etc. As the code to initialize a contract is executed, it uses gas. The transaction is not allowed to use up more gas than the remaining gas. If it does, the execution will hit an out-of-gas (OOG) exception and exit. If the transaction exits due to an out-of-gas exception, then the state is reverted to the point immediately prior to transaction. The sender is not refunded the gas that was spent before running out. Boo hoo. However, if the sender sent any Ether value with the transaction, the Ether value will be refunded even if the contract creation fails. Phew! If the initialization code executes successfully, a final contract-creation cost is paid. This is a storage cost, and is proportional to the size of the created contractâ€™s code (again, no free lunch!) If thereâ€™s not enough gas remaining to
 if a message call execution exits because it runs out of gas or because the transaction is invalid (e.g. stack overflow
--:-- How Does the Blockchain Work? Blockchain technology explained in simple words Michele D'Aliessi Jun 1, 2016Â·15 min read Blockchain technology is probably the best invention since the internet itself. It allows value exchange without the need for trust or a central authority. Imagine you and I bet $50 on tomorrowâ€™s weather in San Francisco. I bet it will be sunny, you that it will rain. Today we have three options to manage this transaction: Neither trust nor contract is an optimal solution: We canâ€™t trust strangers, and enforcing a contract requires time and money. The blockchain technology is interesting because it offers us a third option which is secure, quick, and cheap. Blockchain allows us to write a few lines of code, a program running on the blockchain, to which both of us send $50. This program will keep the $100 safe and check tomorrowâ€™s weather automatically on several data sources. Sunny or rainy, it will automatically transfer the whole amount to the winner. Each party can check the contract logic, and once itâ€™s running on the blockchain it canâ€™t be changed or stopped. This may be too much effort for a $50 bet, but imagine selling a house or a company. This article explains how the blockchain works without discussing the technical details in depth, but by digging just enough to give you a general idea of the underlying logic and mechanisms. Also available in Simplified Chinese and Mandarin thanks to volunteering efforts and blockchain community support. The Basics of Bitcoin The most known and discussed application of the blockchain technology is bitcoin, a digital currency that can be used to exchange products and services, just like the U.S. dollar, euro, Chinese yuan, and other national currencies. Letâ€™s use this first application of the blockchain technology to learn how it works. â€œBitcoin gives us, for the first time, a way for one Internet user to transfer a unique piece of digital property to another Internet user, such that the transfer is guaranteed to be safe and secure, everyone knows that the transfer has taken place, and nobody can challenge the legitimacy of the transfer. The consequences of this breakthrough are hard to overstate.â€ â€” Marc Andreessen One bitcoin is a single unit of the Bitcoin (BTC) digital currency. Just like a dollar, a bitcoin has no value by itself; it has value only because we agree to trade goods and services to bring more of the currency under our control, and we believe others will do the same. To keep track of the amount of bitcoin each of us owns, the blockchain uses a ledger, a digital file that tracks all bitcoin transactions. The ledger file is not stored in a central entity server, like a bank, or in a single data center. It is distributed across the world via a network of private computers that are both storing data and executing computations. Each of these computers represents a â€œnodeâ€ of the blockchain network and has a copy of the ledger file. If David wants to send bitcoins to Sandra, he broadcasts a message to the network that says the amount of bitcoin in his account should go down by 5 BTC, and the amount in Sandraâ€™s account should increase by the same quantity. Each node in the network will receive the message and apply the requested transaction to its copy of the ledger, updating the account balances. The fact that the ledger is maintained by a group of connected computers rather than by a centralized entity like a bank has several implications: In our bank system we only know our own transactions and account balances; on the blockchain everyone can see everyone elseâ€™s transactions. While you can generally trust your bank, the bitcoin network is distributed and if something goes wrong there is no help desk to call or anyone to sue. The blockchain system is designed in such a way that no trust is needed; security and reliability are obtained via special mathematical functions and code. We can define the blockchain as a system that allows a group of connected computers to maintain a single updated and secure ledger. In order to perform transactions on the blockchain, you need a wallet, a program that allows you to store and exchange your bitcoins. Since only you should be able to spend your bitcoins, each wallet is protected by a special cryptographic method that uses a unique pair of distinct but connected keys: a private and a public key. If a message is encrypted with a specific public key, only the owner of the paired private key can decrypt and read the message. The reverse is also true: If you encrypt a message with your private key, only the paired public key can decrypt it. When David wants to send bitcoins, he needs to broadcast a message encrypted with the private key of his wallet. As David is the only one who knows the private key necessary to unlock his wallet, he is the only one who can spend his bitcoins. Each node in the network can cross-check that the transaction request is coming from David by decrypting the message with the public key of his wallet. When you encrypt a transaction request with your walletâ€™s private key, you are generating a digital signature that is used by blockchain computers to verify the source and authenticity of the transaction. The digital signature is a string of text resulting from your transaction request and your private key; therefore it cannot be used for other transactions. If you change a single character in the transaction request message, the digital signature will change, so no potential attacker can change your transaction requests or alter the amount of bitcoin you are sending. To send bitcoin you need to prove that you own the private key of a specific wallet as you need the key to encrypt your transaction request message. Since you broadcast the message only after it has been encrypted, you never have to reveal your private key. Tracking Your Wallet Balance Each node in the blockchain is keeping a copy of the ledger. So, how does a node know your account balance? The blockchain system doesnâ€™t keep track of account balances at all; it only records each and every transaction that is verified and approved. The ledger in fact does not keep track of balances, it only keeps track of every transaction broadcasted within the bitcoin network (Fig. 4). To determine your wallet balance, you need to analyze and verify all the transactions that ever took place on the whole network connected to your wallet. This â€œbalanceâ€ verification is performed based on links to previous transactions. In order to send 10 bitcoins to John, Mary has to generate a transaction request that includes links to previous incoming transactions that add up to at least 10 bitcoins. These links are called â€œinputs.â€ Nodes in the network verify the amount and ensure that these inputs havenâ€™t been spent yet. In fact, each time you reference inputs in a transaction, they are deemed invalid for any future transaction. This is all performed automatically in Maryâ€™s wallet and double-checked by the bitcoin network nodes; she only sends a 10 BTC transaction to Johnâ€™s wallet using his public key. So, how can the system trust that input transactions are valid? It checks all the previous transactions correlated to the wallet you use to send bitcoins via the input references. To speed up the verification process, a special record of unspent transactions is kept by the network nodes. Thanks to this security check, it is not possible to double-spend bitcoins. Owning bitcoins means that there are transactions written in the ledger that point to your wallet address and havenâ€™t been used as inputs yet. All the code to perform transactions on the bitcoin network is open source; this means that anyone with a laptop and an internet connection can operate transactions. However, should there be a mistake in the code used to broadcast a transaction request message, the associated bitcoins will be permanently lost. Remember that since the network is distributed, there is no customer support to call nor anyone who could help you restore a lost transaction or forgotten wallet password. For this reason, if you are interested in transacting on the bitcoin network, itâ€™s a good idea to use the open source and official version of bitcoin wallet software (such as Bitcoin Core), and to store your walletâ€™s password or private key in a very safe repository. But Is It Really Safe? And Why Is It Called Blockchain? Anyone can access the bitcoin network via an anonymous connection (for example, the TOR network or a VPN network), and submit or receive transactions revealing nothing more than his public key. However if someone uses the same public key over and over, itâ€™s possible to connect all the transactions to the same owner. The bitcoin network allows you to generate as many wallets as you like, each with its own private and public keys. This allows you to receive payments on different wallets, and there is no way for anyone to know that you own all these walletsâ€™ private keys, unless you send all the received bitcoins to a single wallet. The total number of possible bitcoin addresses is 2Â¹â¶â° or 1461501637330902918203684832716283019655932542976. This large number protects the network from possible attacks while allowing anyone to own a wallet. With this setup, there is still a major security hole that could be exploited to recall bitcoins after spending them. Transactions are passed from node to node within the network, so the order in which two transactions reach each node can be different. An attacker could send a transaction, wait for the counterpart to ship a product, and then send a reverse transaction back to his own account. In this case, some nodes could receive the second transaction before the first and therefore consider the initial payment transaction invalid, as the transaction inputs would be marked as already spent. How do you know which transaction has been requested first? Itâ€™s not secure to order the transactions by timestamp because it could easily be counterfeit. Therefore, there is no way to tell if a transaction happened before another, and this opens up the potential for fraud. If this happens, there will be disagreement among the network nodes regarding the order of transactions each of them received. So the blockchain system has been designed to use node agreement to order transactions and prevent the fraud described above. The bitcoin network orders transactions by grouping them into blocks; each block contains a definite number of transactions and a link to the previous block. This is what puts one block after the other in time. Blocks are therefore organized into a time-related chain (Fig. 6) that gives the name to the whole system: blockchain. Transactions in the same block are considered to have happened at the same time, and transactions not yet in a block are considered unconfirmed. Each node can group transactions into a block and broadcast it to the network as a suggestion for which block should be next. Since any node can suggest a new block, how does the system agree on which block should be the next? To be added to the blockchain, each block must contain the answer to a complex mathematical problem created using an irreversible cryptographic hash function. The only way to solve such a mathematical problem is to guess random numbers that, combined with the previous block content, generate a defined result. It could take about a year for a typical computer to guess the right number and solve the mathematical problem. However, due to the large number of computers in the network that are guessing numbers, a block is solved on average every 10 minutes. The node that solves the mathematical problem acquires the right to place the next block on the chain and broadcast it to the network. And what if two nodes solve the problem at the same time and send their blocks to the network simultaneously? In this case, both blocks are broadcast and each node builds on the block that it received first. However, the blockchain system requires each node to build immediately on the longest blockchain available. So if there is ambiguity about which is the last block, as soon as the next block is solved, each node will adopt the longest chain as the only option. Due to the low probability of solving blocks simultaneously, itâ€™s almost impossible that multiple blocks would be solved at the same time over and over, building different â€œtails,â€ so the whole blockchain stabilizes quickly to one single string of blocks that every node agrees on. A disagreement about which block represents the end of the chain tail opens up the potential for fraud again. If a transaction happens to be in a block that belongs to a shorter tail (like block B in Fig. 7), once the next block is solved, this transaction, along with all others in its block, will go back to the unconfirmed transactions. Transactions in the Bitcoin blockchain system are protected by a mathematical race: Any attacker is competing against the whole network. Letâ€™s see how Mary could leverage this end-of-chain ambiguity to perform a double-spending attack. Mary sends money to John, John ships the product to Mary. Since nodes always adopt the longer tail as the confirmed transactions, if Mary could generate a longer tail that contains a reverse transaction with the same input references, John would be out of both his money and his product. How does the system prevent this kind of fraud? Each block contains a reference to the previous block (see Fig. 6). That reference is part of the mathematical problem that needs to be solved in order to spread the following block to the network. So, itâ€™s extremely hard to pre-compute a series of blocks due to the high number of random guesses needed to solve a block and place it on the blockchain. Mary is in a race against the rest of the network to solve the math problem that allows her to place the next block on the chain. Even if she solves it before anyone else, itâ€™s very unlikely she could solve two, three, or more blocks in a row, since each time she is competing against the whole network. Could Mary use a super fast computer to generate enough random guesses to compete with the whole network in solving blocks? Yes, but even with a very, very fast computer, due to the large number of members in the network, itâ€™s highly unlikely Mary could solve several blocks in a row at the exact time needed to perform a double-spending attack. She would need control of 50 percent of the computing power of the whole network to have a 50 percent chance of solving a block before some other node does â€” and even in this case, sheâ€™d only have a 25 percent chance of solving two blocks in a row. The more blocks to be solves in a row, the lower the probability of her success. Transactions in the bitcoin blockchain system are protected by a mathematical race: Any attacker is competing against the entire network. Therefore, transactions grow more secure with time. Those included in a block confirmed one hour ago, for example, are more secure than those in a block confirmed in the last 10 minutes. Since a block is added to the chain every 10 minutes on average, a transaction included in a block for the first time an hour ago has most likely been processed and is now irreversible. Mining Bitcoin In order to send bitcoins, you need to reference an incoming transaction to your own wallet. This applies to every single transaction across the network. So, where do bitcoins come from in the first place? As a way to balance the deflationary nature of bitcoin due to software errors and wallet password loss, a reward is given to those who solve the mathematical problem of each block. The activity of running the bitcoin blockchain software in order to obtain these bitcoin rewards is called â€œminingâ€ â€” and itâ€™s very much like mining gold. Rewards are the main incentive for private people to operate the nodes, thus providing the necessary computing power to process transactions and stabilize the blockchain network. Because it takes a long time for a typical computer to solve a block (about one year on average), nodes band together in groups that divide up the number of guesses to solve the next block. Working as a group speeds up the process of guessing the right number and getting the reward, which is then shared among group members. These groups are called mining pools. Some of these mining pools are very large, and represent more than 20 percent of the total network computing power. This has clear implications for network security, as seen in the double-spend attack example above. Even if one of these pools could potentially gain 50 percent of the network computing power, the further back along the chain a block goes, the more secure the transactions within it become. However, some of these mining pools with substantial computing power have decided to limit their members in order to safeguard overall network security. Since the overall network computing power is likely to increase over time due to technological innovation and the increasing number of nodes, the blockchain system recalibrates the mathematical difficulty of solving the next block to target 10 minutes on average for the entire network. This ensures the networkâ€™s stability and overall security. Moreover, every four years the block reward is cut in half, so mining bitcoin (running the network) gets less interesting over time. To encourage nodes to keep operating, small reward fees can be attached to each transaction; these rewards are collected by the node that successfully includes such transactions in a block and solves its mathematical problem. Due to this mechanism, transactions associated with a higher reward are usually processed faster than those associated with a low reward. What this means is that, when sending a transaction, you can decide if youâ€™d like to process it faster (more expensive) or cheaper (takes more time). Transaction fees in the bitcoin network are currently very small compared with what banks charge, and theyâ€™re not associated with the transaction amount. Blockchain Benefits and Challenges Now that you have a general understanding of how the blockchain works, letâ€™s take a quick look at why itâ€™s so interesting. Using blockchain technology has remarkable benefits: You have complete control of the value you own; there is no third party that holds your value or can limit your access to it. The cost to perform a value transaction from and to anywhere on the planet is very low. This allows micropayments. Value can be transferred in a few minutes, and the transaction can be considered secure after a few hours, rather than days or weeks. Anyone at any time can verify every transaction made on the blockchain, resulting in full transparency. Itâ€™s possible to leverage the blockchain technology to build decentralized applications that would be able to manage information and transfer value fast and securely. However, there are a few challenges that need to be addressed: Transactions can be sent and received anonymously. This preserves user privacy, but it also allows illegal activity on the network. Though many exchange platforms are emerging, and digital currencies are gaining popularity, itâ€™s still not easy to trade bitcoins for goods and services. Bitcoin, like many other cryptocurrencies, is very volatile: There arenâ€™t many bitcoins available in the market and the demand is changing rapidly. Bitcoin price is erratic, changing based on large events or announcements in the cryptocurrencies industry. Overall, the blockchain technology has the potential to revolutionize several industries, from advertising to energy distribution. Its main power lies in its decentralized nature and ability to eliminate the need for trust. New use cases are arising all the time â€” like the possibility of creating a fully decentralized platform that runs smart contracts like Ethereum. But itâ€™s important to remember that the technology is still in its infancy. New tools are being developed every day to improve blockchain security while offering a broader range of features, tools, and services. Useful Links Get your own Bitcoin wallet Buy your first Bitcoins Start mining Bitcoin as a beginner or a pro Learn more about decentralized applications Make sure your Bitcoins are kept safe, away from hackers, with a Ledger Wallet
Oct 26Â·5 min read Looking at my 26-day-old Tweet, it feels certain I was going to write *something* about Coinbaseâ€™s culture memo. Now in late October, weâ€™re far enough removed to see initial reactions have play out, and thatâ€™s left space to see what has remained tumbling in my head. Whatâ€™s the â€œBestâ€ Version of What Coinbase is Attempting to Do? Iâ€™ve observed a tendency to label corporate cultures â€œgoodâ€ or â€œbadâ€ based on whether the critiquing individual agrees with the companyâ€™s professed values or could imagine themselves working there. While there are certainly â€œbadâ€ cultures, I believe that label most accurately applies to organizations that have either (i) inconsistently executed their values, (ii) a set of professed beliefs that are strategically at odds with the company strategy or (iii) ingrained practices which cause unethical or illegal actions to flourish with management approval. Outside of these situations company cultures translate more simply as â€œappeals to meâ€ or â€œdoesnâ€™t appeal to me.â€ From what Iâ€™ve read recently, I donâ€™t believe a company like Coinbase would appeal to me as an employee, but that doesnâ€™t make it â€œbad.â€ Similarly, I donâ€™t know enough about their current team and their practices to suggest that any of my earlier â€œbadâ€ culture framework conditions apply, and Iâ€™ve certainly respected many of the folks who have worked there over the course of its life. What I would like to understand over time is whether Brianâ€™s vision for a company that doesnâ€™t want to broadly engage in societal/political issues is the best version of its own playbook. That is to say, itâ€™s unlikely Coinbaseâ€™s first stab at implementing this is the perfect way to address the concerns he outlines since weâ€™re always learning and evolving. Iâ€™m curious how he and other like-minded CEOs will analyze where they were right, where they were wrong and how this collective knowledge will get shared. A non-zero percentage of companies over time will likely have their own version of Coinbaseâ€™s strategy, and for their benefit, Iâ€™m hoping heâ€™ll engage in measurement and reflection. There are probably better versions of what Brian wants to implement and worse versions. My assumption around a stable future state for Coinbase and others would be they offer a â€œgoodâ€ version of this approach, and then employees vote with their feet â€” some people will love it, others wonâ€™t, and thatâ€™s ok in the scheme of things (as Brian has noted himself). Whatâ€™s the â€œBestâ€ Version of a Corporate Culture Thatâ€™s Societally Aware, Politically Active and High Performing? Separately, Iâ€™m personally not a believer that an organizationâ€™s political and societal engagement needs to come at the expense of company execution. In fact, and maybe this is my optimism about people (plus projection of my own personal preferences), I hope the best version of this type of company has a higher ceiling than the version Brian is laying out. Also I recognize that â€œsocietalâ€ and â€œpoliticalâ€ are getting conjoined here in a way thatâ€™s problematic. Perhaps a company can decide it wonâ€™t be broadly political but canâ€™t avoid societal questions, no matter how hard it doth protest, so long as it employs, you know, actual human beings. Hold â€œsocietal != politicalâ€ aside for the moment because itâ€™s likely a post all its own, better written by someone more deft on these issues than I am. What I hungered for after reading Brianâ€™s memo was bringing likeminded people together to build the best playbook for the opposite approach. Founders who desire embracing stances on the other side of the spectrum need some guidance too. Because itâ€™s messy right now in many companies and we shouldnâ€™t be glib to just assume â€œhire good peopleâ€ is the solution for anyone who wants to be the â€œanti-Coinbase.â€ Who has built the model for high performance societally-active corporate success? What tech companies should we be studying, while knowing that no company is perfect? Twilio and Salesforce come to mind. Others? Sorry, You Donâ€™t Get to Own â€œMission-Focusedâ€ So if Iâ€™m pretty even-keel on this issue overall is it weird that what *actually* bugs me is an attempt to claim â€œmission-focusedâ€ as the way to describe Coinbaseâ€™s culture? To say that, as a company, youâ€™re â€œmission-focusedâ€ if you try to reduce the footprint of corporate time and energy spent on broadly societal issues seems unfair. It reflexively turns alternative POVs into â€œnon-mission-focused,â€ which has all sorts of pejorative connotations. Itâ€™s one thing to use this language internally but itâ€™s clear â€” from for example the tweet above â€” Coinbase wants to make it the external phrasing for their culture as well. And that to me feels like an overreach that I want to try and prevent. To me, mission-focused means having, articulating and living your mission, *not* any one point on the spectrum Brian is articulating. So dear reader, do me and George Lakoff a favor, and donâ€™t blindly accept anyone elseâ€™s framing techniques. There we go, three things that I wanted to get out. Probably disappointed the folks who wanted to see me slag Coinbase but at same time questioning enough to keep me in the SJW box for those who prefer to accuse me of performative wokeness. Mission accomplished! Notes and More ðŸ“š George Lakoffâ€™s â€œDonâ€™t Think of an Elephantâ€ is a solid overview on how language frames moral issues, through a lens of why Republicans are so good at this and Democrats so bad. ðŸ“¦ Things Iâ€™m Enjoying Passenger Coffee from Lancaster, PA; Our Fake History podcast; single serving oatmeal cups ðŸ— Highlighted Homebrew Portfolio Jobs Stir is building software and financial tools to help creators manage their business. Theyâ€™re hiring in design, engineering and also letting you create your own role.
If someone truly believes in Bitcoin, itâ€™s Didi Taihuttu. Shortly after his father died, Taihuttu had an idea. First, heâ€™d sell his 11-year old business along with everything he owned â€” from his house all the way down to his childrenâ€™s toys. Second, Taihuttu would buy a van large enough for him, his wife, and their three daughters. Then, the family would convert every penny they had into bitcoins. From there, the Bitcoin Family would hit the road and live a minimalistic nomad life waiting for a crypto-boom to happen. This was in 2017. Today, nearly four years later, Taihuttu was supposed to cash in on his crazy investment and become a multi-millionnaire. Except he didnâ€™t â€” yet. He Invested When One Bitcoin Was Worth $3.5k â€” Now Itâ€™s Worth $20k Taihuttu didnâ€™t go all-in blindly â€” he had a past with Bitcoins. Heâ€™d learned about cryptocurrency in 2013 and dove in with a friend. In Taihuttuâ€™s words: â€œI am an entrepreneur, so when I first heard about bitcoin, I said: Letâ€™s do this.â€ The two friends started mining for Bitcoin. Whatâ€™s mining Bitcoin? You plug powerful computers 24/7 and make them do calculations involved with bitcoin transactions. If youâ€™re lucky, your calculations will get rewarded with bitcoin crumbs. Taihuttuâ€™s mine didnâ€™t gather enough crumbs to be profitable, so he quickly shut it down. Four years later, bitcoin came back to haunt him like a golden ghost. This time, though, Taihuttu played big â€”as big as he could. Nobody seems to know exactly how much Bitcoin Taihuttu had gathered back in 2017. We know that his house was worth $300kâ€” roughly 85 bitcoins at the time. Letâ€™s say that with the rest of his sales minus all expenses, he managed to hoard 100 bitcoins â€” $350k. This means that today he owns 2 million dollars with a return on investment of nearly 600%. Not bad. But not enough for Taihuttu, whoâ€™s already proven his strength against temptation. Six months into his 2017 crazy bet, the bitcoin price had skyrocketed from $3.5k to somewhere between 15 and $19k. Taihuttuâ€™s reaction? â€œEverybody talks the talk, but nobody walks the walk.â€ Taihuttu was right. 2017â€™s peak was a bubble that burst quickly. Today, despite the $20k peak announcing a potential second bubble, Taihuttu is still walking his walk. Taihuttuâ€™s New Bet: Bitcoin Price Reaching $200k by 2022 When the bitcoin bubble burst in 2018, Taihuttu and his family didnâ€™t panic. Instead, they re-invested. â€œWhen bitcoin dipped, we started to buy more,â€ Taihuttu recalls. â€œI think in this [2020] bull cycle, we are going to see a minimal peak of $100,000. I wonâ€™t be surprised if it hits $200,000 by 2022.â€ More determined and ambitious than ever, Taihuttuâ€™s family plans to continue their adventure with no bank accounts nor properties. Where does this confidence come from? Perhaps from Taihuttuâ€™s accurate prediction skills. Fun fact: When Taihuttu first invested in 2017, he claimed that the bitcoin price would hit $20k by 2020. His guess turned out to be exactly right. Though he didnâ€™t rely on tangible explanations back in 2017, he suggested a solid argument to support his $200k forecast. In an interview with CNBC, Taihuttu predicted a supply crisis in bitcoin. He claims that Paypalâ€™s 350 million users will play a big role in a future bitcoin shortage. According to Taihuttu, after being granted access to bitcoins, Paypal users would want to buy some. The imbalance between supply and demand would generate scarcity in bitcoin â€” and thus, price explosions. Other evidence supports Taihuttuâ€™s claims. Bitcoin is getting easier to manage and more popular â€” especially among wealthy investors. â€œThe 2017 rally was driven by retail speculation, and in 2020, itâ€™s the billionaires and corporations that are buying bitcoin en masse,â€ writes CNBC. In addition to billionaires and companies, finance experts are also anticipating a crypto boom no later than next year. Citibank analyst from the U.S. predicts that bitcoin price will skyrocket to $318k by the end of 2021. Once again, bitcoin bets are placed, and only time will tell. Thereâ€™s Always More to the Story Taihuttuâ€™s adventure has been and continues to be a compelling one. He involved his family in a crazy bet. The family stood together and resisted temptation both in 2017 and 2020. But thereâ€™s always a hidden side of the coin. Despite going from business owner to a cryptocurrency nomad, Taihuttu remained an entrepreneur. Perhaps, the lessons hidden behind the curtains are more valuable than the outside noise thatâ€™s impressing us. Here are six lessons you may want to keep in mind when remembering the Bitcoin Family: Keep in Touch Hey there, itâ€™s Nabil. If you enjoyed this story, you can click this quick-follow link to see more of my work on your Medium feed.
Charlie Andrews-Jubelt Dec 16Â·9 min read Is downgrading climbs more common than upgrading? Do climbers begin to prefer longer routes over shorter ones as they get older? What route will you enjoy the most at a new climbing area? In the past, these questions have been difficult or impossible to answer objectively. In general, we remain largely in the dark about the underlying mechanics of climber preferences and our habits of grading climbs. However, tens of thousands of climbers have added millions of ascents to â€œonline logbooks,â€ which I define as web applications for keeping track of which climbs you have done. This presents an exciting opportunity for the social science of climbing to answer questions that require a lot of data to answer. The goal of this project is to leverage the data available on online logbooks to gain a better understanding of climber preferences and route grading. In collaboration with thecrag.com and Beta Angel, I have analyzed a dataset of 1.5 million ascents logged by climbers from all over the world. I plan to release results in batches that will address the following questions: The backstory About three years ago I was cooped up in my AirBnB on a bouldering trip in Hueco Tanks, Texas. It was a day some friends and I were forced to rest, due to some winter storms. We were getting cagey, and a little tired of watching climbing videos â€” like any group of cooped-up college student climbers on their last few days of winter break, we just wanted to climb. Someone was listing off the latest routes that had been downgraded on 8a.nu, an online logbook website that was popular among some of the groupâ€™s more ambitious grade chasers. And then someone said, â€œWouldnâ€™t it be cool to use all that data on 8a.nu to do some Machine Learning on what people like to climb?â€ We loved the idea. (Yes, my group was hopelessly nerdy.) Right then, the idea for Online Logbook Data Science was born. Wouldnâ€™t it be cool to use all that data on 8a.nu to do some machine learning on what people like to climb? After some excited brainstorming, the literal storm cleared, and just as youâ€™d expect, we dropped everything and flocked to the boulders. But the idea stayed with meâ€¦ and today Iâ€™m excited to announce the first batch of results! Our approach The goal of this project is to gain a deeper understanding of climber preferences and habits of grading climbs. The project is deliberately open-ended. We did not run an experiment with any single hypothesis in mind. Instead, we are starting with the data, and exploring the space of possible questions that might be answerable from it. Letâ€™s get more concrete about what it means to do data science on an online logbook. What is an online logbook? I define an online logbook as a web application where climbers keep track of which climbs theyâ€™ve done. 8a.nu, the logbook mentioned in the project backstory, is just one example â€” thereâ€™s also thecrag.com, Mountain Project, 27crags, and several others. Why do data science on online logbooks? On each of the major online logbook platforms, tens of thousands of climbers have logged over a million ascents. Thatâ€™s exciting, but you canâ€™t even open an Excel file with that many rows. How are we ever going to spot patterns in that much data? Data science. If youâ€™re not familiar with the phrase, think statistics, but with computer programming mixed in. And you are already familiar with data scienceâ€¦read on! My goal for this series is to write for beginners and experts alike. Hasnâ€™t this been done already? Interestingly, soon after the idea of Online Logbook Data Science was floated on that Hueco Trip I described above, David Cohen independently scraped 8a.nu and posted the resulting dataset on Kaggle, leading to some interesting spinoff research. Some highlights: Steve Bachmeier used the dataset to investigate demographic factors for climber performance. (Tl;dr: experience helps, height doesnâ€™t.) Durand Dâ€™souza posted an interesting notebook showing how long it takes climbers to reach the next grade (a few months to a year before the data thins out). However, David Cohenâ€™s script was taken down, and the author himself was threatened with legal action (scraping is a dangerous game). Although the data is still available on Kaggle, it has not been â€œrefreshedâ€ in some time, and some may argue that itâ€™s ethically questionable to use it at this point (at least since 8a.nuâ€™s parent company objected to its use). Fortunately, 8a.nu isnâ€™t the only online logbook in the game; neither is their data the best-suited to data science, necessarily. One issue is that routes are reportedly keyed by route name, so misspellings during logging can lead to messier data. On the other hand, I was excited to find that the team at theCrag.com promotes collaborations with data scientists and keeps their data relatively clean and structured. For example, all ascents of the same route are linked by a route ID, even if the climbers who logged the route misspelled the routeâ€™s name. Where we are today Courtesy of Ulf Fuchsleuger and the team at theCrag.com, one of the largest online logbook platforms (and the largest in Australia), I was permitted to access a sizable dataset consisting of around a million and a half ascents. From several excited conversations with my project mentor Taylor Reed, we have almost as many ideas for research questions as we have rows of data. Since our goal is to research continuously and let the data speak for itself, I have decided to release our research results in small-ish batches, rather than allow insights to accumulate over a long period of time. Results Batch One We began with a question that I thought was a bit of a long shot: do longer climbs appeal to older climbers? Though it may sound funny when stated that way, I think the assumption that age has something to do with route length preference is relatively common. Imagine hearing this at the crag: â€œThat route is just 10 meters, and incredibly crimpy. Itâ€™s like one long boulder problem. Iâ€™ll leave it to the gym kids and save my tendons for a nice long 40-meter adventure.â€ In my mind, the climber voicing this opinion has a few more gray hairs than those gym kids. But does the data support the stereotype? The answer, surprisingly, is yes. To reach that conclusion, I started by formulating the question as one that could be answered from the available data. The precise question I chose was â€œDo longer routes get a higher proportion of older ascentionists?â€ For example, if 75% of the climbers who logged a 35m route were over 50 years old, but the typical route on thecrag.com had only 20% of its ascents logged by climbers in that demographic, then that climb could be said to get a higher proportion of older ascentionists. Do longer routes get a higher proportion of older ascentionists? Notably lacking in rigor is the phrase â€œolder ascentionistsâ€ â€” what qualifies as old? Choosing a threshold for â€œoldâ€ required a look at the data of whoâ€™s logging their climbs on thecrag.com From the histogram in Figure 1, itâ€™s clear that thecrag.com users are a young crowd, with a median age of just 27. As a result, I chose â€œover 35â€ as my definition of â€œolder climberâ€ for the purposes of this problem. Please donâ€™t be offended if you are over 35! I do not consider you â€œoldâ€ if you are 36. I just needed your data ;). To spot a subtle difference between two groups, you need as many â€œobservationsâ€ (climbersâ€™ log records) as you can get for both of them, and 35 was the highest age for which there were still a reasonable number of climbs logged in the dataset (about 22% of log records). Next, I chose to consider only sport climbing routes under 40m, also due to data availability. thecrag.com has about twice as many sport climbing ascents as the next route gear style (trad), and as shown in Figure 2, the number of routes logged also drops off significantly with climbs above 40m. 40m also happens to be a good proxy for â€œsingle-pitchâ€ sport routes, so this works out nicely. To answer the main research question I needed just two more steps. First, I grouped routes by length, assigning each route to a five-meter bucket and combining their ascents. (In other words, all ascents of routes between 20 and 25m were counted in the same bucket.) Then, I took the fraction of all climbers who logged a route in that bucket who were over 35, and plotted them against route length. Figure 3 demonstrates the result, which I found surprisingly coherent. So for routes around 5 meters, about 20% of their climbers are above 35, whereas for routes around 35 meters, that number is about 25%. A simple linear regression on the data from Figure 3 yields a p-value of about 0.0015. For reference, 0.05 is considered â€œstatistically significant,â€ so this is pretty good! If p-values are unfamiliar to you, suffice it to say that if there were no relationship between route length and climber age, this result would have been very unlikely. I repeated the experiment with a slightly different formulation, looking at mean climber age vs route length bucket. I got a similar result, shown in Figure 4. I did another linear regression on the mean age vs route length data and got a p-value of about 0.003. Still well below the threshold for statistical significance! So in what turned out to be kind of an extra-mathy, climber-edition Mythbusters style exercise of checking an admittedly niche â€œmythâ€, I get to declare this oneâ€¦ Though of course, take it with a grain of salt. The average climber logging a route gets about 10 days older per meter. Sneak Peek at Results Batch Two Next on the agenda is an exploration of downgrading as a social phenomenon. Looking at hundreds of thousands of routes logged gives us an exciting chance to spot trends in who is downgrading climbs, and how often it takes place. My next post in this series will focus on how to write an algorithm that will identify when a climb has been downgraded with no human in the loop. After that, weâ€™ll do some social science on the climbs we identified as having been downgraded. Maybe weâ€™ll even tackle another climbing myth while weâ€™re at it! Acknowledgements Thanks again to the team at thecrag.com for providing data and support. Thanks Taylor Reed of Beta Angel for mentoring me and listening to many, many oscillations of excitement and disappointment as I worked through different research directions, software bugs, procrastination and stress. You guys are the best! Thanks also to the Data Science team at Lark, who offered some feedback on my analysis. Iâ€™m going to post a followup soon to discuss their comments and go deeper on the â€œwhyâ€ questions of the route length vs climber age correlation that was the subject of this article.
Andrew Rothman 3 days agoÂ·10 min read 1. Background and Motivation Causal Inference is a field that touches several domains and is of interest to a wide range of practitioners including Statisticians, Data Scientists, Machine Learning Scientists, and other Computational Researchers. Recovery of unbiased estimates of Causal Effects is at times a tough task, particularly in non-randomized settings. I have written several technical pieces on leveraging G-Methods for necessary adjustment to recover causal inferences/contrasts of interests; these include pieces on Efficient Sampling Designs in Causal Inference, Doubly Robust Estimation Techniques, G-Estimation of Structural Nested Models, and Marginal Structural Modeling for informative censoring adjustment in randomized A/B Tests. This piece is a bit shorter, but touches on an important topic. Specifically, we will discuss the structure and issues of â€œM-Biasâ€, and how confounding identification is technically unverifiable using empirical analysis alone. The contents of this piece are as follows: 2. Specification of Toy Example and â€œIdentifying Confoundingâ€ Let us postulate a simple toy example: We have binary Intervention A with support {0,1}, a continuous Outcome Y, and we are working in an observational (i.e. non-randomized) setting. We would like to recover an unbiased estimate of the Mean Causal Effect Difference of Intervention A on Outcome Y. We want to check if there is â€œconfoundingâ€ by binary variable L. As per empirical analysis of our dataset, we would like to â€œproveâ€ that L is in-fact a confounder of the A-Y causal relationship. And after proving L is in-fact a confounder, we can adjust for L in our analysis to recover an unbiased estimate of the Mean Causal Effect of A on Y. As per the â€œtraditionalâ€ definition of confounding, a variable L is a confounder of the Intervention-Outcome relationship if it meets the following three criteria: L is not a â€œdownstream consequenceâ€ of Intervention A (i.e., L occurs in time before Intervention A) L is associated with Intervention A L is associated with Outcome Y within levels of Intervention A (i.e. when conditioning on A) We would like to confirm the following three criteria in our dataset empirically: Given we have empirically confirmed the three criteria above, we can now specify the causal structure of our problem. We have â€œprovenâ€ that the true causal structure is specified by the Causal Directed Acyclic Graph (DAG) shown below in Figure 1: Given the established Causal DAG in Figure 1, we would like to recover an unbiased estimate of the Mean Causal Effect of Intervention A on Outcome Y. In the marginal DAG, there is an open back-door associational path A <- L -> Y, meaning we need to â€œadjustâ€ for L. So weâ€™re done! â€¦ Or are we? â€¦ Is there possibly something amiss here?? 3. The Structure of M-Bias So what is the issue with our analysis in Section 2? Well, it turns out, the structure of our problem is in-truth different than we thought. Figure 2 below shows the true causal structure of our problem: In the Causal DAG in Figure 2, variables U1 and U2 are unmeasured (we do not have access to them in our dataset). And importantly, itâ€™s evident that variable L is not a â€œconfounderâ€ of the A-Y relationship even though L meets all three criteria specified in the previous section. It turns out the three criteria listed in Section 2 are not sufficient to guarantee a variable is a â€œconfounderâ€. In Figure 2 variable L is a â€œcolliderâ€ between unmeasured variables U1 and U2, and there is no open back-door associational path from A to Y in the marginal Causal DAG. Rather A and Y are marginally d-separated. The causal structure presented in Figure 2 is referred to as M-Bias in the academic literature. Given A and Y are marginally d-separated, the marginal effect estimate of A on Y recovers an unbiased estimate of the Causal Effect Difference of A on Y. And importantly, what would happen if we were to condition on L? Again, given the Causal DAG in Figure 2, L is a collider between unmeasured variables U1 and U2. If we were to condition on L, we would be opening an associational backdoor path from A to Y via A <- U2 -> [L] <- U1 -> Y. Implicitly this is a form of selection bias. Therefore, not only is there not confounding by L, but conditioning on L would cause bias (i.e. selection bias) in our analysis, not prevent it. Aside on origins of the name â€œM-Biasâ€: When presenting a Causal DAG visually, I like to rank order my variables in the graph from left to right in the order said variables occur in time. This is simply a preference of mine; I find Causal DAGs drawn in this manner easier to analyze and understand. However, because in a Causal DAG the direction of causality between any two variables is already mathematically encoded in the direction of the edge (i.e. arrow) between them, rank-ordering the variables visually in their causal-order is not required. In the academic literature, it is far more common to see the Causal DAG in Figure 2 visually presented as the figure to the bottom right: The two Causal DAGs shown above are one in the same. Given the clear â€œM structureâ€ of the figure on the right above, the origins of the name M-Bias are hopefully clear. 4. Takeaways, and the Challenges with Causal Discovery Letâ€™s recap where we are. In Section 2, we performed a small empirical â€œexplorationâ€ of our dataset in an attempt to determine the causal structure of our problem. We determined variable L was a â€œconfounderâ€ and needed to be â€œadjustedâ€ for. However, in Section 3 we revealed that our understanding of the causal structure from Section 2 was incorrect. Variable L is in-fact not a confounder, and given the true Causal DAG in Figure 2, â€œadjustingâ€ for L would cause bias in our analysis (i.e. selection bias), not prevent it. So what is the takeaway message here? The lesson is that empirical analysis of our sample data cannot alone definitively reveal the â€œtrueâ€ causal structure of a problem with certainty. Rather our understanding of the causal structure should be supplemented with subject-matter domain knowledge. The small â€œexplorationâ€ analysis we conducted in Section 2 is a mini-example of a process called â€œCausal Discoveryâ€, a topic I plan on writing a detailed piece about in the future. In short, Causal Discovery is hard. Very hard in fact. It relies on quite a number of empirical assumptions that are often untestable and/or unverifiable. M-Bias is but one example that throws a wrench into most Causal Discovery techniques. In short, in the interest of recovering an unbiased estimate of the causal effect of an Intervention on an Outcome in a non-randomized setting, when determining if a variable should be â€œadjusted forâ€ it is important to also lean on domain and subject-matter expertise to inform the specification of the causal structure of a problem, and not rely on blind empirical analysis of the sample data alone. 5. Computational Simulation of M-Bias Weâ€™re going to conduct a computational simulation in Python to investigate the M-Bias causal structure discussed in this piece. We will: Letâ€™s import our needed libraries: Simulate Dataset: Specify a function to simulate a dataset as the per the Causal DAG specified in Figure 2 above with the Causal Effect Difference of A on Y being null: Letâ€™s print the first few observations of our simulated dataset: Given we simulated our dataset with this toy example, we are in the unique position of having access to all of the data, included unmeasured variables U1 and U2. In a real-world analysis, we would be blind to said data. Letâ€™s look at what our dataset looks like if we were conducting this analysis in the â€œreal-worldâ€: In terms of the three â€œclassicalâ€ confounder criteria, we know that variable L was collected and occurs in time before Intervention A occurred. Therefore L cannot be a downstream consequence of A. Letâ€™s show L is associated with Intervention A: In the fit logistic regression model above, the coefficient estimate for the L term is 1.597 with 95% CI (1.58 , 1.61). There is ample empirical evidence that L is indeed marginally associated with Intervention A. Lastly, letâ€™s show L is associated with Outcome Y conditional on Intervention A: In the fit OLS regression model above, the coefficient estimate for the L term is 2.198 with 95% CI (2.19 , 2.21). There is ample empirical evidence that L is indeed associated with Outcome Y conditional on Intervention A. However, look at the coefficient estimate for the Intervention A term above, which should be our â€œunbiasedâ€ estimate of the Mean Causal Difference of Intervention a=1 and a=0 on Outcome Y. The Mean Causal Difference is estimate to be -0.445 with 95% CI (-0.454 , -0.435). Given we simulated our dataset, we know that this result is incorrect, and the true Causal Difference is null. Given the Causal DAG in Figure 2, and that Intervention A and Outcome Y are marginally d-separated, let us recover an estimate of the marginal effect of A on Y: In the fit OLS regression model above, the coefficient estimate for the A term is 0.0056 with 95% CI (-0.004 , 0.015). This is in fact, an unbiased estimate for the Mean Causal Effect Difference of Intervention A on Outcome Y (i.e. a null effect). Letâ€™s again repeat the analysis, but with a simulated dataset where the Mean Causal Effect Difference of A on Y is 1.62: We again see that the marginal effect estimate of A on Y correctly recovers an unbiased estimate of the Mean Causal Effect Difference of A on Y (1.623 , 95% CI (1.614 , 1.633)), while the effect estimate of A on Y conditional on L returns a biased estimate of the Mean Causal Effect Difference of A on Y (1.175 , 95% CI (1.165 , 1.185)). For the complete code/notebook for the above computational simulation, please see the following github link. 6. Wrap-up and Conclusions This concludes our explanation and working example of M-Bias in Causal Inference, and the challenges it presents with regards to confounding identification. As mentioned, M-Bias is but one potential issue that makes empirical Causal Discovery very challenging. In a future piece, I will delve into methods for Causal Discovery in great detail. I would also encourage anyone who found the above material insightful to create there own simulation examples, leverage my code as a starting point, and experiment on your own! If you would like to learn more about methods and considerations for Causal Inference, I would suggest textbooks â€œCausal Inference: What Ifâ€ by Miguel Hernan and Jamie Robins at Harvard (two of my former Professors), â€œCausalityâ€ by Judea Pearl at UCLA, and â€œProbabilistic Graphical Models: Principles and Techniquesâ€ by Daphne Killer at Stanford and Nir Friedman at Hebrew University of Jerusalem. These are all fantastic texts. I plan on writing more in-depth pieces on Causal Inference in the future. Causal Inference: What If Causality Probabilistic Graphical Models: Principles and Techniques I hope the above is insightful. As Iâ€™ve mentioned in some of my previous pieces, itâ€™s my opinion not enough folks take the time to go through these types of exercises. For me, this type of theory-based insight leaves me more comfortable using methods in practice. A personal goal of mine is to encourage others in the field to take a similar approach. Iâ€™m planning on writing based pieces in the future, so feel free to connect with me on LinkedIn, and follow me here on Medium for updates!
Andrew Rothman 6 days agoÂ·13 min read 1: Background and Motivation Causal Inference is of interest to a wide range of practitioners including Statisticians, Data Scientists, Machine Learning Scientists, and other Computational Researchers. Recovery of unbiased estimates of Causal Effects is at times a tough task. In my previous pieces on Doubly Robust Estimation and G-Estimation of Structural Nested Models, we discussed leveraging G-Methods in non-randomized settings. We also discussed issues with M-Bias and confounding identification in non-randomized settings. Contrary to popular belief, recovery of valid estimates of Causal Effects can be equally as difficult (or sometimes even unidentifiable and impossible) with A/B Testing (aka randomized trials). Regarding the use of A/B Testing in industry settings, Iâ€™ve noticed an often over-simplified view of randomized trial designs; a view of A/B Testing that corresponds to â€œidealized randomized trialsâ€. For A/B Tests conducted at scale, particularly those that are longitudinal and carried-out over lengthy periods of time, very rarely do they meet the assumptions of an idealized randomized trial. For example, letâ€™s assume you work for a technology company that hosts an online app. Your department is interested in measuring the impact of a new feature design on the user experience. You are tasked with recovering an unbiased estimate of the Mean Causal Effect of Intervention Z (the new feature design) on outcome Y (the click through rate, or some other success metric). You design your A/B Test by randomly assigning users via IP-address to a version of Intervention Z (experience the new feature design vs. not). You carry-out your experiment collecting data on Outcome Y (success metric). As a function of your observed data, you recover an empirical estimate of the mean marginal effect difference of Z on Y. You may naively assume the â€œtrue causal structureâ€ of your A/B Test corresponds to the Causal Directed Acyclic Graph (DAG) below: â€¦ with the assumption that the recovered marginal empirical estimate is an unbiased estimate of the Mean Causal Effect Difference of Z on Y: Well, the above Causal DAG is under the following conditions of an â€œidealized randomized trialâ€: No Intervention cross-over, with perfect subject adherence No informative censoring of outcome Y due to loss-to-follow-up The above conditions rarely hold in real-world A/B Tests. As inconvenient as this may be to hear, the true causal structure of most large-scale real-world A/B Tests usually look something more like this: Due to informative censoring (conditioning on C, and by proxy condition on L), randomized intervention assignment Z and outcome Y are clearly not d-separated in the DAG above due to the open path from Z -> A -> [L] <- U2 -> Y. Therefore, the marginal empirical estimate of the effect of Z on Y over the available uncensored data will not recover an unbiased estimate of the Mean Causal Effect of Z on Y, and our inferences will be incorrect. In this article, we will clearly walk through an example where a large-scale long-term A/B Test corresponds to something like the complex DAG above, and how G-Methods (specifically Inverse Probability Weighting with Marginal Structural Modeling) is the only appropriate method for adjustment due to informative censoring. We will also conduct a computational simulation to investigate these methods. The contents of this piece are as follows: 2. Subject non-adherence, Intervention Cross-over, and the Intent-to-Treat (ITT) Estimator Again, starting with an idealized randomized trial, we have the following Causal DAG: Given Z is randomized, there are by definition no â€œcausesâ€ (i.e. arrows) into Z. However, itâ€™s important to recognize that Z is in truth just the â€œrandomized intervention assignmentâ€, the key-word being assignment. There may be a small percentage of subjects that non-adhere to their randomized assignment and instead experience (i.e. observe) the other intervention arm. The intervention observed is A, of which Z is a direct cause of. And given A is not randomized, there may be unmeasured common-causes U1 of A and Y. Let us postulate a descriptive use-case. We randomize subjects to intervention assignment Z based on their IP address. However, some heavy-tech using subjects may have multiple IP addresses (represented by U1). This allows those subjects to possibly experience the intervention arm they explicitly want (i.e. observed intervention A). Being heavy-tech savvy with multiple IP addresses is also a direct determinant of the outcome Y (click-through rate). The Causal DAG we are left with is below: Itâ€™s additionally important to recognize from a scientific perspective the causal contrast we are most interested in estimating? We are not really interested in the Causal Effect of intervention assignment Z on outcome Y. Rather, we are really interested in the Causal Effect of observed intervention A on outcome Y. Given A and Y are not marginally d-separated in the Causal DAG above due to unmeasured common-cause U1 (and given itâ€™s unmeasured we cannot condition on it), what are we to do? We reasonably have two options: Note that randomized intervention assignment Z is in fact an Instrumental Variable (IV) of the Causal Effect of A on Y. Assuming our use-case meets certain homogeneity conditions, we could specify an IV estimator to recover an unbiased estimate of the Mean Causal Effect of A on Y among the compliers. I will not cover this method here, but rather write a future piece specifically on Instrumental Variable estimation. Recognizing that our problem is a superiority trial, the Mean Causal Effect of randomized intervention assignment Z on Y is the Intent to Treat (ITT) Estimator of the Mean Causal Effect of observed intervention A on outcome Y. This is the method we will leverage. In the context of superiority trials with a binary intervention, and under the assumption of no informative censoring, the expected value of the ITT estimator is â€œnull-preservingâ€. Mathematically this means the expected value of the estimated Mean Causal Effect of Z on Y will always be closer to the null than the true Mean Causal Effect of A on Y. Therefore, by recovering an estimate of the Mean Causal Effect of Z on Y, we are indirectly also recovering a hard lower-bound for the Mean Causal Effect we are interested in (A on Y). And importantly, because the ITT estimator is â€œnull preservingâ€, if the true Mean Causal Effect of A on Y is null, then the expected value of the estimated Mean Causal Effect of Z on Y will also be null. However, weâ€™re not quite done yet. Recall how I stated above that the ITT estimator is only valid under the assumption of no informative censoring, or that informative censoring has been adjusted for. This is the issue we will next tackle. 3. Informative Censoring due to loss-to-follow-up, and adjustment with Marginal Structural Modeling Starting with the DAG from the previous section where we described the ITT estimator, we now acknowledge that in our experiment we have informative censoring leading to the Causal DAG shown below: Specifically: There is a variable L that is a direct determinant of observed outcome A There are unmeasured common causes U2 of L and outcome Y Variable L is a direct and sole cause of variable C, being â€œcensoredâ€ in the analysis. By â€œcensoredâ€ we mean we â€œlose the subject to follow-upâ€. For censored subjects (C=1), we do not observe and measure their outcome Y. Therefore, these subjects are implicitly dropped from the analysis, given we do not observe the outcome information to include them. This is why in the Causal DAG above we condition on variable C. We do so because we have no choice. We only observe outcome information for subjects with C=0, and are restricted to this population for the analysis. Letâ€™s add to our descriptive use-case. Letâ€™s say before deploying our AB-test, we did not thoroughly check that the new feature was correctly configured or default-compatible with all popular (and some less popular) internet browsers. Letâ€™s say the new feature is inaccessible with the default settings on Internet Explorer. Explorer users first need to remove a default firewall block (represented by variable L). Predominant users of Explorer are older in age (unmeasured variable U2) which is also a direct determinant of the outcome Y (click-through rate). Many Explorer users therefore donâ€™t experience the intervention A at all, we donâ€™t measure their outcome Y, and they are â€œcensoredâ€ from the analysis (C=1). We call this structure informative censoring. Letâ€™s now discuss how we can adjust for informative censoring under the given conditions. As noted earlier in this article, randomized intervention assignment Z and outcome Y are clearly not d-separated in the DAG above due to the open path from Z -> A -> [L] <- U2 -> Y. Therefore, what are our options? The ITT estimator addresses confounding by U1, but does not address the informative censoring issue. So this alone cannot save us. An IV estimator alone cannot save us either for the same reasoning as the ITT estimator. If we had recorded data on U1 and U2, we could adjust for these variables using standard conditional methods. However, given U1 and U2 are unmeasured, this is not an option. We additionally may not be willing to assume we even know what U1 and/or U2 represent; simply that they exist. We may be able to leverage the class of G-Methods? Perhaps Standardization with the g-formula or Inverse Probability Weighting (IPW) with Marginal Structural Modeling? We cannot use Standardization via the g-formula, given we donâ€™t observe the outcome Y for those that are censored (C=1). We can however leverage IPW with Marginal Structural Modeling, given we observe data on the intervention for all subjects. We can recover IPWs for each observation via specifying the following Causal SWIG (Single World Intervention Graph): As we are only interested in recovering estimates of the Mean Causal Effect in a pseudo-population where all observations where uncensored (c=0), we specify the IPWs for uncensored observations accordingly, and IPWs of zero for all observed observations that were censored (C=1): If we empirically compute the appropriate IPW for each observation in our observed dataset, we can generate a pseudo-population by weighting each observation by itâ€™s corresponding IPW. This pseudo-population will correspond to the Causal SWIG in the figure above. We then fit the following Marginal Structural Model on the pseudo-population by Weighted Least Squares (WLS) Regression: And thatâ€™s it! Weâ€™ve recovered a null-preserving lower-bound for our mean causal contrast of interest. A bit more involved than your typical marginal estimate. Letâ€™s proceed to our computational simulation. 4. Computational Simulation of Toy Example with Marginal Structural Modeling Weâ€™re going to conduct a computational simulation in Python to investigate A/B testing with adjustment for informative censoring by Marginal Structural Modeling. We will: Letâ€™s import our needed libraries: Simulate Dataset: Specify a function to simulate a dataset as the per the Causal DAG specified in Figure above with the Causal Effect Difference of A on Y being null: Letâ€™s print the first few observations of our simulated dataset: Given we simulated our dataset with this toy example, we are in the unique position of having access to all of the data, included all censored data and variables U1 and U2. In a real-world analysis, we would be blind to said data, motivating the need for the G-Methods described in this article. Letâ€™s look at what our dataset looks like if we were conducting this analysis in the â€œreal-worldâ€: As shown above, in our â€œreal-worldâ€ observed dataset we do not have access to data on variables U1 and U2, nor do we observe the outcome data Y for observations that were censored (C=1). We need to leverage techniques for informative censoring adjustment using only the data we have above. This is the motivation for leveraging G-Methods. Letâ€™s examine the percentage of observations in our observed dataset that are censored (C=1): Shown above, approximately 8% of the dataset is censored, for which we donâ€™t observe the outcome Y. Letâ€™s next recover our Inverse Probability Weights (IPW) as described in this article, in preparation for the Marginal Structural Model: Letâ€™s now conduct a â€œnaiveâ€ analysis; the naive ITT estimator of Z on Y in the observed data, throwing away and â€œignoringâ€ the seemingly small and unimportant 8% of observations for which we did not observe outcome Y. Keeping in mind that given we simulated this dataset, we are all-knowing and know the true Causal Effect Difference of A on Y is zero. So given the ITT estimator is null preserving, the ITT estimator should also be null. Letâ€™s see what are naive analysis returns: As shown below, our â€œnaiveâ€ ITT analysis returned a significant Causal Effect Difference of Z on Y of -0.2035, with 95% CI (-0.211 , -0.196). Certainly different than zero. What a great deal of trouble our â€œnaiveâ€ assumption of simply ignoring the 8% of censored observations has caused us! Letâ€™s now fit the Marginal Structural Model described in this article, returning an ITT estimator adjusting for informative censoring: We now find an estimate of a null Causal Effect of Z on Y, with point estimate 0.001 and 95% CI (-0.007 , 0.009). This is in fact the correct result! We have provided an example where the Marginal Structural Model correctly returns a null result in the case of the true Causal Effect being null in the presence of informative censoring. What about when the true Causal Effect Difference of A on Y is not null? Letâ€™s simulate a dataset where the true Causal Effect Difference of A on Y is 0.11 Letâ€™s examine the naive analysis, returning the ITT estimate without adjusting for the informative censoring: Above we have an ITT estimate of -0.1017 with 95% CI (-0.109 , -0.094). From this result, not only is our naive ITT estimate wrong in magnitude and not null-preserving, but it qualitatively lead to the wrong scientific conclusion. The effect estimate is in the wrong direction! Again, quite a problem throwing away those seemingly unimportant 8% of censored observations caused us. Letâ€™s conduct the analysis adjusting for the informative censoring: With the Marginal Structural Model above, we recovered an unbiased ITT estimate with point estimate 0.1037 and 95% CI (0.096 , 0.111). This is in fact the correct result. For the complete code/notebook for the above computational simulation, please see the following github link. 5. Wrap-up and Conclusions This concludes our explanation and working example of Marginal Structural Modeling to adjust for informative censoring in A/B Testing. Whenever conducting an A/B Test or randomized trial, itâ€™s important to critically think through these issues. Particularly if you are â€œdroppingâ€ observations from your analysis due to incomplete information, there is high likelihood your use-case may suffer from informative censoring. Too often, we are content on making â€œidealized randomized trialâ€ assumptions regardless if they are reasonable in our use-case or not. I would like to see that change, and see folks work through these issues more critically with care. Additionally, there are sensitivity analyses one can conduct to investigate how strong the informative censoring need be to substantively change an effect estimate. There are situations where, even in the presence of informative censoring, the scientific conclusions of an analysis wonâ€™t change substantively. However, we should still take care to critically think through these issues when conducting A/B Tests. I would also encourage anyone who found the above material insightful to create there own simulation examples, leverage my code as a starting point, and experiment on your own! If you would like to learn more about methods and considerations for Causal Inference, I would suggest textbooks â€œCausal Inference: What Ifâ€ by Miguel Hernan and Jamie Robins at Harvard (two of my former Professors), â€œCausalityâ€ by Judea Pearl at UCLA, and â€œProbabilistic Graphical Models: Principles and Techniquesâ€ by Daphne Killer at Stanford and Nir Friedman at Hebrew University of Jerusalem. These are all fantastic texts. I plan on writing more in-depth pieces on Causal Inference in the future. Causal Inference: What If Causality Probabilistic Graphical Models: Principles and Techniques I hope the above is insightful. As Iâ€™ve mentioned in some of my previous pieces, itâ€™s my opinion not enough folks take the time to go through these types of exercises. For me, this type of theory-based insight leaves me more comfortable using methods in practice. A personal goal of mine is to encourage others in the field to take a similar approach. Iâ€™m planning on writing based pieces in the future, so feel free to connect with me on LinkedIn, and follow me here on Medium for updates!
Agni Kumar 5 hours agoÂ·8 min read Undoing randomization is tricky. Letâ€™s consider a randomly scrambled sequence of letters. With a naÃ¯ve unscrambling strategy, we ask: Does how â€œfar awayâ€ the scrambled text is from being a real English word impact the time it takes to decode it? How? And what happens when we change our decoding strategy? Letâ€™s attempt to quantify some relationships between scrambled texts and decoding type. Encoding and decoding Weâ€™ll evaluate a decryption method that attempts to unscramble a text encoded by a substitution cipher, by repeatedly (with some exchange probability) randomly switching all incidences of two letters at a time. We would like to know whether this method will in general result in a successful decoding, and if so, how long we can expect this process to take (i.e., how many steps before we reach the original text). We model the problem as an absorbing Markov chain, which is a Markov chain containing states that are impossible to leave. We present both theoretical and experimental results for small text lengths, which differ for reasons to be explored in the future. In addition to quantifying the relationship between the time to decode and text length, we also examine how time to decode changes as the exchange probability varies, as well as when the difference between a scrambled textâ€™s formulation and those of general English words increases. Unscrambling text We give a concrete application of Markov chains, specifically in the context of understanding randomness. We present a possible message decryption method based on random substitutions. Given a message encoded using a simple substitution cipher, we want to decode the ciphertext using the following (to simplify the problem, we restrict the set of characters for use in the substitution cipher to include only the characters already included in the original message): We obtain an incidence matrix of the English language, where rows and columns are the letters of the alphabet and entries are the probabilities with which two symbols typically follow each other. We obtain an encoded version of the text using a simple substitution cipher, restricted to the characters included in the original text. For simplicity, we take the original text to be an English word with non-repeating characters. Comparing the incidence matrix of the encoded text with the one for the English language (referred to as typical), we exchange the positions of two randomly chosen characters and then run the same comparison again. If the text has become more â€œtypicalâ€™â€™ (closer by Euclidean distance to the typical incidence matrix), we accept the change and continue with the new text. However, if the text has become less typical, we accept the change only with some set probability (referred to as the exchange probability, or switch_prob) and discard it otherwise. If on the n-th turn we obtain the original message, we stop the decoding process and record the number of steps. Absorbing Markov chains An absorbing Markov chain is a Markov chain such that there exists at least one state which is impossible to leave, called an absorbing state, and from any state it is possible to get to an absorbing state (not necessarily in one step). Our message decryption method can be represented by a Markov chain since the probability distribution for the word we obtain on each subsequent turn is not dependent on any of the words that came before in the chain. It can be represented by an absorbing Markov chain because, once we have successfully decoded the text, we stop switching letters, which is represented by setting the probability of getting the decoded text on the next turn, given that the current state is the decoded text, to one (and consequently the probabilities of getting any other encoded versions on the next turn to zero). Decryption method Recall that we originally had two questions about this decryption method: Will the method definitely result in a successful decoding? If so, how long we can expect this process to take? To answer the first question, we make use of the theorem below. Theorem 1 â€” The probability that an absorbing Markov chain will eventually reach an absorbing state is 1. Since our Markov chain has only one absorbing state, namely the original text, the probability that the chain will eventually reach the desired state is 1. Our decoding method is guaranteed to work. To answer the second question, we write the transition matrix associated with our chain as the block matrix Here, Q and R are t Ã— t and t Ã— 1 dimensional matrices, respectively, where t is the number of non-absorbing states, i.e., the number of possible encrypted versions of the text which are not the original text. The row {0, 0, â€¦, 0, 1} represents the original text. We define the fundamental matrix N = (Iâˆ’Q)â»Â¹, if this exists. Theorem 2 â€” The matrix N as defined above exists, and the expected number of steps from the state represented by the i-th row of Q is equal to the i-th entry of the vector Nc, where c = (1, 1, â€¦, 1)áµ€. Proofs for the theorems above are contained here. Letâ€™s use these to calculate the expected number of steps to decode various permutations of a three-letter English word, â€˜yesâ€™. Experiments Letâ€™s compute the fundamental matrix and expected number of steps for each permutation when the original word is â€˜yesâ€™. Ordering the permutations, from least to most typical, we have: â€˜syeâ€™, â€˜eysâ€™, â€˜seyâ€™, â€˜yseâ€™, â€˜esyâ€™, and â€˜yesâ€™. The transition matrix looks like so: Then N = (Iâˆ’Q)â»Â¹ is the following, and Nc = (7.5, 5.5, 5.5, 6.5, 5.5)áµ€. The expected numbers of steps to absorption from each of the states â€˜esyâ€™, â€˜eysâ€™, â€˜seyâ€™, â€˜syeâ€™, and â€˜yseâ€™ are 7.5, 5.5, 5.5, 6.5, and 5.5, respectively. Simulations We present simulations to evaluate whether the aforementioned method efficiently decodes texts. (In theoretical computer science, usually every polynomial time algorithm is considered efficient; an O(log n) algorithm is considered highly efficient, since as n increases, the ratio of the number of operations to the size of the input decreases and tends to zero.) Text data To begin, we constructed the English incidence matrix given depicted below using frequencies of bigrams (pairs of consecutive letters). Row labels are the first letters of the bigrams and column labels are the second. Cells are colored according to their corresponding bigramsâ€™ probabilities, where warm colors indicate higher values and cool colors indicate lower ones. For a given text, we constructed an incidence matrix for it, by examining its bigram frequencies. We also code the functions get_subpart_typical(text, typical), to obtain a submatrix from the English incidence matrix by combining the rows and columns associated with a textâ€™s characters, and get_dist_from_typical(text, typical), to compute the Euclidean distance between a textâ€™s incidence matrix and the typical one. The decoding scheme was applied over texts of multiple lengths and over multiple exchange probabilities. The distribution of text lengths in the British National Corpus, from which texts were drawn, is displayed below. Given the time taken to run the simulations, ten random texts from each of seven length categories (lengths two through eight characters) were used. Decoding time by text length To quantify the time taken to decode texts, we code the function decode(typical, text, encoded_text, switch_prob), which computes the number of two-character exchanges required to obtain the original text. The function, which repeatedly randomly switches two letters of an encoded string with the goal of minimizing distance from a typical incidence matrix, was applied to several texts. Below, switch_prob is taken to be 0.5. The plots depict distributions of the number of exchanges over various text lengths, where each point represents a different text upon which the experiment was run. The right plot is a zoomed-in version of the left. The box and whisker plots depict several measures of spread. For longer text lengths, the numbers of exchanges differ quite a bit between texts, but it is clear that over text length, the time to decode increases exponentially. This trend is more clearly evident in the plots below, which depicts averages of the number of exchanges taken to decode the text over all experiments in each of the length classes. Error bars represent one standard deviation of uncertainty. A log-log plot of the left panel of the figure above is useful for recognizing an exponential relationship between increasing text length and time to decode, given the linear trend observed here. Decoding time by similarity to English In addition, we examined how the number of exchanges changes as the distances between textsâ€™ incidence matrices and their corresponding typical submatrices increase. A visualization is given below, where each point represents a text upon which the experiment was run. As expected, we see that longer texts are associated with smaller distances from their associated typical submatrices. That longer texts are associated with high decode times is also captured. We also examined how the number of exchanges changes as the switch_prob is varied, as shown below. Each point represents a text upon which the experiment was run. For each exchange probability value, it is evident that the time to decode increases exponentially as text length increases linearly. For longer texts, the time to decode fluctuates wildly between text lengths; it is not completely clear that always accepting an exchange (switch_prob = 1) yields a longer decode time, but this could be further investigated by experimenting over more texts with longer lengths. All code for this particular Markov chain application is given here. Through simulation, we examined trends for the number of transitions taken to reach a certain state (the original text) from a random start state (the encoded text). The experiments involved achieving order from randomness, and showed that the time taken to do so is exponential, but that varying the exchange probabilities on the transitions does not affect decode time much. Conclusions We evaluated a decryption method represented by an absorbing Markov chain. Through simulation, we observed how long it took to unscramble text as its length and similarity to actual English words varied, unearthing interesting patterns. In this paper, we explore the opposite direction, asking how long it takes for a process to become approximately random. Check it out if youâ€™re curious!
Frank Andrade 5 hours agoÂ·7 min read When it comes to presenting data, spreadsheets and reports full of text arenâ€™t enough to explain what we found. This is when we need data visualization to present the data in a way that helps everyone grasp difficult concepts. In this article, Iâ€™ll show you the best free options to easily start making stunning visualizations â€” leaving sophisticated options like Power BI and Google Studio. I selected these 8 online tools because theyâ€™re easy to use, have great graphic design, can be produced without writing code and are free. On top of that, all of them are interactive, so the graphs are more playful and contain more detail. Since some visualization tools come and go, I included only those that most likely would be in the market for a long time, so itâ€™ll be worth learning them. As I mentioned before, all the tools listed donâ€™t require writing code, but if youâ€™re interested in learning to make visualization with code check this article. 1. Flourish Flourish allows you to create stunning charts, maps and interactive stories. Itâ€™s straightforward and contains a template library with different visualization options. Who should use Flourish? Anyone interested in data storytelling. The different visualization and animation options help with it. Pros of the free version Apart from the usual charts and maps, you can create bar races, quizzes and carousels with Flourish. Itâ€™s great for storytelling. You can create impressive â€œscrollyâ€ stories, with no custom code. It offers a lot of options to customize visualization. Cons of the free version You canâ€™t keep your data and projects private. All data uploaded is public. You canâ€™t connect to live CSV or Google Sheets files in the free version. Example This is a bar race of the population in different countries between 1960 and 2016. You can filter out countries in specific continents by clicking on the labels. 2. DataWrapper Datawrapper is a tool that allows you to create interactive charts, maps and tables. No design skills are required since they already took care of the many design rules one must follow for great data visualization. On top of that, the charts, maps and tables you create with Datawrapper are readable on all devices. Who should use DataWrapper? The tool is primarily designed for creative writers, bloggers, and journalists. In general, Datawrapper helps people produce great visuals to accompany their articles. Pros of the free version You can connect to live CSV or Google Sheets files. The free plan allows you to create and publish unlimited charts, maps and tables. Cons of the free version The free version only allows you to export graphs on PNG files. In case you want an upgrade, the paid plan starts at 499â‚¬/month. Example This is a scatter plot I created to rank Netflix shows. 3. Chartblocks Chartblocks is part of Ceros, a cloud-based design platform that allows marketers and designers to create immersive content without writing a single line of code. ChartBlocks helps create charts that look great quickly and easily in just a couple of minutes. Some of the types of charts available are bar, line, scatter and pie. To make a chart, you just need to create a free account. Who should use Chartblocks? Anyone who wants to create simple data visualizations in little time. Pros of the free version The process of creating charts is one of the simplest and most straightforward among all the options. You can make your charts private with the free version. Cons of the free version Only allows up to 50 active charts. Only supports 50,000 monthly views. You can only upload CSV or Excel files. The maximum file size is 50MB. Example This is a simple column chart that shows the number of lines of the main Game of Thrones characters in 7 seasons. It only took a couple of minutes to create it. 4. Infogram Infogram is a visualization tool that helps create beautiful content. Apart from the typical bar, line and pie chart, Infogram also allows you to create infographics and reports easily. Who should use Infogram? People who are interested in creating more than simple visualizations. Pros of the free version Unlike other options, Infogram contains animation to set objects to zoom, bounce, flip, fade, and slide effortlessly. You can add elements, graphics and shapes to your visualizations. Cons of the free version The free version only lets you create up to 10 projects with 5 pages per project. Infogram has more than 550 map types for you to choose from; however, only 13 map types are available in the free version. The free version only allows you to keep data public. No data connection and live data in the free version. Example This is one of the free maps available on Infogram. You can add any element and animations to improve the visualization. 5. Chart Studio Plotlyâ€™s powerful, web-based online chart creator. Itâ€™s an editor for creating D3.js and WebGL charts. Who should use Chart Studio? As written on its website, Chart Studio is for everyone who creates and views charts. Data scientists, dash developers, report & slide designers and journalists are some examples. Pros of the free version In case you know how to code, Chart Studio has API available for Python, R, Julia, and MATLAB. Cons of the free version Limits public charts up to 1000 views per day. You can upload data in more than 20 file types and connect to SQL through Falcon, Plotlyâ€™s free SQL client; however, the free version only allows you to upload data from Excel and CSV files. You can only export in PNG and JPEG. Example I created this scatterplot to analyze the effect of the number of spoken words on IMDb Ratings for every episode of the TV show Avatar. 6. Knight Lab They offer open-source, adaptable, and lightweight tools for media makers that help tell better stories. Their data visualization tools are different from the other options. You can tell stories behind the numbers with Storyline, tell stories with maps with Storymap and timelines with Timeline. Who should use Knight Lab? Itâ€™s mainly designed for journalism, but it can be used for anyone who likes storytelling through visualizations. Pros of the free version You only need a Google spreadsheet to create timelines, storylines or storymaps. If you know how to code, you can dynamically create or update timelines through JSON data format. Cons: Unlike the other data visualization options, Knight Lab doesnâ€™t offer the traditional charts and tables commonly used. Example This is a free template available on Timeline. 7. Tableau Public This is one of the most popular options, but for those who never thought of Tableau Public, itâ€™s a free platform to publicly share and explore data visualizations online. You can easily create interactive graphs, maps, and live dashboards in just minutes. Who should use Tableau Public? For any data enthusiast who wants to create stunning interactive visualizations. Pros of the free version All visualizations on Tableau Public can handle millions of viewers at no cost. There are lots of tutorials available on how to use Tableau. It has a lot of options to customize your graph. Cons of the free version Visualizations published to Tableau Public are available for anyone to see online. Tableau Public is a platform for public (not private) data. You can import data from CSV, Excel and Google Sheets. However, you canâ€™t connect to a database with the free version. Example This visualization shows Tableau Public authors from around the world. Last but not least Although you canâ€™t create interactive visualizations with RAWGraphs, itâ€™s still worth mentioning. RAW Graphs aims to provide a missing link between spreadsheet applications like Microsoft Excel and vector graphics editors such as Adobe Illustrator, Inkscape, and Sketch. Pros of the free version The creation of graphs is really straightforward and you donâ€™t even need to create an account to start working. Different options to upload data such as TSV, CSV, DSV, JSON and Excel (.xls, .xlsx) files. According to RAWGraphs, the data uploaded will be processed only by the web browser. No server-side operations or storages are performed. No one will see, touch or copy your data. RAWGraphs is scalable. You can add new charts with a basic knowledge of D3.js. Cons of the free version The graphs are sometimes too simple and there are few ways to customize them as you want. The visualizations are not interactive. Example This visualization shows the population of some cities in a hierarchical structure. Final thoughts There isnâ€™t a perfect data visualization tool. Thatâ€™s why itâ€™s a good idea to make the most of at least 2 or 3 of them since they complement each other. Now you know 8 new tools to create amazing interactive data visualizations without writing any code. Once you master a couple of them, I recommend using data visualization libraries in your favorite programming language to customize your charts even further. In case you code in Python, check this article to learn how to make great visualizations with Plotly, Seaborn and Matplotlib. Avatar Meets Data Visualization Refresh your Data Visualization skills with Team Avatar! towardsdatascience.com
"Khuyen Tran 6 hours agoÂ·5 min read Motivation If you have applied machine learning or deep learning for your data science projects, you probably know how overwhelming it is to keep track and compare different experiments. In each experiment, you might want to track the outputs when you change the model, hyperparameters, feature engineering techniques, etc. You can keep track of your results by writing them in an Excel spreadsheet or saving all outputs of different models in your Jupyter Notebook. However, there are multiple disadvantages to the above methods: You cannot record every kind of output in your Excel spreadsheet It takes quite a bit of time to manually log the results for each experiment You cannot keep all outputs of your experiments in one place You cannot visually compare all outputs What if there is a way to track and visualize all experiments like below in 3 lines of code? This is when Weight & Biases becomes handy. What is Weight & Biases? Weight & Biases is a free Python library that allows you to track, compare, and visualize ML experiments. It is also integrated with many popular libraries such as TensorFlow, PyTorch, Kera, Scikit, Hugging Face, and XGBoost. To install Weight & Biases, use Sign up for a free account by going to the sign up page. Then login using Thatâ€™s it! Now youâ€™re ready to use wandb. Get Started In this tutorial, we will work with the famous Iris flower data set then use Weight & Biases to track the results. To track the results, start with initializing the project Now all outputs for this experiment will be saved under â€˜irisâ€™ project: Then log all metrics using Put everything together Thatâ€™s it! Now we are ready to run our script. You should see something like this If you click the link provided in the output, you should see something like below The output now is a little bit boring since there is just one experiment. Letâ€™s experiment with different models then compare them with one another. Compare Different Experiments Start with initializing the model. This time, we will add reinit=True to allow reinitializing runs. We will also group the runs according to model to compare the outputs of different models. Put everything together Awesome! Letâ€™s see how the outputs look Looks cool! Now we can compare the results of different runs by looking at the charts automatically created by wandb . Since we are more interested in grouping the results by model, letâ€™s adjust the grouping. The video below shows you how to do that. After adjusting the grouping, you should see something like this: It is much clearer now! Based on 3 charts, LinearDiscrimnantAnalysis seems to be the best model since it has the highest f1 macro score and accuracy and has the lowest negative log loss. You can also view the results by clicking the table icon It also allows you to hide or move columns or sort the table by a certain variable as shown below: Visualize Sklearn Models wandb also allows us to create common plots to evaluate Sklearn models with built-in functions. To visualize all classification plots, Put everything together Run python sklearn_model.py , we should see 8 different classification plots for a run like below! View all plots here. Find all possible plots and other integrations here. Save Model If you want to reproduce the result, you need to save the model. wandb allows you to do that by using wandb.save() Start with saving the model as a local file, then save it in wandb Put everything together Click the run link, then click the Files tab, you should see something like this In this tab, many files are automatically logged including summary, terminal outputs, config files, requirements.txt, and the model we have just saved. Click the download button next to the model to download the model. When clicking the model.pkl, you should see information about your model as shown below: You can also find other information about your run including the overview, charts, system, terminal logs, and files by clicking tabs on the left-hand side of the run. Pretty cool, isnâ€™t it? Conclusion Congratulations! You have just learned how to keep track of your ML experiments using Weight & Biases. With Weight & Biases, you can log everything you can imagine, including metrics, files, images, terminal outputs, and much more! Now you can keep everything in one place, compare different experiments, and reproduce the results in several lines of code. I encourage you to explore the documentation here. The source code for this article could be found here. I like to write about basic data science concepts and play with different algorithms and data science tools. You could connect with me on LinkedIn and Twitter. Star this repo if you want to check out the codes for all of the articles I have written. Follow me on Medium to stay informed with my latest data science articles like these: How to Create Fake Data with Faker You can either Collect Data or Create your Own Data towardsdatascience.com Introduction to Hydra.cc: A Powerful Framework to Configure your Data Science Projects Experiment with Different Parameters and Models without Spending Hours Fixing your Code! towardsdatascience.com Introduction to Schema: A Python Libary to Validate your Data Validating your data just Gets more Pythonic! towardsdatascience.com Introduction to DVC: Data
Version Control Tool for Machine Learning Projects Just like Git, but with Data! towardsdatascience.com Introduction to DVC: Data
Version Control Tool for Machine Learning Projects Just like Git, but with Data! towardsdatascience.com"
Daniel Bourke 2 days agoÂ·11 min read I watched the keynote and saw the graphs, the battery life, the instant wake. And they got me. I started to think, how could one of these new M1-powered MacBooks make their way into my life? Of course, I didnâ€™t need one but I kept wondering what story could I tell myself to justify purchasing another computer? Then I had it. My 16-inch MacBook Pro is too heavy to carry around all the time. Yeah, thatâ€™ll do. This 2.0 kg aluminium powerhouse is too much to be galavanting. Waitâ€¦ 2.0 kg, as in, 4.4 pounds? Thatâ€™s it? Yes. Wow. Itâ€™s not even that heavy. Câ€™mon nowâ€¦ letâ€™s not let the truth get in the way of a good story. I had it. My reason for placing an order on a shiny new M1 MacBook (or two). My 16-inch MacBook is too heavy to lug around to cafes and write code, words, edit videos and check emails sporadically. And Apple seems to think their new M1 chip is 11x, 15x, 12x, 3x faster on a bunch of different things. Thought-provoking numbers but Iâ€™ve never measured any of these in the past. All I care about is: can I do what I need to do, fast. The last word of the previous sentence is the most important. Iâ€™ve become conditioned. Speed is a part of me now. Ever since the transition from hard drives to solid-state drives. And Iâ€™m not going back. I bought the 16-inch in February 2020. Iâ€™d just completed a large project and was flush with cash, so I decided to future proof my work station. Since I edit videos daily and hate lag, I opted for the biggest dawg I could buy and basically maxed everything except for the storage (see the specs below). Thankfully Iâ€™ve still got a friend at Apple who was able to apply their employee discount to the beast (shout out to Joey). Anyway, weâ€™ve discussed my primary criteria: speed. Letâ€™s consider the others: Why test/compare them Why not? But really, Iâ€™m a nerd. And an Apple fan. Plus, I wanted to see how my big-dawg-almost-top-of-the-line 16-inch MacBook Pro faired against the new M1 chip-powered MacBookâ€™s. Plus, I canâ€™t remember being this excited for a new computing device since the original iPhone. Other reasons include: carrying around a lighter laptop and tax benefits (if I buy another machine before the end of the year, I can claim it on tax). Mac specs Whenever I buy a new machine, I usually upgrade the RAM and the storage at least a step or two from baseline. 512GB storage and 16GB RAM seems to be the minimum for me these days (seriously, who is running a 128GB MacBook effectively?). So for the M1 MacBookâ€™s, I upgraded both of their RAM from 8GB to 16GB and for the 13-inch Pro, I upgraded from 256GB to 512GB storage. The 16-inch MacBook is my current machine, which Iâ€™ve never had a problem with until running the tests below. *Price (actual) is the actual price I paid for each model. Note for the MacBook Pro 16-inch, I actually paid ~$5,500AUD since I have a friend who works at Apple and applied his employee discount (thank you Joey). **Price (baseline) is the price youâ€™d pay if you upgraded all processing components (e.g. 8GB -> 16GB RAM on the M1 models and 2.3GHz -> 2.4GHz on the Intel model) except storage (since storage is usually the most expensive upgrade). The tests Appleâ€™s graphs were impressive. And the GeekBench scores were even more impressive. But these are just numbers on a page to me. I wanted to see how these machines performed doing what Iâ€™d actually do day-to-day: Writing words. I assume they all perform well at this. Browsing the web. Same as above. Editing videos. One of the primary uses I was interested in and one of the main reasons I bought the 16-inch MacBook Pro with dedicated GPU. Writing code. A text editor doesnâ€™t require much but Xcode is getting pretty hefty these days. Training machine learning models. I write a lot of machine learning code. I donâ€™t expect to be able to train state-of-the-art models on a laptop but at least being able to tweak things/experiment would be nice. Reflecting on the above, I devised three tests: Why not test more extensively? These are enough for me. Iâ€™ve got other sh*t to do. Alright, time for the results. The best results for each experiment have been highlighted in bold. Experiment 1: Final Cut Pro video export For this one, all machines were given ample time to pre-render the raw footage. So when the export button got clicked, they all shouldâ€™ve been relatively on the same page. Experiment details: Video length: 2 hours, 37 minutes+ Export file size: 26.6GB (quoted), ~6.5GB (actual) No surprise here, the 16-inch MacBook Pro exported in the fastest time. Most likely because of the dedicated 8GB GPU or 64GB of RAM. However, it seems using the dedicated GPU came at the cost of battery life drain and fan speed (in the video you can hear the fans going off like a jet). During the video export, the M1-powered MacBook Air and MacBook Pro 13-inch remained completely silent (the MacBook Air had no choice, it doesnâ€™t have a fan but the MacBook Pro 13-inchâ€™s fan never turned on). Experiment 2: CreateML machine learning model training Iâ€™ve never actually used a machine learning model trained by CreateML. However, I decided to see how one of Appleâ€™s custom apps would leverage their new silicon. For this test, each MacBook was setup with the following CreateML settings: Problem: Multi-class image classification. Model: CreateML image classification (Apple donâ€™t tell you what architecture they use, but Iâ€™d take a guess itâ€™s a ResNet). Data size: 7500 training images, 2500 testing images. Maximum iterations: 25. Data augmentation: Flip, rotate. *The MacBook Pro 16-inch died before testing finished. Potentially the most surprising result of all the tests is that the M1 MacBook Air won this one by a clear margin, both in training time and battery life. All the while without a fan and one less GPU than the MacBook Pro 13-inch (the MacBook Air I used has an 8-core CPU, 7-core GPU M1 chip). Itâ€™s also quite clear the CreateML app has potentially been optimised for the M1 chip. As, despite having 8 CPU cores and a dedicated GPU, the 16-inch Intel-powered MacBook Pro ran out of juice before finishing the experiment. Experiment 3: TensorFlow macOS code During the M1 announcement keynote, Apple claimed their new silicon was capable of â€œrunning popular deep learning frameworks such as TensorFlowâ€ at much greater speeds than previous generations. Hearing this forced me to sit up a little straighter. Did they just say TensorFlow? Natively? I read back through the announcement. Yes. They did. They said TensorFlow. Then I found the blog posts by the TensorFlow team and Apple Machine Learning team showcasing the new results on the M1 chips and Intel-based Macs. Turns out, Apple recently released a fork of TensorFlow, tensorflow_macos which allows you to run native TensorFlow code right on your Mac (something which was previously a pain in the ass, actually, not really, I hear PlaidML has made it easier but I haven't tried that). Naturally, after hearing this news, I had to try it. By some miracle, I installed Appleâ€™s fork of TensorFlow into a Python 3.8 environment without 8â€“10 hours of troubleshooting and created the following mini-experiments: 3.1 A basic convolutional neural network (CNN) I copied the CNN architecture on the CNN explainer website (TinyVGG). And used similar data to the CreateML test. Problem: Multi-class image classification. Model: TinyVGG (see Google Colab notebook for exact code). Data: 750 training images, 2500 testing images. Number of classes: 10 (from Food101 dataset). Number of epochs: 5. Batch size: 32. 3.2 Transfer learning with EfficientNetB0 These days I rarely build models from scratch. I use either an existing untrained architecture and train it on my own data or a pretrained architecture like EfficientNet and fine-tune it on my own data. Problem: Multi-class image classification. Model: Headless EfficientNetBO (I only trained the top layer, all others were frozen). Data: 750 training images, 625 testing images (2500 * 0.25 validation steps). Number of classes: 10 (from Food101 dataset). Number of epochs: 5. Batch size: 4 (the lower batch size was required due to the M1 not having the memory capacity to deal with a higher size, I tried 32, 16, 8 & they all failed). 3.3 tensorflow_macos GitHub benchmark Browsing Appleâ€™s tensorflow_macos GitHub, I came across an issue thread containing a benchmark a fair few people had run on their various machines. So I decided to include it in my testing. Problem: Multi-class image classification. Model: LeNet. Data: 60,000 training images, 10,000 testing images (MNIST). Number of classes: 10. Number of epochs: 12. Batch size: 128. Source: https://github.com/apple/tensorflow_macos/issues/25 TensorFlow code results As well as running the above three experiments on all of the MacBookâ€™s I had, I also ran them on a GPU-powered Google Colab instance as a control (my usual workflow is: experiment on Google Colab, scale up on larger cloud servers when needed). ^Google Colab GPU instance used pure TensorFlow rather than tensorflow_macos. The Google Colab GPU-powered instance performed the fastest across all three tests. Notably, the M1 machines significantly outperformed the Intel machine in the Basic CNN and Transfer learning experiments. However, the Intel-powered machine clawed back some ground on the tensorflow_macos benchmark. I believe this was due to explicitly telling TensorFlow to use the GPU, using the lines: I tried running these lines with the previous two experiments and it didnâ€™t make a difference. Perhaps, itâ€™s something to do with the different data loading schemes used between the Basic CNN/Transfer learning setup and the tensorflow_macosbenchmark. See all code experiments in the accompanying Google Colab Notebook. Portability If youâ€™re buying a laptop, you want to be able to move it around. Perhaps write some words while looking at the beach or code up your latest experiment whilst sipping coffee at your local cafe. So what we have here is a compilation of the various amounts of battery used over the three experiments, plus a new portability score Iâ€™ve invented. *16-inch MacBook Pro was charged to full (from 0%) once. **Portability score = battery life/weight ratio = battery life lost x weight (lower is better). The M1 Macs crushed it here. Finishing all tests with over 30% of battery left each. The MacBook Air was again the standout, using the least amount of power and scoring the lowest on the portability score. For sale: 16-inch MacBook Pro Anyone want to buy a second-hand close-to-top-spec MacBook Pro 16-inch? Its been well looked after, I promise. After running the above experiments and using the M1 MacBooks for a couple of weeks, it looks like the graphs Apple showed were pretty close to reality. And all those raving reviews? Well, in my experience, theyâ€™re correct too. I originally bought the 16-inch MacBook Pro to be a powerhouse, a speed machine. And it is that. But so are the M1 machines. Except the M1 machines are lighter, quieter and have better battery life. The only test the MacBook Pro 16-inch won on was the video rendering test (and of course screen size). And even then, the results werenâ€™t dramatically better, certainly not â€œthis machine costs 2.5x more betterâ€. So what next? Iâ€™m keeping the 13-inch MacBook Pro. The little extra boost for video editing won it over the 13-inch Air (plus, I edited the entire video version of this article on the 13-inch Pro without a single hiccup). Iâ€™ll use it as a portable machine and probably set my 16-inch up as a permanent desktop. But the Airâ€¦ Oh the Airâ€¦ I remember when I worked at Apple Retail, Iâ€™d tell customers to get the Air if they were only concerned with word processing and browsing the web. Now you can train machine learning models on it. Which one should you get? â€œI write code, words and browse the web.â€ Or â€œI just want a portable, capable machine.â€ Get the MacBook Air, you wonâ€™t be disappointed. Itâ€™s what Iâ€™d get if I didnâ€™t edit videos daily. â€œI write code, words, browse the web and edit videos.â€ From my tests, the 13-inch M1 MacBook Pro or M1 MacBook Air will perform at 70â€“90% of what a nearly-top-spec Intel 16-inch MacBook Pro can offer, so either of those would be phenomenal. For a slight edge on video editing, you might opt for the 13-inch M1 MacBook Pro. â€œI want a larger screen size and donâ€™t care about cost.â€ As of now, the only valid reason Iâ€™d consider buying an Intel-based 16-inch MacBook Pro would be if screen size was of utmost importance to you and you didnâ€™t have a budget. Screen size doesnâ€™t matter so much to me as most of the time Iâ€™m running a single full-screen app or two apps split down the middle of the screen. Or if I do want to run multiple apps, Iâ€™ll plug my machine into an external monitor (note: for now the M1 Macs only support a single external monitor, so for the 3+ monitor fans out there, youâ€™ll need an Intel-based Mac). This being said, even if you were after a larger screen, Iâ€™d hold off and wait for the Apple silicon 16-inch. Conclusion As I said, I had no problems with 16-inch MacBook Pro. But in comparison to these new M1 MacBookâ€™s, it feels dated. I havenâ€™t been this impressed with a new computer since I first switched from hard drive to solid-state drive. Iâ€™m writing this on the 13-inch M1 MacBook Pro and it feels like butter. The little things, the instance wake, the new keyboard style, the native Apple apps, the battery life, they all add up. If Apple were capable of pulling off these kinds of performance improvements with a 1st-generation chip in a laptop (even one without a fan), I canâ€™t imagine whatâ€™s going to happen on the machines without power constraints (the Mac mini hints at the potential here). How about a 16-inch with an M2? Hopefully they wait at least another year, I mean, my Intel-based 16-inch isnâ€™t even a year old yetâ€¦ PS you can see the video version of this article, including all of the tests above being run on YouTube:
Dimitris Poulopoulos 2 days agoÂ·5 min read Visual Studio Code is arguably the best open-source code editor, and, of course, Python is treated as a first-class citizen inside its ecosystem. The corresponding Microsoft extension comes with â€œrich support for the Python Language, including features like IntelliSense, linting, debugging, code navigation, code formatting, code refactoring, test explorer, snippets, Jupyter Notebook support, and much more.â€ However, the enemy of good is better; thus, Microsoft created Pylance, a new, improved, faster, and feature-packed Python language server. Pylance, a reference to Monty Python's Sir Lancelot the Brave, depends on the core Python extension and builds upon that experience. What is more, with the VS Code December update, Pylance can perform long-awaited actions that take the Python developing experience to a new level. This story presents Pylance, what it is, and how you can leverage its full functionality today. Finally, we examine the new version that was released as part of the latest December update. Learning Rate is a newsletter for those who are curious about the world of AI and MLOps. Youâ€™ll hear from me every Friday with updates and thoughts on the latest AI news and articles. Subscribe here! What is Pylance? Pylance is the new Python language server from Microsoft. It offers fast, feature-rich language support for Python in Visual Studio Code, is dependent on the core Python extensions but brings a hell of a lot more to the table. Letâ€™s see what the Microsoft VS Code Python team has been building for months. Python stub files Pylance leverages type stub files (.pyi files) and lazy type inferencing to provide an efficient development experience. But what are stub files, you may ask? As a quick sidenote, Stub files are used to provide type-hinting information for Python modules. The full official documentation can be found in the section about stub-files in PEP-484. For example, consider the following Python function that resides in my_function.py module: You can create a new stub file, my_function.pyi, to provide type-hinting: NOTE: The ... at the end of the function definition in the stub file is part of the syntax However, we know that we can insert type-hints inside the Python module like that: So, why should we use stub files? There are a couple of reasons. For example, if you want to keep your .py files backward compatible or if you want to provide type-hints into an existing code-base and want to minimize the changes in the source-code itself. Supercharging IntelliSense Coming back to Pylance, the use of stub files can supercharge your Python IntelliSense experience with rich type information, helping you write code faster. What is more, it already comes with a collection of stubs for popular modules out of the box. The built-in stub library provides accurate type checking as well as fast auto-completion. But is that all? Of course not; we are just scratching the surface. Feature highlights Pylance main characteristic is its performance, but many other features make this extension a must-have. Type information: You can now get the type definition of a function when you hover over it. Auto-imports: You are now able to auto-import suggestions for installed and standard library modules. This is probably the most requested feature by Python developers, and itâ€™s finally available. Type-checking: You can now validate that the arguments you pass to a function are of the right type prior to its execution. You have to enable it through settings; set python.analysis.typeCheckingMode to basic or strict. New features The December 2020 Visual Studio Code update introduced several new Pylance features, with code extraction and the Pylance Insiders program being the most important. Code extraction: You are now able to extract methods and variables with one click. Pylance Insiders: Pylance now has its own club of dedicated users, the Pylance Insiders program, which offers early access to new language server features and improvements. To enable insiders, set "pylance.insidersChannel": "daily". Conclusion Pylance seems to be the future of the Python language server for VS Code. It is still in preview, but I would recommend turning it on immediately. The new features and improvements coming almost every day will make you more productive and elevate your overall developing experience. The new VS Code update features many improvements and features outside Pylance, like Ipywidget Support in Native Notebooks. You can read the changelog for more details. About the author My name is Dimitris Poulopoulos, and Iâ€™m a machine learning engineer working for Arrikto. I have designed and implemented AI and software solutions for major clients such as the European Commission, Eurostat, IMF, the European Central Bank, OECD, and IKEA. If you are interested in reading more posts about Machine Learning, Deep Learning, Data Science, and DataOps, follow me on Medium, LinkedIn, or @james2pl on Twitter. Opinions expressed are solely my own and do not express the views or opinions of my employer.
Luay Matalka 1 day agoÂ·13 min read There are three functions in python that provide vast practicality and usefulness when programming. These three functions, which provide a functional programming style within the object-oriented python language, are the map(), filter(), and reduce() functions. Not only can these functions be used individually, but they can also be combined to provide even more utility. In this tutorial, we will cover these three functions and some examples of when they can be useful. But before we start, we need to review what lambda (or anonymous) functions are and how to write them. Lambda Expressions Lambda expressions are used to create anonymous functions, or functions without a name. They are useful when we need to create a function that will only need to be used once (a throw-away function) and can be written in one line. As such, they can be very useful to use in functions that take in another function as an argument. Lambda functions can have any number of parameters but can only have one expression. They generally have this format which yields a function object: lambda parameters: expression Lambda Expression with One ParameterLetâ€™s create a lambda expression that returns the square of a number: And thatâ€™s it! We first start with the lambda keyword, then the parameter num, a colon, and what you want that function to return, which is num**2. Note that this function is anonymous, or does not have a name. So we cannot invoke the function at a later point. In addition, we did not write the return keyword. Everything after the colon is part of the expression that will be returned. If we want to assign a lambda function to a variable so that it can be invoked later, we can do so by using an assignment operator: We can then invoke or call this function the same way we would with a function that was defined with the def keyword. For example: Lambda Expression with Multiple ParametersLetâ€™s write a lambda function that has two parameters instead of just one. For example, we can write a lambda expression that returns the sum of two numbers: As we can see, if we want our function to have multiple parameters in a lambda expression, we would just separate those parameters by a comma. Conditional Statements in Lambda ExpressionsWe can also include if else statements in lambda expressions. We would just need to make sure that it is all on one line. For example, letâ€™s create a function that takes in two numbers and returns the greater of those numbers: Our lambda expression takes in two numbers, num1 and num2, and returns num1 if num1 is greater than num2, else, it returns num2. Obviously this function doesnâ€™t take into account if the numbers are equal, as it will just return num2 in that case, however, we are just illustrating how we can use conditional statements within a lambda expression. For more on lambda expressions: Lambda Expressions in Python How to write anonymous functions in python towardsdatascience.com Now that we understand what lambda functions are, letâ€™s talk about the first function on todayâ€™s agenda: the map function. Map Function Simply put, the map function provides us a way to apply a function to each element in an iterable object, such as lists, strings, tuples, etcâ€¦ Thus, the map function takes in two arguments: the function we want to apply, and the iterable object we want to apply it to. map(function, iterable) For example, we have a list of numbers (the iterable object), and we want to create a new list that contains the squares of those numbers. The map function will return a map object, which is an iterator. If we want to create a list from this map object, we would need to pass in our map object to the built-in list function. What happened in this line of code? The map function took the first element from num_list, which is a 1, and passed it in as an argument to the lambda function (since we passed that function in as the first argument to the map function). The lambda function took that argument, 1, and returned 1**2, or 1 squared, and that was added to our map object. The map function then took the second element from num_list, which is 2, and passed it in as an argument to the lambda function. The lambda function returned the square of 2, which is 4, which was then added to our map object. After it finished going through the num_list elements and the rest of our squared numbers were added to the map object, the list function casts this map object onto a list, and that list was assigned to the variable squared_list. Cryptography: Caesar Cipher Letâ€™s try a slightly more interesting example involving cryptography, specifically the caesar cipher. The caesar cipher encrypts a message by taking each letter in the message, and replacing it with a shifted letter, or a letter that is a specified number of spaces away in the alphabet. So if we choose the number of spaces to be 1, then each letter will be replaced with the letter 1 space away in the alphabet. So the letter a will be replaced with the letter b, the letter b will be replaced with the letter c, and so on. If we choose the number of spaces to be 2, then a will be replaced with c, and b will be replaced with d. If while we are counting spaces we get to the end of the alphabet, then we go back to the beginning of the alphabet. In other words, the letter z will be replaced with the letter a (if we shift 1 space), or with the letter b (if we shift 2 spaces). For example, if the message we want to encrypt is â€˜abcâ€™ and we choose the number of spaces to be 1, then the encrypted message will be â€˜bcdâ€™. If the message is â€˜xyzâ€™, then the encrypted message will be â€˜yzaâ€™. How can we use the map() function to accomplish this? Well, we are applying something to each element of an iterable object. In this case, the iterable object is a string, and we would like to replace each letter/element in our string by a different one. And the map function can do exactly that! Letâ€™s assume that all messages will be lowercase letters only. And the number of spaces will be a number between 0â€“26. Remember, we only want to replace letters with other letters. Thus, any non-letter elements, such as a space or symbol, will be unchanged. We first need access to the lowercase alphabet. We can either write out a string with all the lowercase letters, or we can use the string module as follows: Then, we can write our encrypt function as follows: We create the function encrypt with two parameters: the message we want to encrypt, msg, and the number of spaces n we want to shift the letters by. The iterable we pass in to the map function is the message, msg. The function we pass in to the map function will be a lambda function, which takes each element from the msg string, and if the element is a letter in the alphabet, it replaces it with the shifted letter depending on the n value we pass in. It does so by taking the current index of that letter in the alphabet, or abc.index(x), adds the n value to it, and then takes the modulus of that sum. The modulus operator is used to start back at the beginning of the alphabet if we get to the end (if abc.index(x)+n is a number greater than 25). In other words, if the original letter was z (which will have the index 25 in the abc string we created above), and the n value is 2, then (abc.index(x)+n)%26 will end up being 27%26, and that will yield a remainder of 1. Thus replacing the letter z with the letter of the index 1 in the alphabet, which is b. Remember that the map function will return a map object. Thus, we can use the string method join, which takes in an iterable (which the map object is, since it is an iterator, and all iterators are iterable), and then joins it into a string. The function then returns this string. To decrypt a message, we can use the following decrypt function (notice how we are subtract n from abc.index(x) instead of adding it): Filter Function Simply put, the filter function will â€œfilter outâ€ an iterable object based on a specified condition. This condition will be decided by the function that we pass in. filter(function, iterable) The filter function takes in two arguments: the function that checks for a specific condition and the iterable we want to apply it to (such as a list). The filter function takes each element from our list (or other iterable) and passes it in to the function we give it. If the function with that specific element as an argument returns True, the filter function will add that value to the filter object (that we can then create a list from just like we did with the map object above). If the function returns False, then that element will not be added to our filter object. In other words, we can think of the filter function as filtering our list or sequence based on some condition. For example, we have a list of numbers, and we want to create a new list that contains only the even numbers from our list. This filter function will return a filter object, which is an iterator. If we want to create a list from this filter object, we would need to pass in our filter object to the built-in list function (just like we did with the map object). So letâ€™s break down what happened in this line of code: The filter function took the first element from num_list, which is a 1, and passed it in as an argument to the lambda function (since we passed that function in as the first argument to the filter function). The lambda function then returns False, since 1 is not even, and 1 is not added to our filter object. The filter function then took the second element from num_list, which is 2, and passed it in as an argument to the lambda function. The lambda function returns True, since 2 is even, and thus 2 is added to our filter object. After it goes through the rest of the elements in num_list and the rest of the even numbers are added to our filter object, the list function casts this filter object onto a list, and that list was assigned to the variable list_of_even_nums. List Comprehensions vs. Map and Filter If we recall, list comprehensions are used to create lists out of other sequences, either by applying some operation to the elements, by filtering through the elements, or some combination of both. In other words, list comprehensions can have the same functionality as the built-in map and filter functions. The operation applied to each element is similar to the map function, and if we add a condition to which elements are added to the list in the list comprehension, thatâ€™s similar to the filter function. Also, the expression that is added in the beginning of a list comprehension is similar to the lambda expression that can be used inside the map and filter functions. Example: This is a list comprehension that adds the square of the elements from 0 to 9, only if the element is even: We can use the map and filter functions, along with lambda functions, to accomplish the same thing: The function passed into the map function is a lambda expression that takes input x and returns its square. The list passed into the map function is a filtered list that contains the even elements from 0 to 9. For more on list comprehensions: List Comprehensions in Python A more elegant and concise way to create lists in python towardsdatascience.com Reduce Function Simply put, the reduce function is taking an iterable, such as a list, and reduces it down to a single cumulative value. The reduce function can take in three arguments, two of which are required. The two required arguments are: a function (that itself takes in two arguments), and an iterable (such as a list). The third argument, which is an initializer, is optional. reduce(function, iterable[, initializer]) To use the reduce function, we need to import it as follows: The first argument that reduce takes in, the function, must itself take in two arguments. Reduce will then apply this function cumulatively to the elements of the iterable (from left to right), and reduces it to a single value. Examples: Letâ€™s say the iterable we use is a list of numbers, such as num_list: We would like to take the product of all the numbers in num_list. We can do so as follows: Our iterable object is num_list, which is the list: [1,2,3,4,5]. Our lambda function takes in two arguments, x and y. Reduce will start by taking the first two elements of num_list, 1 and 2, and passes them in to the lambda function as the x and y arguments. The lambda function returns their product, or 1 * 2, which equals 2. Reduce will then use this accumulated value of 2 as the new or updated x value, and uses the next element in num_list, which is 3, as our new or updated y value. It then sends these two values (2 and 3) as x and y to our lambda function, which then returns their product, 2 * 3, or 6. This 6 will then be used as our new or updated x value, and the next element in num_list will be used as our new or updated y value, which is 4. It then sends 6 and 4 as our x and y values to the lambda function, which returns 24. Thus, our new x value is 24, and our new y value is the next element from num_list, or 5. These two values are passed to the lambda function as our x and y values, and it returns their product, 24 * 5, which equals 120. Thus, reduce took an iterable object, in this case num_list, and reduced it down to a single value, 120. This value is then assigned to the variable product. In other words, the x argument gets updated with the accumulated value, and the y argument gets updated from the iterable. Mathematically, you can think of reduce as doing the following: f(f(f(f(f(x,y),y),y),y),y) So the first time the lambda function runs, it takes x and y (the first two elements in the list), and returns an output. That output for f(x,y) will then be used as the new x for the next iteration, and y will be the next element in the list, thus f(f(x,y),y). And so on. Reduce Third Argument: Initializer Remember how we said that reduce can take in an optional third argument, the initializer? The default value for it is None. If we pass in an initializer, it will be used as the first x value by reduce (instead of x being the first element of the iterable). So if we pass in the number 2 in the above example as the initializer: Then the first two values or arguments for x and y will be 2 and 1, respectively. All subsequent steps will be the same. In other words, the initializer is placed before the elements of our iterable in the calculation. Other Examples of Using Reduce There are so many other scenarios of when we can use reduce. For example, we can find the sum of the numbers in a list: Or we can find the maximum number in a list: Or the minimum number in a list: And so many other applications! Note: Python does have built-in functions such as max(), min(), and sum() that would have been easier to use for these three examples. However, the goal was to show how reduce() can be used to accomplish many different tasks. We can also combine the reduce function along with the filter or map function. For example, if we have a list of numbers, but we only want to return the product of the odd numbers only, we can do so using reduce and filter as follows: In other words, the iterable we pass in to the reduce function is a filter object, which only includes the odd numbers. We can think of it as a list of only the odd numbers. Then, the reduce function will take that filtered list and returns the product of those numbers. Conclusion In this tutorial, we first looked at how we can create lambda (or anonymous) functions. We then learned about the map function, which gives a way to apply some function to each element in an iterable object. We then looked at the filter function, and how it can be used to filter an iterable object based on some condition. Furthermore, we compared the map and filter functions to list comprehensions. Next, we looked at how the reduce function can be used to reduce an entire iterable object down into a single value. We also saw examples of all of these functions being used, including an example involving cryptography. Lastly, we learned how these functions can be combined in order to provide even more functionality.
Terence Shin 3 days agoÂ·10 min read â€œEveryone wants to eat, but few are willing to huntâ€ Introduction If you want to be a data scientist but havenâ€™t yet made the commitment, now is the time. Last year, I made a commitment to learn something new about data science every week for 52 weeks, and I think that was one of the best decisions Iâ€™ve ever made. Youâ€™d be surprised at how far you can get in a year. And so, Iâ€™m presenting to you a complete 52 weeks curriculum that you can do in 2021 as a new years resolution! It is crammed and it will be a grind, but it will be worth it. Immediately, youâ€™ll notice that this guide does not start with machine learning, and I have good reasons for that. If you want to learn more about why machine learning comes in the later weeks, check out my article: Want to Be a Data Scientist? Donâ€™t Start With Machine Learning. The biggest misconception aspiring data scientists have towardsdatascience.com A couple of notes before we dive into it: This will not cover everything that you need to know to be a fully equipped data scientist. That being said, this will cover what I believe are the most fundamental skills of a data scientist. This assumes that you already know differential calculus since we all learned it in high school This curriculum will not include anything related to deep learning. Deep learning deserves its own 52 weeks on its own â€” it would be a disservice if I tried to squeeze it in! With that said, letâ€™s dive into it! Course Structure Statistics & Probability Why Statistics and Probability? Data science and machine learning are essentially a modern version of statistics. By learning statistics first, youâ€™ll have a much easier time when it comes to learning machine learning concepts and algorithms! Even though it may seem like youâ€™re not getting anything tangible out of the first few weeks, it will be worth it in the later weeks. Week 1: Descriptive Statistics Intro to Descriptive Statistics Another Intro to Descriptive Statistics Week 2: Probability Theoretical probability Sample spaces Set operations Addition rule Multiplication rule for independent events Multiplication rule for dependent events Conditional probability and independence Week 3: Combinations and Permutations Counting principle and factorial Permutations Combinations Combinatorics Week 4: Normal Distribution and Sampling Distributions Normal distribution and the Empirical rule Introduction to Sampling Distributions Sampling distribution of a sample proportion Sampling distribution of a sample mean Week 5: Confidence Intervals Introduction to Confidence Intervals Estimating Sample Proportions Estimating Sample Means Week 6: Hypothesis Testing Introduction to Hypothesis Testing Error probabilities and power Tests about a population proportion Tests about a population mean More videos Mathematics Why Mathematics? Like statistics, many data science concepts build on fundamental mathematical concepts. In order to understand cost functions, you need to know differential calculus. In order to understand hypothesis testing, you need to understand integration. And to give more one more example, linear algebra is essential to learning deep learning concepts, recommendation systems, and principal component analysis. Week 7: Vectors and Spaces Vectors Linear Combinations and Spans Linear Dependence and Independence Subspaces and the basis for a subspace Week 8: Dot Product and Matrix Transformations pt. 1 Vector dot and cross products Functions and Linear Transformations Transformations and Matrix Multiplications Week 9: Matrix Transformations pt. 2 Inverse Functions and Transformations Inverses and Determinants Transpose of a Matrix Week 10: Eigenvalues and Eigenvectors Eigenvalues and Eigenvectors Anything that you couldnâ€™t finish in the past few weeks! Week 11: Integrals Approximation with Riemann Sums Definite Integrals with Riemann Sums The Fundamental Theorem of Calculus and Accumulation Functions Properties of Definite Integrals Week 12: Integrals Part 2! The Fundamental Theorem of Calculus and Definite Integrals Reverse Power Rule Indefinite Integrals of Common Functions Definite Integrals of Common Functions SQL Why SQL? SQL is arguably the most important skill to learn across any type of data-related profession, whether youâ€™re a data scientist, data engineer, data analyst, business analyst, the list goes on. At its core, SQL is used to extract (or query) specific data from a database, so that you can do things like analyze the data, visualize the data, model the data, etc. Therefore, developing strong SQL skills will allow you to take your analyses, visualizations, and modeling to the next level because you will be able to extract and manipulate the data in advanced ways. I came across Modeâ€™s curriculum a while back and it is fantastic! So I would first get familiar with using SQL in Mode and then youâ€™ll be able to go through the topics below! Week 13: Basic SQL Introduction to SQL SELECT FROM statement WHERE statement ORDER BY statement LIMIT statement Week 14: LOGICAL and COMPARISON Operators Logical Operators Comparison Operators Week 15: AGGREGATES Aggregate Functions (COUNT, SUM, MIN/MAX, AVG) GROUP BY clause HAVING clause Week 16: DISTINCT, CASE WHEN DISTINCT CASE WHEN Week 17: JOINS and UNIONS JOINs UNIONs Week 18: Subqueries and Common Table Expressions Subqueries Common Table Expressions (CTEs) Week 19: String Manipulations String Functions in SQL (LEFT/RIGHT, TRIM, STRPOS, SUBSTR, CONCAT, UPPER/LOWER, etcâ€¦) Week 20: Date-time manipulation EXTRACT DATE_ADD() DATE_SUB() DATE_DIFF() See here for more functions (on the left of the webpage) Week 21: Windows Functions Windows Functions (ROW_NUMBER(), RANK(), DENSE_RANK(), LAG, LEAD, SUM, COUNT, AVG) See here for advanced window functions. Python and Programming Why Python? I started with Python, and Iâ€™ll probably stick with Python for the rest of my life. Itâ€™s so far ahead in terms of open source contributions, and itâ€™s straightforward to learn. Feel free to go with R if you want, but I have no opinions or advice to provide regarding R. Week 22: Introduction to Python Video: (0:00 to 1:03:10) Week 23: List, Tuples, Functions, Conditional Statements, Comparisons Video: (1:03:10 to 2:00:37) Week 24: Dictionaries, Loops, Comments Video: (2:00:37 to 3:04:17) Week 25: Try/Except, Reading & Writing files, Classes and Objects Video: (3:04:17 to 4:20:43) Week 26: Recursion Video explanation of recursion Course on recursion Week 27: Binary Trees Course on Binary Trees Week 28: APIs and Anaconda APIs for beginners Getting Anaconda setup on your computer Pandas Why Pandas? Arguably the most important library to know in Python is Pandas, which is specifically meant for data manipulation and analysis. Week 29: Getting and Knowing your data Follow along and learn â€” YouTube video Practice problem set #1 Practice problem set #2 Week 30: Filtering and Sorting Follow along and learn â€” YouTube video Practice problem set #1 Practice problem set #2 Week 31: Grouping Follow along and learn â€” YouTube video Practice problem set #1 Practice problem set #2 Week 32: Apply Follow along and learn â€” YouTube video Practice problem set #1 Week 33: Merge Follow along and learn â€” YouTube video Practice problem set #1 Practice problem set #2 Visualizing Data Why Data Visualizations? The ability to visualize data and insights is so important because itâ€™s the easiest way to communicate intricate information and a lot of information at once. As a data scientist, youâ€™re always selling yourself and your ideas, whether your pitching a new project or convincing others why your model should be productionalized â€” data visualizations are a great tool to help you with that. There are dozens of data visualization libraries out there, but Iâ€™m going to focus on two: Matplotlib and Plotly. Week 34: Data Visualizations with Matplotlib Introduction to Matplotlib 3-D Visualizations in Matplotlib Types of Data Visualizations in Matplotlib Cheatsheet Week 35: Data Visualizations with Plotly Types of Visualizations in Plotly (beginner) Types of Visualizations in Plotly (beginner and advanced) Data Exploration and Preparation Why Data Exploration and Preparation? â€œGarbage in, garbage outâ€ The models that you create can only be as good as the data that you feed into it. To understand what the state of your data is in, i.e. whether itâ€™s â€œgoodâ€ or not, you have to explore the data and prepare the data. Therefore, for the next four weeks, Iâ€™m going to provide several amazing resources for you to go through and get a better understanding of what data exploration and preparation entails. Week 36: Exploratory Data Analysis (EDA) Exploratory Data Analysis (EDA) can be difficult because thereâ€™s no one set way of doing it â€” but thatâ€™s also what keeps it exciting. Generally, you want toâ€¦ Derive descriptive statistics (eg. central tendency) Perform uni-variable analysis (distributions and spread) Perform multi-variable analysis (scatterplots, correlation matrix, predictive power score, etcâ€¦) Check for missing data and check for outliers Check out a beginnerâ€™s guide to EDA here. Week 37: Data Preparation: Feature Imputation and Normalization What is Feature Imputation? 6 ways to impute missing data Normalization vs Standardization Example of implementing normalization vs standardization Week 38: Feature Engineering and Feature Selection Feature Engineering Mini Course Feature Encoding An Introduction to Feature Selection Week 39: Imbalanced Datasets An Introduction to Imbalanced Classification Problems The Right Way to Oversample in Predictive Modeling Machine Learning Why Machine Learning? Everything that youâ€™ve learned has led up to this point! Not only is machine learning interesting and exciting, but it is also a skill that all data scientists have. Itâ€™s true that modeling makes up a small portion of a data scientistâ€™s time, but it doesnâ€™t take away from its importance. Later in your career, you might notice that I left out some machine learning algorithms, like K Nearest-Neighbors, Gradient Boost, and CatBoost. This is completely intentional â€” if you can understand the following machine learning concepts, youâ€™ll have the skills to learn any other machine learning algorithms in the future. Week 40: Introduction to Machine Learning Supervised vs Unsupervised, Continuous vs Discrete Bias-variance tradeoff Week 41: Linear Regression Linear Models: Linear Regression Linear Models: Multiple Regression Mathematics behind linear regression Week 42: Logistic Regression Introduction to Logistic Regression Part 1: Coefficients Part 2: Maximum likelihood Part 3: R-squared and P-value Week 43: Regularization Ridge Regression (L2) Lasso Regression (L1) Elastic Net Regression Week 44: Decision Trees Decision Trees Introduction Feature Selection and Missing Date Implementing a Decision Tree in Python Week 45: NaÃ¯ve Bayes A Mathematical Explanation of NaÃ¯ve Bayes NaÃ¯ve Bayes (StatQuest) Week 46: Support Vector Machines Intuition of Support Vector Machines Support Vector Machines in Python A mathematical explanation of Support Vector Machines Week 47: Clustering K-means clustering Hierarchical clustering Week 48: Principal Component Analysis Principal Component Analysis (PCA) step-by-step Another detailed explanation by Luis Serrano (I highly suggest you watch both) Mathematical explanation of PCA Week 49: Bootstrap Sampling, Bagging, and Boosting Bootstrap Sampling Ensemble learning, Bagging, Boosting Week 50: Random Forests and Other Boosted Trees Random Forests pt.1 Random Forests pt.2 XGBoost â€” Regression XGBoost â€” Classification XGBoost â€” Mathematical Details XGBoost in Python Week 51: Model Evaluation Metrics Evaluation Metrics with Python Code Understanding the confusion matrix and how to implement it in Python Week 52: Data Science Project If you feel comfortable with the materials above, youâ€™re definitely ready to start your own data science project! Just in case, Iâ€™ve provided three ideas that you can use as inspiration to get started, but feel free to do whatever you like. Idea 1: SQL Case Study Link to the case. The objective of this case is to determine the cause for a drop in user engagement for a social network called Yammer. Before diving into the data, you should read the overview of what Yammer does here. There are 4 tables that you should work with. The link to the case above will provide you with much more detail pertaining to the problem, the data, and the questions that should be answered. Check out how I approached this case study here if youâ€™d like guidance. Skills Youâ€™ll Develop SQL Data Analysis Data Visualization if you choose to visualize your insights. Idea 2: Trustpilot Webscraper Learning how to webscrape data is simple to learn and extremely useful, especially when it comes to collecting data for personal projects. Scraping a customer review website, like Trustpilot, is valuable for a company as it allows them to understand review trends (getting better or worse) and see what customers are saying via NLP. First I would get familiar with how Trustpilot is organized, and decide upon which kinds of businesses to analyze. Then I would take a look at this walkthrough of how to scrape Trustpilot reviews. Skills Youâ€™ll Develop Writing Python Scripts Data Wrangling BeautifulSoup/Selenium (webscraping libraries) Data Analysis Take it further and apply NLP to extract insights from reviews. Idea 3: Titanic Machine Learning Competition In my opinion, thereâ€™s no better way of showing that youâ€™re ready for a data science job than to showcase your code through competitions. Kaggle hosts a variety of competitions that involves building a model to optimize a certain metric, one of them being the Titanic Machine Learning Competition. If you want to get some inspiration and guidance, check out this step-by-step walkthrough of one of the solutions. Skills Youâ€™ll Develop Data Exploration and Cleaning with Pandas Feature Engineering Machine Learning Modelling Thanks for Reading! I hope you found this useful! If you managed to get through this, you should have a strong understanding of the fundamentals in Statistics, Mathematics, SQL, Python/Pandas, and several machine learning algorithms! I hope this inspired you to continue learning too â€” there are so many things that you can continue to explore like more advanced models (eg. CatBoost), deep learning, experimental design, Bayesian modeling, cloud architecture, and the list goes on. If you like this and want to see future content, be sure to give me a follow on Medium. And as always, I wish you the best in your data science endeavors. Not sure what to read next? Iâ€™ve picked another article for you: Want to Be a Data Scientist? Donâ€™t Start With Machine Learning. The biggest misconception aspiring data scientists have towardsdatascience.com and another! 12 Data Science Projects for 12 Days of Christmas Relevant and valuable data science projects that you can do in a day! towardsdatascience.com Terence Shin If you enjoyed this, follow me on Medium for more Sign up for my email list here! Letâ€™s connect on LinkedIn Interested in collaborating? Check out my website
RubÃ©n Romero Dec 18Â·12 min read A couple of weeks ago I was casually chatting with a friend, masks on, social distance, the usual stuff. He was telling me how he was trying to, and I quote, detox from the broker app he was using. I asked him about the meaning of the word detox in this particular context, worrying that he might go broke, but nah: he told me that he was constantly trading. â€œIf a particular stock has been going up for more than one hour or so and Iâ€™m already over the 1% profit threshold then I sellâ€, he said, â€œamong other personal rules Iâ€™ve been followingâ€. Leaving aside the slight pseudoscientific aspect of those rules, I understood what he meant by detox: following them implied checking the phone an astronomically high number of times. So I started wondering: would it be possible to automate the set of rules this guy has in mind? And actually â€” would it be possible to automate a saner set of rules, so I let the system do the trading for me? Since youâ€™re reading this I assume you got caught by the title, so youâ€™ve probably already guessed that the answer is yes. Letâ€™s elaborate on that, but first of all: time is gold and I donâ€™t want to clickbait anyone. This is what weâ€™re going to do: And what are we going to need? Python 3.6 with some libraries. An AWS account with admin rights, for storage and deployment. Node.js, just to set up the serverless framework for deployment. A Telegram account, for monitoring. Everything Iâ€™ve coded is available here. Okay! So, without further ado, letâ€™s go for the first part: getting the data. Getting the data Getting the data is not easy. Some years ago there was an official Yahoo! Finance API, as well as alternatives like Google Finance â€” sadly, both have been discontinued for years now. But donâ€™t worry, thereâ€™s still plenty of alternatives in the market. My personal requirements were: Free of charge: for a production system I would definitely change this bullet point to cheap alternatives, but for a toy system, or proof of concept, whatever you want to call it, I want it free. High limit rate: ideally no limit, but anything above 500-ish hits per minute is more than enough. Real-time data: some APIs provide data with a slight delay, letâ€™s say 15 minutes. I want the real deal â€” the closest I can get to the real-time price of the stock. Ease to use: Again â€” this is just a POC. I want the easiest one. With that list in mind, I went for yfinance â€” the unofficial alternative to the old Yahoo Finance API. Bear in mind that for a real system, and based on the awesome list provided by Patrick Collins, I would definitely choose the Alpha Vantage API â€” but letâ€™s keep it simple for now. The yfinance library was developed by Ran Aroussi to get access to the Yahoo! Finance data when the official API was shut down. Quoting from the GitHub repository, Ever since Yahoo! finance decommissioned their historical data API, many programs that relied on it to stop working. yfinance aimes to solve this problem by offering a reliable, threaded, and Pythonic way to download historical market data from Yahoo! finance. Sweet, good enough for me. How does it work? First we need to install it: And then we can access everything using the Ticker object: That method is quite fast, slightly above 0.005 seconds on average, and returns LOTS of info about the stock; for instance, google.info contains 123 fields, including the following: There is more info available through several methods: dividends, splits, balance_sheet or earnings among others. Most of these methods return the data in a pandas DataFrame object, so weâ€™ll need to play with it a bit to get whatever we want. For now I just need the information of the stock price through the time; the history method is the best one for that purpose. We can select both the period or the interval dates and the frequency of the data down to one minute â€” note that intraday information is only available if the period is minor than 60 days, and that only 7 days worth of 1m granularity data are allowed to be fetched per request. The transposed data of the last entry with a 1m interval is as follows: We can see how itâ€™s indexed by the datetime and every entry has seven features: four fixed points of the stock price during that minute (open, high, low and close) plus the volume, dividends and stock splits. Iâ€™m going to use just the low, so letâ€™s keep that data: Finally, since weâ€™re going to use the data just for the last day, letâ€™s reindex the dataframe to remove the date and timezone components and keep just the time one: Looking good! We already know how to fetch the latest info from yfinance â€” weâ€™ll later feed our algorithm with this. But for that, we need an algorithm to feed: letâ€™s go for the next part. Adding the AI I said it before but Iâ€™ll say this again: donâ€™t try this at home. What Iâ€™m going to do here is fitting a VERY simple ARIMA model to forecast the next value of the stock price; think of it as a dummy model. If you want to use this for real trading, Iâ€™d recommend to look for better and stronger models, but be aware: if it were easy, everyone would do it. First letâ€™s split the dataframe into train and test, so we can use the test set to validate the results of the dummy model â€” Iâ€™m going to keep the last 10% of the data as the test set: If we plot it, we get: Now letâ€™s fit the model with the training data and get the forecast. Note that the hyperparameters of the model are fixed whereas in the real world you should use cross-validation to get the optimal ones â€” check out this awesome tutorial about How To Grid Search ARIMA Hyperparameters With Python. Iâ€™m using a 5, 0, 1 configuration and getting the forecast for the moment immediately after the training data ends: Letâ€™s see how well performed our dummy model: Thatâ€™s not bad â€” we can work with it. With this info we can define a set of rules based on whatever we want to do, like holding if itâ€™s going up or selling if itâ€™s going down. Iâ€™m not going to elaborate on this part because I donâ€™t want yâ€™all to sue me saying you lost all your money, so please go ahead and define your own set of rules :) In the meantime, Iâ€™m going to explain the next part: connecting to the broker. Connecting to the broker As you probably have guessed, this part highly depends on the broker youâ€™re using. Iâ€™m covering here two brokers, RobinHood and Alpaca; the reason is that both of them: Have a public API (official or not) available. Do not charge commissions for trading. Depending on the type of your account you might have some limits: for instance, RobinHood allows just 3 trades over a 5 day period if your account balance is below 25000$; Alpaca allows far more requests but still has a limit of 200 requests per minute per API key. RobinHood There are several libraries that wrap the RobinHood API, but sadly, as far as I know no one of them is official. Sankoâ€™s library was the biggest one, with 1.5k stars in GitHub, but it has been discontinued; LichAmnesiaâ€™s has continued Sankoâ€™s path, but has just 99 stars so far. Iâ€™m going to use robin_stocks library, which has a little over 670 stars at the moment of writing this. Letâ€™s install it: Not all actions require login, but most of them do, so itâ€™s useful to login before doing anything else. RobinHood requires MFA, so itâ€™s necessary to set it up: go to your account, turn on the two factor authentication and select â€œotherâ€ when asked about the app you want to use. You will be presented with an alphanumeric code, which you will use in the code below: To buy or sell is pretty easy: Check the docs for advanced usage and examples. Alpaca For Alpaca we are going to use the alpaca-trade-api library, which has over 700 stars in GitHub. To install: After signing in your account youâ€™ll get an API key ID and a secret key; both are needed for login: Submitting orders is slightly more complex than with RobinHood: Thatâ€™s it! Note that leaving your credentials in plain text is a very, VERY bad thing to do â€” do not worry though, weâ€™ll switch in the next step to environment variables, which is far safer. Now letâ€™s deploy everything to the cloud and monitor it. Deploy and monitoring We are going to deploy everything in AWS Lambda. This wouldnâ€™t be the best option for a production system, obviously, since Lambda does not have storage and we would want to store the trained model somewhere, for instance in S3. However, this will do for now â€” weâ€™ll schedule the Lambda to run daily, training the model every time with the data from the current day. For monitoring purposes weâ€™ll set up a Telegram bot that will send a message with the action to be taken and its outcome. Note that AWS Lambda is free up to a certain limit, but be aware of the quotas in case you want to send lots of messages. The first thing on the to-do list is creating a bot. I followed the official instructions from Telegram: Search for the user @BotFather in Telegram. Use the command \newbot and choose a name and username for your bot. Get the token and store it somewhere safe, youâ€™re going to need it shortly. Next step: deployment. There are several ways of deploying to Lambda. Iâ€™m going to use the serverless framework, so letâ€™s install it and create a template: That will create a scheduled_tg_bot folder with three files: .gitignore, serverless.yml, and handler.py. The serverless file defines the deployment: what, when, and how it is going to be run. The handler file will contain the code to run: You need to change CHAT_ID to the ID of the group, the channel, or the conversation you want the bot to interact with. Here you can find how to get the ID from a channel and here is how to get the ID from a group. Now, weâ€™re going to define how to run the code. Open serverless.yml and write: This code tells AWS the kind of runtime we want and propagates the Telegram token from our own environment so we donâ€™t have to deploy it. Afterwards, weâ€™re defining the cron to run the function daily at 21:00 UTC time. The only thing left is to get the AWS credentials and set them, along with the token and the rest of variables, as environment variables before deploying. Getting the credentials is fairly easy: From your AWS console: Go to My Security Credentials â€” Users â€” Add user. Choose a username and select Programmatic access. Next page: select Attach existing policies directly â€” AdministratorAccess. Copy the Access Key ID and the Secret Access Key and store them. Thatâ€™s it. Now, letâ€™s export the AWS credentials and the Telegram token. Open a terminal and write: Install the necessary packages locally and finally, deploy everything to AWS: Weâ€™re done! The bot will trade for us every day at 21:00 UTC time and will message us with the action performed. Not bad for a proof of concept â€” now I can tell my friend he can stop frantically checking his phone to trade :) Note that all the resources weâ€™ve used through this tutorial have their own documentation: I encourage yâ€™all to go deeper on whatever you think is interesting â€” remember that this is just a toy system! However, as a toy system, I believe it is a good starting point for a richer, more complex product. Happy coding! You can check the code in GitHub. Note from Towards Data Scienceâ€™s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each authorâ€™s contribution. In this case, as the author himself points out: do not attempt to trade without seeking professional advice. See our Reader Terms for details. References: [1] P. Collins, Best Stock APIs and Industry Landscape in 2020 (2020), Medium [2] R. Aroussi, Reliably download historical market data from Yahoo! Finance with Python (2019), Aroussy.com [3] J. Brownlee, How to Grid Search ARIMA Model Hyperparameters with Python (2017), Machine Learning Mastery [4] J. Brownlee, How to Make Out-of-Sample Forecasts with ARIMA in Python (2017), Machine Learning Mastery [5] Serverless team, AWS Python Scheduled Cron Example, GitHub
A.I.-assisted weapons are proliferating quickly Dave Gershgorn Dec 11Â·4 min read OneZeroâ€™s General Intelligence is a roundup of the most important artificial intelligence and facial recognition news of the week. In late November, Iranâ€™s top nuclear scientist, Mohsen Fakhrizadeh, was assassinated on a highway outside of Tehran. Iranian military and state-owned news outlets blame Israel for the attack but also claim that Fakhrizadeh was killed by an A.I.-controlled machine gun mounted to a Nissan truck. A deputy commander of Iranâ€™s Revolutionary Guards described the machine gun as â€œequipped with an intelligent satellite system which zoomed in on martyr Fakhrizadeh.â€ Little other information is known. Eyewitnesses and the scientistâ€™s family contest claims that A.I. technology had anything to with the assassination, according to the New York Times. Instead, they say, the story of an A.I.-powered boogeyman is an attempt to save face after Iranâ€™s failure to protect one of its top scientists. Surprising as it may be, this internet column about A.I. research doesnâ€™t have the inside scoop as to whether international assassins used a robot. But we can shed light on how far-fetched this claim really is based on what we know about military robots. Unlike most of the A.I. research community, eager to post its latest work in conferences and public-facing repositories like arXiv, defense contractors are notoriously secretive about their R&D projects. In the United States, these projects can be branded as national security secrets to shield themselves from public records laws. This handy loophole is used to hide how sophisticated our military systems have become â€” in fact, $76 billion was spent on classified defense projects in 2020 alone. But we do know that hundreds of autonomous military systems already exist. A 2017 report from the Stockholm International Peace Research Institute (SIPRI) surveyed publicly available information to catalog 381 autonomous military systems, 175 of which were armed. â€œAutonomous military systemâ€ is a vague term that encompasses everything from a self-flying drone to record intelligence footage to a robotic gun. Self-guided missiles, autonomous submarines, and automatic missile defense systems all fall into this category. The word â€œautonomousâ€ is also a gray area. For instance, the U.S. military is making an â€œoptionally mannedâ€ turret that can allegedly identify and aim at an enemy while a human pulls the trigger. On the other end of the spectrum, suicide drones armed with explosives can be equipped to find their own target. Some of these weapons are used to guard military bases. The SIPRI report identified three stationary autonomous weapons used to guard tactical positions: a Samsung device called the SGR-A1, another device made by Israeli defense contractor Rafael called the Sentry Tech, and a third made by South Korean company DoDaam called the Super aEgis II. These automated turrets are equipped with cameras and infrared sensors that allow them to see and recognize the heat of human bodies. The Super aEgis II can allegedly detect and track human-sized targets from nearly two miles away, according to the SIPRI report. Other autonomous military systems are being deployed as mobile weapons, as detailed in a 2019 report from Pax, a Dutch humanitarian organization. An Estonian company called Milrem has been building a kind of autonomous mini-tank called THeMIS since 2014. The THeMIS isnâ€™t a weapon itself but a mobile robot like Boston Dynamicsâ€™ robot dog except with tank treads instead of legs. Other companies like Raytheon, Lockheed Martin, and ST Engineering build autonomous and remotely operated weapons made to be carried into battle on top of a THeMIS robot. One of the first automated machine guns for the THeMIS was made by ST Engineering in 2016, formerly known as Singapore Technologies. The company has expanded its â€œremote weapon stationsâ€ to include seven kinds of weapons. All publicly available information says that A.I.-enhanced turrets for both defense and attack can be sold with some semblance of human control, whether that be literally controlling the machine from afar or simply designating what to attack. But much less is known about which weapons have full autonomous capabilities and how those systems function. In July 2020, Israeli company Smart Shooter unveiled a portable and autonomous weapon mounting system called Smash Hopper, which can aim and fire a gun either autonomously or controlled from a distant tablet computer. The whole thing weighs around 50 pounds, and thereâ€™s even a smaller version that folds up like a camera tripod. The existence of these kinds of autonomous weapons doesnâ€™t mean they were involved in the killing of Fakhrizadeh, and thereâ€™s no hard evidence to suggest the Iranian militaryâ€™s claims. The truck that allegedly held the automated weapon exploded at the scene. But the era of autonomous weapons has already begun, and countries like Britain are already sketching what an army of robots might look like in the future.
With the Intel Compute Stick David Moore 10 hours agoÂ·4 min read If you have been reading my column here on Towards Data Science, you will know that I am on a mission. I wanted to count the number of cars passing my house using Computer Vision and Motion Detection. This article provides a small update on Computer Vision using the Intel Neural Compute Stick and the OpenVino library. As a catch-up, I previously built a camera and then explained the need to tune the motion detection device(s). My earlier posts described the camera build and early scripts to review the data and make sense of passing traffic. We had even made a very early chart to plot motion events. Last time I sent 192 images forward through the Yolo model using the Raspberry Pi ARM CPU. There was a lot of heat, and things took a long time. In fact, it took 10 seconds per image. Worse still, I had to slow everything down to only do a shot in 15 seconds. The system over-heated getting to 85% degrees, and I closed, for XMAS, with wanting to run the Neural Compute Stick with that workload to see if things would be any better. My love for Raspberry Pi had started to wear off, and I was feeling a little sad. Letâ€™s get on with updating my code to use the Intel co-processor and just see what happens! Updating my class and re doing the workload I mentioned I had the Neural Compute Stick in my last article, I therefore went ahead and refactored my class to exploit OpenVino. That would allow me to transfer the CPU workload to a specialist Inference Engine. Since I had committed myself to report back on the effort here is my summary of the exercise. There is an excellent tutorial on the whole topic over at pyimagesearch.com and plenty written about OpenCV and OpenVINO already such that I wonâ€™t need to get into the details here. OpenVINO, OpenCV, and Movidius NCS on the Raspberry Pi - PyImageSearch Inside this tutorial, you will learn how to utilize the OpenVINO toolkit with OpenCV for faster deep learning inferenceâ€¦ www.pyimagesearch.com Updating my code The value of using the Object Orientated approach is that the code gets nicely segregated out and is therefore much easier to maintain and update. I only really needed to add a single line of code to my __init__ function. After I load the â€˜netâ€™ from the disk files, I can update the Object to set my preferred device as the Intel stick. How cool is that! Naturally, there are all sorts of problems when we take this simple approach. What if the Neural Compute Stick isnâ€™t plugged in? Rather than default to the Raspberry Pi CPU I prefer the script to abend! Obviously, for production workloads, youâ€™d really want to write more robust code! The last run, 192 photos, took 15 seconds per image, and produced a very hot Raspberry Pi, so how did we do? Using the Intel Neural Compute stick I couldnâ€™t be happier as it turns out. The co-processor cost me about 100.00 dollars, far less than an Nvidia Titan GPU and is much easier to set-up. I couldnâ€™t be happier, really. Intel Neural Compute Stick 2 Buy Intel Neural Compute Stick 2: Desktop Barebones - Amazon.com âœ“ FREE DELIVERY possible on eligible purchases www.amazon.com So I ran my code once again This time I removed a 5-second delay to allow things to cool down. I can tell you that the compute stick is a Turbocharger (Supercharger) add-on to the Raspberry Pi for Deep Neural Network inference workloads. Let us examine the results. Image 0 â€” took 29 seconds, and that might seem like a lot. But remember that we have the overhead of transferring our model to the Neural Compute stick on the first forward pass. Image 1 took a mere .58, so just half a second to perform a forward pass on the neural network. If we exclude image 0, as an outlier, and get the average of the remaining 191 images, the average inference time is .57 or half a second. The Intel device performs object detection in .57 seconds whereas the Arm-based CPU for Raspberry Pi 4b took at least 9.5 seconds. Isnâ€™t that incredible! But wait! there is more. Since we are offloading the compute-intensive workload to the Intel device, there is nothing much for the CPU to do. The CPU must mostly handle a bit of I/O between the SSD storage drive, cache memory, and the Neural stick. Therefore there is absolutely no heat produced. We went from >80degrees to a more or less constant 45degrees. That really is amazing. The Intel stick gets warm to the touch, but there is no hum or any noises at all. My love of the Raspberry Pi is now restored, and I am delighted. Next steps Now it makes no difference, to the accuracy, if you use the Intel Neural Compute stick or the Arm-based CPU. Things just move slower or faster, but the object detection is the system. I reported back on doing Computer Vision operations on the Intel Stick, but I still have my investigation into 100 pictures where Yolo didnâ€™t find an object to complete! Is the data pointing to a need to train Yolo on an Irish data set? Exciting, please do come back! I can now run Yolo in a reasonable time and pass many more photos through the model.
Exploring a method that maps Neural Network predictions to its inputs, with justification. Pramod Pai Dec 20Â·7 min read After having seen how various interpretability techniques are being applied to traditional ML models like XGBoost and Random Forest, we now shift our focus to Neural Networks. Predict vs Interpret Neural networks are great at approximating complex functions that represent real-world events. This has enabled some wonderful feats not just in tech but in areas of Finance, Biology, Physics, etc. The improvement in accuracy and breaking records for state-of-the-art seems to be happening at a breakneck speed. This comes at a cost. As the models get bigger and more complex, they lose the characteristics that make them interpretable. It is very important to learn how and why a machine learning model behaves a certain way while making predictions. We would want to ask questions along the following lines. We look to the paper Axiomatic Attribution for Deep Networks to understand how we can take a step closer to understanding the decisions made by a deep neural network. Axiomatic Attribution for Deep Networks A Neural Network is a mathematical function, just as f(x) = xÂ² is. The function output is heavily dependent on x, or the input. If someone told us that f(x) evaluated to a trillion, we would say that the input was a relatively large number. In other words, input to the mathematical function shown above absolutely decides the output. The large output can be attributed to a relatively large input. This attribution to the input is something that can help us understand a neural networkâ€™s prediction. For example, when a neural network predicts the image that it was shown as a â€˜catâ€™, the pixels in the image belonging to the cat attributed to the prediction. If there was a score for this, then the attribution score for those pixels would be very high given that it was a well-trained model. The research paper Axiomatic Attribution for Deep Networks defines axioms for the correctness of attribution methods â€” methods that generate attribution scores for inputs to deep networks. Axioms are nothing but desirable characteristics that we want these methods to have, so we can trust that they will do a good job of attributing the right scores to the right input features. The paper mentioned above highlights two such axioms: Sensitivity Implementation Invariance We will get back to what these mean later in the post. For now, we will focus on an attribution method that satisfies both these axioms. This method is called Integrated Gradients. Unfortunately, most of the similar methods do not satisfy one of these two axioms, which makes learning Integrated Gradients a fruitful task. Another desirable feature of Integrated Gradients is that it does not need any instrumentation of the network, and can be computed easily using a few calls to the gradient operation, which can be done quite easily in any modern neural network programming framework, allowing even novice practitioners to easily apply the technique. Here is a simple way to understand Integrated Gradients. Integrated Gradients Explained An Attribution method scores the input data based on the predictions the model makes, i.e. it attributes the predictions to its input signals or features, using scores for each feature. For example, In sentiment classification for movie reviews, the input text could be â€˜It was a fantastic performanceâ€™. Using an attribution method, we could generate a score for each word in the input the model predicted for. And these scores associated with each of these words could tell us what part they played in an instance of a prediction. Integrated Gradients is one such method. In rough terms, it is equal to (feature x gradient). The gradient is the signal that tells the neural network how much to increase or decrease a certain weight/coefficient in the network during backpropagation. It relies heavily on the input features to do so. Therefore, the gradient associated with each input feature with respect to the output (partial derivative dout/din) can help us get a clue about how important a feature is. But there is one small problem. It becomes easier to formalize this problem using the example of image classification using deep learning. Consider the above image. We correctly predict the image being a â€˜Fireboatâ€™. We calculate the gradients per feature, in this case, a pixel in the image, to get the dark image on the right. But it looks like noise and is nowhere close to identifying the fireboat. Then how did the model get the prediction right? Did the model randomly guess that the correct prediction was a fireboat? It turns out that the model function is flat in the vicinity of the input of a well-trained model. What we mean by that is, assume that the model had only one input feature and one weight value â€” y = f(x) or y = w * x. If the error in the network is zero, in other words, if the goal weight is reached as shown below, the slope at that point (which is the derivative dy/dx) is 0. Therefore in the (feature x gradient) paradigm, we get (feature x 0). This explains the black pixels and random noise in the image. There is a way to counter this as shown in the paper Axiomatic Attribution for Deep Networks. We arrive at the next and final important concept in understanding Integrated Gradients, The Baseline. Now, the next step is to desaturate the network in order to see the effect of the input features on the output predictions. To perform this, we take the image in question and dial down the brightness/intensity of the image all the way down to black as shown below. Next, we scale the intensity slowly on a range of 0â€“1 to make the black image look more and more like the fireboat image. We see in the graph above that around the 0.3 mark, the score reaches 1.0 and the gradients level off. This is basically the saturated network that we previously discussed. The gradients that we need to compute the attributions lie below this mark. And the black image is called the baseline. In the case of NLP models, the baseline is a zero-embedding word vector. Now that we have looked at Integrated Gradients, Attribution Scores, and the Baseline, It is a good time to visit the two axioms that were discussed earlier in the post. Sensitivity An attribution method satisfies Sensitivity if for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution. Gradients violate sensitivity: consider the following function. It is a one variable, one ReLU network (ReLU is nothing but max(0,x)). f(x) = 1-ReLU(1-x) Suppose the baseline is x=0 and the input is x=5. The range of this function is 0 to 1. For any value greater than or equal to 1, the function returns 1. Early interpretability methods for neural networks assigned feature importance scores using gradients. A reason to choose Integrated Gradients over gradients is that gradients break sensitivity according to the definition above as the differing feature x=5 is being given a zero attribution. Implementation Invariance Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations. For example a CNN and an RNN classifying a piece of text to have a positive sentiment. Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks. To motivate this, notice that attribution can be roughly defined as assigning the blame (or credit) for the output to the input features. Such a definition does not refer to implementation details. Other popular attribution methods such as DeepLift and LRP break Implementation Invariance. The integrated Gradient method does not. These are some great properties that Integrated Gradients have which make them a very good choice to explain any differentiable model. Conclusion Integrated gradients attribute the predictions of a deep network to its inputs. It can be implemented using a few calls to the gradients operator, can be applied to a variety of deep networks, and has a strong theoretical justification. We feel that the Integrated Gradients method is a very good place to start the journey into model interpretability for deep neural networks. References Axiomatic Attribution for Deep Networks We study the problem of attributing the prediction of a deep network to its input features, a problem previouslyâ€¦ arxiv.org Grokking Deep Learning Grokking Deep Learning teaches you to build deep learning neural networks from scratch! In his engaging style, seasonedâ€¦ www.manning.com https://github.com/pair-code/lit blog.fiddler.ai
William Falcon Dec 12Â·5 min read Deep learning models have been shown to improve with more data and more parameters. Even with the latest GPT-3 model from Open AI which uses 175B parameters, we have yet to see models plateau as the number of parameters grow. For some domains like NLP, the workhorse model has been the Transformer which requires massive amounts of GPU memory. For realistic models, they just donâ€™t fit in memory. The latest technique called Sharded was introduced by Microsoftâ€™s Zero paper in which they develop a technique to bring us closer to 1 trillion parameters. In this article, I will give the intuition behind sharded, and show you how to leverage this with PyTorch today to train models with twice the memory in just a few minutes. This capability in PyTorch is now available thanks to a collaboration between Facebook AI Researchâ€™s FairScale team and the PyTorch Lightning team. By the way, I write about the latest in deep learning, explain intuition behind methods and tricks to optimize PyTorch. If you enjoy this type of articles, follow me on twitter for more content like this! Outline Who is this article for? How to use sharded with PyTorch Intuition behind sharded Sharded vs model parallel Who is this article for? This article is for anyone using PyTorch to train models. Sharded works on any model no matter what type of model it is, NLP (transformer), vision (SIMCL, Swav, Resnets, and even Speech. Hereâ€™s a quick snapshot of the performance gains you can see with sharded across these model types. SwAV is the state of the art method for self-supervised learning in computer vision. DeepSpeech2 is a state of the art method for speech. Image GPT is a state of the art method for vision. Transformer is a state of the art method for NLP. How To Use Sharded With PyTorch For those with not much time to read through the intuition of how sharded works, Iâ€™m going to explain up-front how to use sharded with your PyTorch code. But I encourage you to read through the end of the article to understand how sharded works. Sharded is meant to be used with multiple GPUs to gain all the benefits. But, training on multiple GPUs can be intimidating and a huge pain to set up. The easiest way to supercharge your code with sharded is to convert your model to PyTorch Lightning (which is just a simple refactor). Hereâ€™s a quick 4-minute video that shows how to convert your PyTorch code to Lightning. Once youâ€™ve done that, enabling sharding on 8 GPUs is as simple as changing one flag because there is NO change required to your code. If your model comes from another deep learning libary, it will still work with Lightning (NVIDIA Nemo, fast.ai, huggingface transformers). All you need to do is import that model into a LightningModule and hit train. Intuition Behind Sharded Training efficiently across many GPUs has a few approaches. In one approach, (DP), each batch is split across GPUs. Here is an illustation of DP where each part of the batch goes to a different GPU and the model is copied many times to each GPU. However, this approach is bad because model weights are transferred across device. In addition, the first GPU maintains all the optimizer states. For example, Adam keeps a full extra copy of your model weights. In another method (distributed data parallel, DDP), each GPU trains on a subset of data, and gradients are synced across GPUs. This method also works across many machines (nodes). In this illustration, each GPU gets a subset of the data and initializes the model weights exactly the same across every GPU. Then after the backward pass, all gradients are synced and updated. However, this method still has the problem that every GPU must maintain a copy of all the optimizer states (roughly 2â€“3x the number of model parameters) and also all the forward and backward activations. Sharded removes these redundancies. It works the same way as DDP except that all the overhead (gradients, optimizer state, etc) are calculated only for a portion of the full parameters and thus we remove the redundancy of storing the same gradient and optimizer states on all GPUs. So, each GPU stores only a subset of activations, optimizer parameters and gradient computations. Using any distributed mode As you see, there are many ways of squeezing maximum efficiency in distributed training by using any of these optimization approaches. The good news is that all of these modes are available in PyTorch Lightning with zero code changes required. You can try any of them and adjust based on your particular model as needed. One method that is missing is model parallel. However, a word of caution on model parallel is that it was found to be much more inefficient than sharded training and should be used with caution. For some cases it might work well, but youâ€™re overall best served just using sharded instead. An advantage of using Lightning is that youâ€™ll never fall behind the latest advancements in AI research! The team and opensource community are dedicated to bringing you the latest advances via Lightning. Acknowledgments: Sharded is now available in PyTorch Lightning thanks to the efforts of the Facebook AI FairScale team, special thanks to Benjamin Lefaudeux from Facebook AI FairScale and Sean Narenthiran from PyTorch Lightning / Grid AI. Sharded was inspired from Microsoftâ€™s Zero paper.
This tutorial implements a variational autoencoder for non-black and white images using PyTorch. William Falcon Dec 5Â·9 min read Itâ€™s likely that youâ€™ve searched for VAE tutorials but have come away empty-handed. Either the tutorial uses MNIST instead of color images or the concepts are conflated and not explained clearly. Youâ€™re in luck! This tutorial covers all aspects of VAEs including the matching math and implementation on a realistic dataset of color images. The outline is as follows: Resources Follow along with this colab. Code is also available on Github here (donâ€™t forget to star!). For a production/research-ready implementation simply install pytorch-lightning-bolts and import and use/subclass ELBO loss In this section, weâ€™ll discuss the VAE loss. If you donâ€™t care for the math, feel free to skip this section! Distributions: First, letâ€™s define a few things. Let p define a probability distribution. Let q define a probability distribution as well. These distributions could be any distribution you want like Normal, etcâ€¦ In this tutorial, we donâ€™t specify what these are to keep things easier to understand. So, when you see p, or q, just think of a blackbox that is a distribution. Donâ€™t worry about what is in there. VAE loss: The loss function for the VAE is called the ELBO. The ELBO looks like this: The first term is the KL divergence. The second term is the reconstruction term. Confusion point 1 MSE: Most tutorials equate reconstruction with MSE. But this is misleading because MSE only works when you use certain distributions for p, q. Confusion point 2 KL divergence: Most other tutorials use p, q that are normal. If you assume p, q are Normal distributions, the KL term looks like this (in code): But in our equation, we DO NOT assume these are normal. We do this because it makes things much easier to understand and keeps the implementation general so you can use any distribution you want. Letâ€™s break down each component of the loss to understand what each is doing. ELBO Loss â€” KL Divergence term Letâ€™s first look at the KL divergence term. The first part (min) says that we want to minimize this. Next to that, the E term stands for expectation under q. This means we draw a sample (z) from the q distribution. Notice that in this case, I used a Normal(0, 1) distribution for q. When we code the loss, we have to specify the distributions we want to use. Now that we have a sample, the next parts of the formula ask for two things: 1) the log probability of z under the q distribution, 2) the log probability of z under the p distribution. Notice that z has almost zero probability of having come from p. But has 6% probability of having come from q. If we visualize this itâ€™s clear why: z has a value of 6.0110. If you look at the area of q where z is (ie: the probability), itâ€™s clear that there is a non-zero chance it came from q. But, if you look at p, thereâ€™s basically a zero chance that it came from p. If we look back at this part of the loss You can see that we are minimizing the difference between these probabilities. So, to maximize the probability of z under p, we have to shift q closer to p, so that when we sample a new z from q, that value will have a much higher probability. Letâ€™s verify this via code and now our new kl divergence is: Now, this z has a single dimension. But in the real world, we care about n-dimensional zs. To handle this in the implementation, we simply sum over the last dimension. The trick here is that when sampling from a univariate distribution (in this case Normal), if you sum across many of these distributions, itâ€™s equivalent to using an n-dimensional distribution (n-dimensional Normal in this case). Hereâ€™s the kl divergence that is distribution agnostic in PyTorch. This generic form of the KL is called the monte-carlo approximation. This means we sample z many times and estimate the KL divergence. (in practice, these estimates are really good and with a batch size of 128 or more, the estimate is very accurate). ELBO loss â€” Reconstruction term The second term weâ€™ll look at is the reconstruction term. In the KL explanation we used p(z), q(z|x). For this equation, we need to define a third distribution, P_rec(x|z). To avoid confusion weâ€™ll use P_rec to differentiate. Tip: DO NOT confuse P_rec(x|z) and p(z). So, in this equation we again sample z from q. But now we use that z to calculate the probability of seeing the input x (ie: a color image in this case) given the z that we sampled. First we need to think of our images as having a distribution in image space. Imagine a very high dimensional distribution. For a color image that is 32x32 pixels, that means this distribution has (3x32x32 = 3072) dimensions. So, now we need a way to map the z vector (which is low dimensional) back into a super high dimensional distribution from which we can measure the probability of seeing this particular image. In VAEs, we use a decoder for that. Confusion point 3: Most tutorials show x_hat as an image. However, this is wrong. x_hat IS NOT an image. These are PARAMETERS for a distribution. But because these tutorials use MNIST, the output is already in the zero-one range and can be interpreted as an image. But with color images, this is not true. To finalize the calculation of this formula, we use x_hat to parametrize a likelihood distribution (in this case a normal again) so that we can measure the probability of the input (image) under this high dimensional distribution. ie: we are asking the same question: Given P_rec(x|z) and this image, what is the probability? Since the reconstruction term has a negative sign in front of it, we minimize it by maximizing the probability of this image under P_rec(x|z). ELBO summary Some things may not be obvious still from this explanation. First, each image will end up with its own q. The KL term will push all the qs towards the same p (called the prior). But if all the qs, collapse to p, then the network can cheat by just mapping everything to zero and thus the VAE will collapse. The reconstruction term, forces each q to be unique and spread out so that the image can be reconstructed correctly. This keeps all the qs from collapsing onto each other. As you can see, both terms provide a nice balance to each other. This is also why you may experience instability in training VAEs! PyTorch Implementation Now that you understand the intuition behind the approach and math, letâ€™s code up the VAE in PyTorch. For this implementation, Iâ€™ll use PyTorch Lightning which will keep the code short but still scalable. If you skipped the earlier sections, recall that we are now going to implement the following VAE loss: This equation has 3 distributions. Our code will be agnostic to the distributions, but weâ€™ll use Normal for all of them. The first distribution: q(z|x) needs parameters which we generate via an encoder. The second distribution: p(z) is the prior which we will fix to a specific location (0,1). By fixing this distribution, the KL divergence term will force q(z|x) to move closer to p by updating the parameters. The optimization start out with two distributions like this (q, p). and over time, moves q closer to p (p is fixed as you saw, and q has learnable parameters). The third distribution: p(x|z) (usually called the reconstruction), will be used to measure the probability of seeing the image (input) given the z that was sampled. Think about this image as having 3072 dimensions (3 channels x 32 pixels x 32 pixels). So, we can now write a full class that implements this algorithm. Whatâ€™s nice about Lightning is that all the hard logic is encapsulated in the training_step. This means everyone can know exactly what something is doing when it is written in Lightning by looking at the training_step. Data: The Lightning VAE is fully decoupled from the data! This means we can train on imagenet, or whatever you want. For speed and cost purposes, Iâ€™ll use cifar-10 (a much smaller image dataset). Lightning uses regular pytorch dataloaders. But itâ€™s annoying to have to figure out transforms, and other settings to get the data in usable shape. For this, weâ€™ll use the optional abstraction (Datamodule) which abstracts all this complexity from me. Now that we have the VAE and the data, we can train it on as many GPUs as I want. In this case, colab gives us just 1, so weâ€™ll use that. And weâ€™ll see training startâ€¦ Even just after 18 epochs, I can look at the reconstruction. Even though we didnâ€™t train for long, and used no fancy tricks like perceptual losses, we get something that kind of looks like samples from CIFAR-10. Next post In the next post, Iâ€™ll cover the derivation of the ELBO! Remember to star the repo and share if this was useful williamFalcon/pytorch-lightning-vae Dismiss GitHub is home to over 50 million developers working together to host and review code, manage projects, andâ€¦ github.com
William Falcon Nov 21Â·4 min read As an open-source developer, the question I hear the most is â€œwhy would you want to give that away for free.?â€ In the field of AI, there are many reasons why opensource is key. First, the code for building models does not give away any competitive advantage because the value comes from models+your own data. Second, it lets the whole world help you find and correct mistakes. Imagine building a house where every architect in the world can contribute one tiny idea. But more importantly, AI is a really hard problem to solve. The problems in the field cannot be solved by any one individual or group. In AI we opensource everything from datasets to models to frameworks that give users incredible sophistication to build machine learning models. As the original creator of one of these frameworks (PyTorch Lightning), Iâ€™ve had the fortune to see the beauty of opensource from the inside. Itâ€™s all about the community. PyTorch Lightning has its humble beginnings as a project that I developed during the first few years of my Ph.D. at NYU CILVR and later at Facebook AI Research. At NYU it gained the powers of rapid iteration and standardization that makes Lightning a pleasure to work with today â€” it standardizes AI research code so everyoneâ€™s code can be formatted the same way, and thus it becomes more readable and reproducible. At FAIR it learned how to train massive neural networks across hundreds of GPUs. But had I remained the only developer of the project it would be nowhere near where it is today as a quickly rising favorite for deep learning research. Our first non-facebook contributor Jirka, forced much-needed formatting and structuring to the internals. But then something magical happened. Tens of thousands of people adopted Lightning to build super-advanced AI models. But the magic is not the adoption but how the community worked with Lightning. Lightning became a living, breathing organism where many of the worldâ€™s top AI researchers and PhD students started contributing their own features back. Lightning became the AI research communityâ€™s framework. What exists today is a highly-tuned, rigorously tested a world-class framework for building AI models. It helps Ph.D. students create new papers, research scientists try insanely creative ideas and data scientists build complex scalable AI production systems. Lightning is more than a framework now, itâ€™s a community. The opensource core values. Opensource is centered around honesty, transparency, and building on top of each otherâ€™s work. Iâ€™ve always considered myself to be an honest person with a strong internal moral compass. But as I learned as a young trainee undergoing US Navy SEAL training in 2007, what you believe your internal compass differs from what you actually do in situations with real consequences. SEAL training taught me a few hard lessons along the way. Integrity is what you do when no one is looking. Courage is how you react when every muscle and mental fiber of your body is aching for you to quit. These principles are behind a lot of how I approach building Lightning and interact with the AI community. There have been many times during the development of the project where Iâ€™ve had to enforce the integrity of our project by having a zero-tolerance policy for copying code from other projects. For example here, one of our contributors took code from another project and copy-pasted it into Lightning. Now, itâ€™s clear that the contributor did not have bad intentions, but nevertheless, this is not the Lightning way. We donâ€™t copy, we create. This particular event was resolved positively. Now, you might wonder why I care so much about this given that â€œopensource copying is fair game.â€ But opensource was designed so that we can build on top of each otherâ€™s works. The objective is not to pull pieces of projects to the point where you just end up duplicating functionality. Building on top of each other is why the field of AI is the fastest-growing field today. This principle is also at the core of the scientific community. Peer reviews, attribution, citations. It is in the DNA of science. In my opinion, slowly copying chunks of functionality instead of building on existing work, is counter to these principles and has no room in the AI research community. Lightning is built by creators, researchers and innovators for those who want to build the next big thing in AI. I hope future projects continue to build on our work to advance the field. Iâ€™ve learned a lot from friends and contributors of Kornia, NVIDIA NeMo, and other amazing projects. Integrating as partners has helped us all deliver exponentially better experiences for users. Letâ€™s build the future of open source AI together.
Baris Yazici 6 days agoÂ·12 min read In this post, I will explain my 1-year experience of working with RL on autonomous robotics manipulation. It is always hard to start a big project which requires many moving parts. It was undoubtedly the same in this project. I want to pass the knowledge I gathered through this process to help others overcome the initial inertia. In the beginning, it was tough for me to judge the difficulty of different components of the problem. I underestimated the effort required for some parts and overestimated others. I will explain how RL can be cumbersome and straightforward to work with at the same time. The below video shows how our agent can transform the learned policy to different domains and different environments. Masterâ€™s thesis is accessible: https://github.com/BarisYazici/tum_masters_thesis/blob/master/final_report.pdf And the link to the GitHub repo: BarisYazici/deep-rl-grasping Train robotics model with integrated curriculum learning-based gripper environment. Choose from different perceptionâ€¦ github.com MANIPULATION Humanlike manipulation is a challenging task for todayâ€™s robots. Robots are still performing based on a manually designed controller specifically designed for only one problem at hand. They lack the following skills: Performing in unseen environments No adaptation to new environments or objects No generalized representation of object manipulation Robots in the future should know how to manipulate a Rubik cube and kitchen appliances at the same time. They should not need supervision to learn new manipulation skills. The Reinforcement Learning (RL) framework promises end-to-end learning of these skills with no hand-coded controller design. REINFORCEMENT LEARNING Reinforcement Learning is a robust framework to learn complex behaviors. It has already shown great success on Atari games and locomotion problems. Significantly, the underactuated motions like tying shoelaces or wearing a shirt are hard to model and control with traditional methods [1]. RL can tackle these problems by sampling in a simulation and optimizing for the maximum reward. The biggest challenges for RL are the [2]: sample efficiency hyperparameter sensitivity The following sections will address these problems to achieve an autonomous robotics manipulator in a simulation. The below components helped to tackle the challenges mentioned above. KEY TO SUCCESS: 1. Curriculum learning 2. Shaped reward 3. Input and reward normalization 4. Raw depth sensor data as input 5. Off-policy maximum entropy RL framework â€” e.g. SAC Experimental Setup First of all, letâ€™s go through our problem definition. Our training environment features a gripper attempting to grasp randomly drawn objects from the floor. The gripper is deprived of an arm and a base. Accordingly, the computation of inverse kinematics is ignored. All training runs and experiments were done in PyBullet simulation [3]. The observed state originates from the camera mounted on the gripper. The gripper is position controlled with continuous input between -1 to 1. An action is represented by [x, y, z, yaw angle, gripper open/close]. The inputted action represents the relative movement based on the position of the gripper. Episode terminates based on the termination goal of the training task; either when an agent lifts an object from clutter or when it successfully picks all objects from the ground. Observation Observation is the eyes of the RL agent. Humans see and touch objects to map the environment in their brains. Like us, RL agent also needs some input to store the environmentâ€™s state. There is a couple of go-to perception types. Some examples are: RGB images and autoencoder from [4][5]. Besides, we implemented depth image observation, which performed better than both of the reference works. We compared the performance of these observation types: 1. RGB-D, 2. Depth image and 3. Autoencoder The below diagram shows how we processed the depth observation from the environment to the learning algorithm. Our simulation environment returns observation with two components: We separated the depth image observation from the gripper width. We then fed the depth image into the convolutional network with a fully connected layer at the end. Finally, we concatenated the processed depth image with gripper width information, which returned the shape of 513. SAMPLE EFFICIENCY In contrast to supervised learning, RL creates its data to optimize. When the data was created, it may not point to the high reward region; optimizing it will not lead to good grasping behavior. Imagine optimizing the image-net with falsely labeled images; naturally, it wonâ€™t perform right [6]. RL has both advantages and disadvantages when it comes to data creation. In the RL setting, the data is expensive. It takes many simulator or physical robot iterations to create the data. But we do not need to label the data. Therefore, a well-defined agent can explore the environment on its own. In supervised learning, grasping scenarios need to be modeled tediously and labeled, which is quite challenging when the optimal policy is stochastic. Policy will render good data and optimizing this data will lead to a better policy [4]. While we strongly rely on the agentâ€™s random actions for good data, it might never explore the environment in a comprehensive way, leading to an incompetent policy due to the bad data. So, we aim to incentivize the agent to the good data region as fast as possible. For this purpose, we used the following techniques: Curriculum learning Shaped reward function Off-policy RL algorithm (SAC) They both contributed to the sample efficiency by creating more meaningful data early on in training. Curriculum learning Curriculum learning governs the difficulty of the environment to facilitate learning. Like our school curriculum, first teaching arithmetic and later introducing differential math. Our curriculum strategy gets more challenging with the success rate of the RL agent. Curriculum strategy modifies the following environment features: lifting height, object count and the limits of the workspace area. In the beginning, it is simpler for the agent to explore the terminal state and the intermediate goals in a comfortable setting. When we slightly change the terminal state, it can still extrapolate from what it already knew to a harder environment. Shaped Reward The shape reward function has the same purpose as curriculum learning. It motivates the agent to explore the high reward region. Through intermediate rewards, it steers the agent to the terminal state. Agent receives an intermediate reward when it grasps an object. As soon as the agent lifts the object to a terminal state, it gets the terminal reward. We apply a time penalty until it reaches the terminal state. The sum of the intermediate reward and the time penalty must stay smaller than zero until the terminal state. Otherwise, the agent would exploit the intermediate reward and wait until the episodeâ€™s end to get to the terminal state. As mentioned before, a shaped reward serves to lead the agent to the good data region. Good data provides better policy, and they reinforce each other during the learning to deliver the optimal policy. Input and Reward Normalization I think of the normalization as the activator of the observation and shaped reward functions. Without the normalization, agent is unlikely to make sense of the input and rewards that are fed to the neural nets. Especially when the input has different components, and the reward isnâ€™t sparse. Our environmentâ€™s state is composed of depth sensor input and the gripper width information. Unnormalized state representation can lead to a false emphasis on the state components, giving more weight to the gripper width information than the depth-sensor data or vice-versa. Normalization helps to scale the observation components to the same level. Learning Algorithm â€œRL uses training information that evaluates the actions taken rather than instructs by giving correct actions â€” This is what creates the need for active exploration, for an explicit search for good behavior.â€ â€“ Introduction to Reinforcement Learning â€” R. Sutton Exploration is innate in RL. The uncertainty on the estimation of the action values is unavoidable. Especially in our environment, where reward distribution over actions has a huge variance, we need to apply a sophisticated exploration strategy [7]. SAC masters the exploration-exploitation trade-off in RL. We expect an RL algorithm to find a balance between exploring and exploiting. This optimal balance could mean finding the optimal policy or stuck at sub-optimal policies. Exploration states how flexible it is to try new actions, while exploitation is how confident it is to take a specific action. In most cases, those two concepts are firmly connected. If we explore enough, we could find newer, better actions that return more rewards. Still, if we are confident enough about the action-value estimation, we should stop exploring and start exploiting the greedy actions. SAC models the RL problem not just for the expected reward maximization but also the expected entropy at the same time. This nature provides the following advantages: 1) Optimum entropy provides enhanced exploration behavior. 2) Reduced hyperparameter sensitivity. SAC is the most robust algorithm we used. It required minimal hyperparameter tuning and sampling. The off-policy nature of the SAC algorithm enables us to use the samples from different policies. Therefore, we can store the samples in a replay buffer and use it as many times as possible. Similar to supervised learning, we draw batches of samples to find optimal actions. We stored the size of 1 million samples in the buffer, which allocated around 50GB of RAM. Be careful if you want to replicate our results; check if you have enough ram on your machine. Off-policy algorithms proved to be more sample efficient than on-policy RL counterparts, where we throw away the data, we use each episode and create new experiences for new episodes. Training Setup We have two different training scenarios: In single object picking from clutter setup, the gripper needs to pick one random object to a predefined threshold to end the episode successfully. And for the table cleaning setup, it needs to pick each object in the environment to the same height threshold. Both scenes required different hyperparameters. For example, we needed to decrease the start object count from three to one for clearing the table scene. Also, neural network layers and buffer size are increased to match the increased complexity of the behavior. We aim to get the most generalized grasping model. This model should perform well with unseen objects and adapt to new domains. Thatâ€™s why we designed two test environments. With these different test setups, we can assess if the model generalizes and adapts new scenes and domains. RESULTS Depth sensor input performs the best We tested with both autoencoder, depth, and RGB-D input. Based on our tests, depth input performed the best. We believe the difference between autoencoder and depth perception lies in the interpretation loss of the depth image. Autoencoders compress the observation onto a latent-space. This compression causes the agent to misinterpret the depth of the objects. On the other hand, the depth perception layer is an online method; therefore, it corrects its network weights when a wrong interpretation occurs. The online perception layer also complies the end-to-end nature of the RL framework. Our depth perception layerâ€™s weights are updated to deliver a better-grasping policy; autoencoderâ€™s weights are immutable throughout learning. Buffer Size Matters: Although SAC is robust to different hyperparameter selections, we still updated some of the learning parameters to achieve a more significant result. Such as the buffer size. Buffer size is a critical hyperparameter, which directly affects the performance of the agent. The agent needs large enough samples/experiences in the buffer to learn, similar to supervised learning datasets. Usually, with complex behaviors, where exploration is a big part of the learning, a larger buffer size is meaningful. Table Clearing Task vs. Single Object Picking from Clutter: Different manipulation skills demand different hyperparameter tuning. More complex behaviors require a large buffer size and neural network layers. For example, the hyperparameters we used for single object picking from clutter did not work correctly for the table clearing task. We needed to increase the buffer size from 1m to 2m, and the neural network layer size from 64 to 128. Aside from the neural networkâ€™s hyperparameters, we also changed the curriculum strategyâ€™s object count parameter from three to one. Agent in table clearing task couldnâ€™t explore the terminal state with three objects at the beginning of the training. Therefore, we had to decrease the object count to smoothen the transition from an easy setting to a more challenging environment. SUMMARY To sum up, in this article we covered how to approach the robotic bin picking problem with the help of RL. We mentioned the importance of: Â· Leading the agent to the good data region as fast as possible Â· Leveraging old experiences with off-policy updates Â· Normalization of the observation and reward Â· Raw depth pixel as observation to ensure end-to-end learning In general, RL can be cumbersome to work with because itâ€™s hard to debug. Itâ€™s always good to start out simple. Try implementing a simplified version of your custom environment. First check that baseline RL algorithms can learn the simplified version. And then, gradually make the environment harder to see which parameters make the RL agent struggle to learn. This way you can guarantee that the agentâ€™s learning will not be bottlenecked, and you will not be stressed out to see your agent suffering :) For readers interested to learn more about RL can check out the below resources: Â· John Schulman gives practical advice for RL training â€” https://www.youtube.com/watch?v=8EcdaCk9KaQ&t=2409s Â· Deep RL Bootcamp: https://sites.google.com/view/deep-rl-bootcamp/lectures Â· Berkeley RL course from Sergey Levine: http://rail.eecs.berkeley.edu/deeprlcourse/ Â· From Prof. Pieter Abbeel: https://youtu.be/OMraS0GRWK0?t=1258 Â· The legendary course from David Silver course: https://youtu.be/2pWv7GOvuf0 Â· Blog post from Andrej Karpathy: http://karpathy.github.io/2016/05/31/rl/ Â· Russ Tedrake underactuated robotics course: http://underactuated.csail.mit.edu/rl_policy_search.html References [1]Russ Tedrake. Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation (Course Notes for MIT 6.832). Downloaded on 19/12/2020 from http://underactuated.mit.edu/ [2]Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., & Levine, S. (2018). Soft Actor-Critic Algorithms and Applications. http://arxiv.org/abs/1812.05905 [3]E. Coumans and Y. Bai. PyBullet, a Python module for physics simulation for games, robotics and machine learning. http://pybullet.org. 2016â€“2020. [4]Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., & Levine, S. (2018). QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. CoRL, 1â€“23. http://arxiv.org/abs/1806.10293 [5]Breyer, M., Furrer, F., Novkovic, T., Siegwart, R., & Nieto, J. (2018). Comparing Task Simplifications to Learn Closed-Loop Object Picking Using Deep Reinforcement Learning. IEEE Robotics and Automation Letters, 4(2), 1549â€“1556. https://doi.org/10.1109/LRA.2019.2896467 [6]Eysenbach B., Kumar A., Gupta A., (2020, 10, 13), Reinforcement learning is supervised learning on optimized data, bair.berkeley.edu, https://bair.berkeley.edu/blog/2020/10/13/supervised-rl/ [7]Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning, Second Edition: An Introduction â€” Complete Draft. In The MIT Press.
18 hours agoÂ·2 min read It is not new that science borrows techniques from several fields to make new techniques. For example, genetic algorithms borrows knowledge from Darwinâ€™s evolution theory and biology to make an algorithm that tackles hard problems. However, I will be discussing Genetic Algorithms in another post, since they are a fascinating topic themselves. Today I will discuss a technique from social sciences that involves voting. Democracy and Machine Learning Voting is the process of casting an opinion towards two or more options. That is, making a choice. When a group of voters cast their choices and the majority choice is selected as the group choice, we say that it was a democratic process (as long as voters were not influenced). Hereâ€™s where things get interesting. Suppose a scenario where there's only two choices: A and B. Moreover, suppose that there is one choice that is the correct. Finally, suppose that there's a probability that each voter will pick the right choice. What I have just described can be found in Machine Learning as an ensemble method. An ensemble is a collection of Machine Learning models that take decisions as a group. It is the basis of bootstrap aggregation, also known as bagging [1]. It is demonstrated that bagging has several benefits over having a simple model. For example, having several weak models can reduce variance, decrease overfitting, increase stability and accuracy [1]. What is impressive is that this technique was actually proposed in 1785 by the Marquis de Condorcet, and is known as the Condorcet TheoremÂ [2]. Thereâ€™s one caveat: to obtain the benefits of bagging the probability of the weak models to make the right choice has to be larger than 50%. If the conditions of higher probability of making the right choice is not met, then the group decision is the same having only one individual making the choicesÂ [3]. I am writing daily knowledge pills so you can keep your knowledge sharp! I write about Data Science, Machine Learning in intersection with Finance. Feel free to connect and check out my other articles! Reference [1] Breiman L. Bagging predictors. Machine learning. 1996 Aug 1;24(2):123-40. [2] Condorcet MD. Essay on the Application of Analysis to the Probability of Majority Decisions. Paris: Imprimerie Royale. 1785. [3] De Prado ML. Advances in financial machine learning. John Wiley & Sons; 2018 Feb 21.
Soner YÄ±ldÄ±rÄ±m 20 hours agoÂ·8 min read Data science is an interdisciplinary field. One of the building blocks of data science is statistics. Without a decent level of statistics knowledge, it would be highly difficult to understand or interpret the data. Statistics helps us explain the data. We use statistics to infer results about a population based on a sample drawn from that population. Furthermore, machine learning and statistics have plenty of overlaps. Long story short, one needs to study and learn statistics and its concepts to become a data scientist. In this article, I will try to explain 10 fundamental statistical concepts. 1. Population and sample Population is all elements in a group. For example, college students in US is a population that includes all of the college students in US. 25-year-old people in Europe is a population that includes all of the people that fits the description. It is not always feasible or possible to do analysis on population because we cannot collect all the data of a population. Therefore, we use samples. Sample is a subset of a population. For example, 1000 college students in US is a subset of â€œcollege students in USâ€ population. 2. Normal distribution Probability distribution is a function that shows the probabilities of the outcomes of an event or experiment. Consider a feature (i.e. column) in a dataframe. This feature is a variable and its probability distribution function shows the likelihood of the values it can take. Probability distribution functions are quite useful in predictive analytics or machine learning. We can make predictions about a population based on the probability distribution function of a sample from that population. Normal (Gaussian) distribution is a probability distribution function that looks like a bell. The peak of the curve indicates the most likely value the variable can take. As we move away from the peak the probability of the values decrease. 3. Measures of central tendency Central tendency is the central (or typical) value of a probability distribution. The most common measures of central tendency are mean, median, and mode. Mean is the average of the values in series. Median is the value in the middle when values are sorted in ascending or descending order. Mode is the value that appears most often. 4. Variance and standard deviation Variance is a measure of the variation among values. It is calculated by adding up squared differences of each value and the mean and then dividing the sum by the number of samples. Standard deviation is a measure of how spread out the values are. To be more specific, it is the square root of variance. Note: Mean, median, mode, variance, and standard deviation are basic descriptive statistics that help to explain a variable. 5. Covariance and correlation Covariance is a quantitative measure that represents how much the variations of two variables match each other. To be more specific, covariance compares two variables in terms of the deviations from their mean (or expected) value. The figure below shows some values of the random variables X and Y. The orange dot represents the mean of these variables. The values change similarly with respect to the mean value of the variables. Thus, there is positive covariance between X and Y. The formula for covariance of two random variables: where E is the expected value and Âµ is the mean. Note: The covariance of a variable with itself is the variance of that variable. Correlation is a normalization of covariance by the standard deviation of each variable. where Ïƒ is the standard deviation. This normalization cancels out the units and the correlation value is always between 0 and 1. Please note that this is the absolute value. In case of a negative correlation between two variables, the correlation is between 0 and -1. If we are comparing the relationship among three or more variables, it is better to use correlation because the value ranges or unit may cause false assumptions. 6. Central limit theorem In many fields including natural and social sciences, when the distribution of a random variable is unknown, normal distribution is used. Central limit theorem (CLT) justifies why normal distribution can be used in such cases. According to the CLT, as we take more samples from a distribution, the sample averages will tend towards a normal distribution regardless of the population distribution. Consider a case that we need to learn the distribution of the heights of all 20-year-old people in a country. It is almost impossible and, of course not practical, to collect this data. So, we take samples of 20-year-old people across the country and calculate the average height of the people in samples. CLT states that as we take more samples from the population, sampling distribution will get close to a normal distribution. Why is it so important to have a normal distribution? Normal distribution is described in terms of mean and standard deviation which can easily be calculated. And, if we know the mean and standard deviation of a normal distribution, we can compute pretty much everything about it. 7. P-value P-value is a measure of the likelihood of a value that a random variable takes. Consider we have a random variable A and the value x. The p-value of x is the probability that A takes the value x or any value that has the same or less chance to be observed. The figure below shows the probability distribution of A. It is highly likely to observe a value around 10. As the values get higher or lower, the probabilities decrease. We have another random variable B and want to see if B is greater than A. The average sample means obtained from B is 12.5 . The p value for 12.5 is the green area in the graph below. The green area indicates the probability of getting 12.5 or a more extreme value (higher than 12.5 in our case). Letâ€™s say the p value is 0.11 but how do we interpret it? A p value of 0.11 means that we are 89% sure of the results. In other words, there is 11% chance that the results are due to random chance. Similarly, a p value of 0.05 means that there is 5% chance that the results are due to random chance. Note: Lower p values show more certainty in the result. If the average of sample means from the random variable B turns out to be 15 which is a more extreme value, the p value will be lower than 0.11. 8. Expected value of random variables The expected value of a random variable is the weighted average of all possible values of the variable. The weight here means the probability of the random variable taking a specific value. The expected value is calculated differently for discrete and continuous random variables. Discrete random variables take finitely many or countably infinitely many values. The number of rainy days in a year is a discrete random variable. Continuous random variables take uncountably infinitely many values. For instance, the time it takes from your home to the office is a continuous random variable. Depending on how you measure it (minutes, seconds, nanoseconds, and so on), it takes uncountably infinitely many values. The formula for the expected value of a discrete random variable is: The expected value of a continuous random variable is calculated with the same logic but using different methods. Since continuous random variables can take uncountably infinitely many values, we cannot talk about a variable taking a specific value. We rather focus on value ranges. In order to calculate the probability of value ranges, probability density functions (PDF) are used. PDF is a function that specifies the probability of a random variable taking value within a particular range. 9. Conditional probability Probability simply means the likelihood of an event to occur and always takes a value between 0 and 1 (0 and 1 inclusive). The probability of event A is denoted as p(A) and calculated as the number of the desired outcome divided by the number of all outcomes. For example, when you roll a die, the probability of getting a number less than three is 2 / 6. The number of desired outcomes is 2 (1 and 2); the number of total outcomes is 6. Conditional probability is the likelihood of an event A to occur given that another event that has a relation with event A has already occurred. Suppose that we have 6 blue balls and 4 yellows placed in two boxes as seen below. I ask you to randomly pick a ball. The probability of getting a blue ball is 6 / 10 = 0,6. What if I ask you to pick a ball from box A? The probability of picking a blue ball clearly decreases. The condition here is to pick from box A which clearly changes the probability of the event (picking a blue ball). The probability of event A given that event B has occurred is denoted as p(A|B). 10. Bayesâ€™ theorem According to Bayesâ€™ theorem, probability of event A given that event B has already occurred can be calculated using the probabilities of event A and event B and probability of event B given that A has already occurred. Bayesâ€™ theorem is so fundamental and ubiquitous that a field called â€œbayesian statisticsâ€ exists. In bayesian statistics, the probability of an event or hypothesis as evidence comes into play. Therefore, prior probabilities and posterior probabilities differ depending on the evidence. Naive bayes algorithm is structured by combining bayesâ€™ theorem and some naive assumptions. Naive bayes algorithm assumes that features are independent of each other and there is no correlation between features. Conclusion We have covered some basic yet fundamental statistical concepts. If you are working or plan to work in the field of data science, you are likely to encounter these concepts. There is, of course, much more to learn about statistics. Once you understand the basics, you can steadily build your way up to advanced topics. Thank you for reading. Please let me know if you have any feedback.
Adrian Lam Nov 26Â·17 min read Introduction Surely anyone who has dabbled in machine learning is familiar with gradient descent, and possibly even its close counterpart, stochastic gradient descent. If you have more than dabbled, then youâ€™re surely also aware of the fancier extensions like gradient descent with momentum, and Adam optimization. Perhaps less well-known are a class of optimization algorithms known as quasi-Newton methods. Though these optimization methods are less fervently advertised in popular accounts of machine learning, they hold an important place in the arsenal of machine learning practitioners. The goal of this article is to provide an introduction to the mathematical formulation of BFGS optimization, by far the most widely used quasi-Newton method. As such, the focus will be on the mathematical derivation of results, rather than the application of BFGS in code. It is my hope that by the end of the article, you will have gained an appreciation for what BFGS is, how it works, and why it was developed. A top priority of mine when crafting this article was to make it as accessible as possible. To this end, any non-trivial derivation will be explicitly shown, and the dreaded phrase â€œit is straightforward to showâ€ will never make an appearance. Instead of jumping right into quasi-Newton methods and BFGS, my strategy is to start off by doing a run-through of a few of the more basic optimization methods first, and explore their deficiencies. This would then provide a natural segue to quasi-Newton methods and how they aim to address these deficiencies. This might seem like long-winded way of getting to the main topic, but I believe it is well-justified if we are to truly appreciate the motivation behind developing BFGS, and to get a sense of where it stands within the landscape of optimization methods. So without further ado, letâ€™s start from the very beginning, with gradient descent. Gradient descent reviewed We begin with a lightning quick review of gradient descent, which is an iterative method for finding a local minimum of a real-valued, differentiable objective function f(x). To find a local minimum, we start off at a random initial point and iteratively take steps proportional to the negative gradient of the function f at the current point. Since the gradient of a function points in the direction of steepest ascent, the negative gradient points in the direction of steepest descent, and thus at each step of gradient descent, we are moving in the direction where f(x) decreases the fastest. Symbolically, an iteration of gradient descent is written as where Î± is a positive real number known as the learning rate, which controls the step size taken at each iteration. Too large of a learning rate and our model diverges out of control; too small of a learning rate and our model becomes highly inefficient, taking unnecessarily long to converge to the minimum. There is no a priori way of choosing a good learning rate that lies in the sweet spot between these two extremes, and in practice the optimal learning rate is typically found empirically, by looping through various values and seeing how they perform. Looking at equation (1), we see that gradient descent is a first-order optimization method, as it uses first-order information (ie. the gradient) to find the minimum. While this often reliably gets the job done, its main disadvantage lies in the fact that it is quite inefficient, even for a suitably chosen learning rate. By approximating our objective function linearly at each point, we are only working with very limited local information at each iteration, and we thus have to be cautious and restrain ourselves to small step sizes at each iteration. Perhaps we can do better, by obtaining more local information of the objective function at each iteration, in hopes that we can make more well-informed steps. A natural next step would be to look at the second-order behavior of the objective function. Newtonâ€™s method For simplicity, letâ€™s consider a function f of one variable first. Consulting Taylor, we know that the second order approximation of f about a point xá´‹+Ïµ is given by Our Taylor approximation is minimized when which corresponds to a step size of We can thus try a new iteration scheme which also takes into account the second-order behavior of the objective function. Generalizing to n dimensions, the first derivative is replaced by the gradient âˆ‡f and the second derivative is replaced by the Hessian H. Newtonâ€™s method in n dimensions is thus written as: This method of optimization, where we take into account the objective functionâ€™s second order behavior in addition to its first order behavior, is known as Newtonâ€™s method. In each iteration k, Newtonâ€™s method approximates the function f at the point xá´‹ with a paraboloid, and then proceeds to minimize that approximation by stepping to the minimum of that paraboloid (there is actually a caveat here that we will get to later). Notice that in contrast to regular gradient descent, there is no longer a need to set a learning rate parameter, because now our step size is determined exactly by the distance to the minimum of the fitted parabola at that point.To see how much we stand to gain from computing the Hessian, consider figure 1 below where we want to minimize our loss function, starting from the point (-5, 5). For a suitably chosen learning rate, gradient descent takes 229 steps to converge to the minimum. On the other hand, Newtonâ€™s method converges to the minimum in only six steps! If this is your first time learning about Newtonâ€™s method, youâ€™re probably just as shocked as I was when it was my first time. â€œSurely this is too good to be true!â€ you might exclaim. Why ever use gradient descent if Newtonâ€™s method converges to the minimum so much faster? It turns out that the benefits we gain from Newtonâ€™s method comes at a cost. There are two main issues with Newtonâ€™s method: Despite its limited practical use, Newtonâ€™s method is still of great theoretical value. We did see how efficient the second-order optimization can be if used correctly. What if we could somehow leverage the efficiency gained from considering second-order behavior, but avoid the computational cost of calculating the Hessian? In other words, can we get a sort of hybrid between gradient descent and Newtonâ€™s method, where we can have faster convergence than gradient descent, but lower operational cost per iteration than Newtonâ€™s method? It turns out thereâ€™s a class of optimization methods, called quasi-Newton methods, that does just that. Quasi-Newton methods We went through Newtonâ€™s method for optimization, which, in contrast to vanilla gradient descent, leverages second-order behavior in addition to first-order behavior at each step, making for a much faster convergence to the minimum. However, we also saw the downsides to Newtonâ€™s method â€” one of which is how computationally expensive it is to both calculate the Hessian and to invert it, especially when dimensions get large. Quasi-Newton methods are a class of optimization methods that attempt to address this issue. Recall that in Newtonâ€™s method, we make the following update at each iteration: where the Hessian is computed and inverted at each step. In quasi-Newton methods, instead of computing the actual Hessian, we just approximate it with a positive definite matrix B, which is updated from iteration to iteration using information computed from previous steps (we require B to be positive definite because we are optimizing a convex function, and this automatically takes care of the symmetry requirement of the Hessian).We immediately see that this scheme would yield a much less costly algorithm compared to Newtonâ€™s method, because instead of computing a large amount of new quantities at each iteration, weâ€™re largely making use of previously computed quantities. At this stage the nature of the update scheme for B has been left vague, because the specific update for B is given by the specific quasi-Newton method used. There is however one condition that all quasi-Newton methods must share, and that is the Hessian approximation B must satisfy the quasi-Newton condition (or secant equation): which is obtained from the first order Taylor expansion of âˆ‡ f(xá´‹â‚Šâ‚) about âˆ‡f(xá´‹) (we can also view this as sort of a finite difference equation of the gradient itself). We can rewrite the quasi-Newton condition more succinctly by letting yá´‹ = âˆ‡ f(xá´‹â‚Šâ‚) âˆ’âˆ‡f(xá´‹) and Î”xá´‹ = xá´‹â‚Šâ‚âˆ’xá´‹, so that we have Furthermore, we can verify the positive definiteness of our Hessian approximation B by pre-multiplying the quasi-Newton condition with Î”xá´‹áµ€, so our requirement for positive definiteness (ie. the curvature condition) can be expressed as Î”xá´‹áµ€ yá´‹>0.Before we go further however, letâ€™s take a step back and consider only one dimension, where our intuition is strong. The secant equation, in one dimension, is and the curvature condition is satisfied by requiring (fâ€²á´‹â‚Šâ‚âˆ’fâ€²á´‹)/(xá´‹â‚Šâ‚âˆ’xá´‹) > 0. Solving for fâ€³ and substituting into Newtonâ€™s method in one dimension, we get We thus have here an optimization method that leverages the (approximate) second-order behavior of the objective function in order to converge faster, without actually taking any second derivatives. Instead, at each iteration k+1, we construct an approximate inverse second derivative using only quantities from previous steps â€” in this case the first derivative from the previous two iterations k and kâˆ’1. This method â€” approximating Newtonâ€™s method in one dimension by replacing the second derivative with its finite difference approximation, is known as the secant method, a subclass of quasi-Newton methods. Now back to our n-dimensional quasi-Newton condition (equation 4). At first glance, it looks like we could just work analogously to our one-dimensional case: we can simply solve for Bá´‹â‚Šâ‚ directly, and substitute it into Newtonâ€™s iterative step (2). Job done, right? Not quite, actually. Despite looking deceptively similar to our one-dimensional case, remember that B is in general a symmetric n Ã— n matrix, with n(n+1)/2 components, whereas our equation only has n components. This means that weâ€™re trying to solve for n(n+1)/2 unknowns with only n equations, making this an underdetermined system. In fact, we were only able to find a unique solution to the secant equation in one dimension because the unknown components of the Hessian, being 1(1+1)/2 = 1, coincide with the one equation that we have. In general, there are n(n+1)/2 âˆ’ n= n(nâˆ’1)/2 unknowns that we cannot solve for. This is where quasi-Newton methods come in, where the secant method is generalized to multidimensional objective functions. Instead of approximating the second derivative merely by using the finite difference like in the secant method, quasi-Newton methods have to impose additional constraints. The common theme still runs through though â€” at each iteration k+1, the new Hessian approximation Bá´‹â‚Šâ‚ is obtained using only previous gradient information. Various quasi-Newton methods have been developed over the years, and they differ in how the approximate Hessian B is updated at each iteration. As of today, the most widely used quasi-Newton method is the BFGS method, and this will be our focus for the remaining of this article. BFGS optimization Itâ€™s been somewhat of a long trek so far, so letâ€™s pause for moment and do a quick recap before moving on. Our objective is to find the minimum of a (twice-differentiable) convex function. A simple approach to this is gradient descent â€” starting from some initial point, we slowly move downhill by taking iterative steps proportional to the negative gradient of the function at each point. Newton then taught us that we can take far less steps and converge much quicker to the minimum if we also consider the second-order behavior of the function, by computing the (inverse) Hessian at each step. This comes at a cost, however, as calculating (and inverting) the Hessian takes a lot of resources. A compromise would be to instead just approximate the Hessian at each step, subject to the quasi-Newton condition. In one dimension, this just amounts to approximating the second derivative by replacing it with a finite difference approximation; this method of optimization is called the secant method. In more than one dimension, the quasi-Newton condition does not uniquely specify the Hessian estimate B, and we need to impose further constraints on B to solve for it. Different quasi-Newton methods offer their own method for constraining the solution. Here, we will focus on one of the most popular methods, known as the BFGS method. The name is an acronym of the algorithmâ€™s creators: Broyden, Fletcher, Goldfarb, and Shanno, who each came up with the algorithm independently in 1970 [7â€“10]. We saw previously that in n>1 dimensions, the quasi-Newton condition (4) is underdetermined. To determine an update scheme for B then, we will need to impose additional constraints. One such constrain that weâ€™ve already mentioned is the symmetry and positive-definiteness of B â€” these properties should be preserved in each update. Another desirable property we want is for Bá´‹â‚Šâ‚ to be sufficiently close to Bá´‹ at each update k+1. A formal way to characterize the notion of â€œclosenessâ€ for matrices is the matrix norm. Thus, we should look to minimize the quantity ||Bá´‹â‚Šâ‚âˆ’Bá´‹||. Putting all our conditions together, we can formulate our problem as Referring back to Newtonâ€™s method, we recall that itâ€™s actually the Hessianâ€™s inverse, instead of the Hessian itself, that makes an appearance. So instead of computing the approximate Hessian B, we can just as well compute the approximate inverse Bâ»Â¹ directly. We can thus alternatively formulate (5) as (Notice the inverted quasi-Newton condition.) To repeat, we minimize the change in Bâ»Â¹ at each iteration, subject to the (inverted) quasi-Newton condition and the requirement that it be symmetric. We have also previously mentioned many times the requirement that B (and by extension Bâ»Â¹) be positive definite; it turns out this property comes along for the ride when we solve this problem. We will show this eventually; in the meantime, Iâ€™ll have to ask you to hold your breath. To solve for Bá´‹â‚Šâ‚, we still have to specify the particular matrix norm weâ€™re using in (6). Different choices of norms result in different quasi-Newton methods, characterized by a different resulting update scheme. In the BFGS method, the norm is chosen to be the Frobenius norm: which is just the square root of the sum of the absolute value squared of the matrix elements. Solving (6) from scratch is no easy feat, and we will not go through it here. For the highly mathematically inclined reader, refer to references [4-6] for detailed derivations. For our purposes, it suffices to know that the problem ends up being equivalent to updating our approximate Hessian at each iteration by adding two symmetric, rank-one matrices U and V: To fulfill the aforementioned conditions, the update matrices can then be chosen to be of the form U = a u uáµ€ and V = b v váµ€, where u and v are linearly independent non-zero vectors, and a and b are constants. The outer product of any two non-zero vectors is always rank one, and since U and V are both the outer product of a vector with itself, they are also symmetric, taking care of the requirement that the Hessian remains symmetric upon updates. We then have Since Uá´‹ and Vá´‹ are rank-one (and u and v are linearly independent), their sum is rank-two, and an update of this form is known as a rank-two update. Thus if we start out with a symmetric matrix Bâ‚€, then Bá´‹ will remain symmetric for all k. The rank-two condition guarantees the â€œclosenessâ€ of Bá´‹ and Bá´‹â‚Šâ‚at each iteration.Furthermore, we have to impose the quasi-Newton condition Bá´‹â‚Šâ‚ Î”x = yá´‹ : At this point, a natural choice for u and v would be u = yá´‹ and v = Bá´‹ Î”xá´‹. We then have Substituting a and b back into (7), we have the BFGS update in all its glory: As advertised, we are updating the approximate Hessian at each iteration using only previous gradient information. Note, however, that in practice we actually want to compute Bâ»Â¹ directly, because itâ€™s the inverse Hessian that appears in a Newton update. To invert (8), we make use of the Woodbury formula, which tells us how to invert the sum of an invertible matrix A and a rank-k correction: We can thus obtain the formula for a BFGS update for the inverse of B. We first rewrite the update in a more suitable form for applying the Woodbury formula (to avoid clutter, weâ€™ll suppress the subscripts k+1 and k, instead denoting the update as Bâ‚Š): Plugging into the Woodbury formula, So while equation (8) is cleaner and allows for more straightforward analysis, equation (9) is what we actually want to compute in practice. Again, each update is made only requiring previous gradient information. At each iteration, we update the value of Bâ»Â¹ using only the values of Î”xá´‹ = xá´‹â‚Šâ‚âˆ’xá´‹ and yá´‹ = âˆ‡ f(xá´‹â‚Šâ‚) âˆ’âˆ‡f(xá´‹) of the previous steps, in accordance to equation (9). By directly estimating the inverse Hessian at each step, we are completely doing away with those laborious O(nÂ³) operations of inverting the Hessian, as in Newtonâ€™s method. We are now also in a position to show that our rank-two update preserves positive definiteness. From (9), we can calculate the quantity for a non-zero vector z. If Bá´‹â»Â¹ is positive definite, then both terms are non-negative, with the second term being zero only if Î”xáµ€ z = 0. Positive definiteness is thus preserved by BFGSâ€™s rank-two update. If we were to go back to (7) and repeat the development but with a rank-one update instead, we wouldnâ€™t actually end up with a positive-definite preserving update [4, 5]. This explains why BFGS uses a rank-two update: it is the update with the lowest rank that preserves positive definiteness. One final implementation detail that we previously glossed over: since an update of Bâ»Â¹ depends on its previous value, we have to initialize Bâ‚€â»Â¹ to kick off the algorithm. There are two natural ways to do this. The first approach is to set Bâ‚€â»Â¹ to the identity matrix, in which case the first step will be equivalent to vanilla gradient descent, and subsequent updates will incrementally refine it to get closer to the inverse Hessian. Another approach would be to compute and invert the true Hessian at the initial point. This would start the algorithm off with more efficient steps, but comes at an initial cost of computing the true Hessian and inverting it. Concluding remarks Being an introductory piece, the aim of this discussion was to present quasi-Newton methods and BFGS in a manner that is as accessible as possible. As such, weâ€™ve really only just skimmed the surface of all the mathematical intricacies and performance considerations that underlie BFGS and other quasi-Newton methods. There is a wealth of resources out there on this subject, a very small selection of which I have included in the references below. I encourage readers who are interested in diving deeper to seek out these resources. Be forewarned though â€” some of these references arenâ€™t easy! Thank you for making it all the way to the end! Hopefully youâ€™ve gained an appreciation for how BFGS works under the hood, so that next time you call scipy.optimize.minimize(method=â€˜BFGS'), instead of viewing it as a complete black box, youâ€™ll smile to yourself knowingly, fully aware that BFGS is simply iteratively making positive-definite preserving rank-two updates to the loss functionâ€™s approximate inverse Hessian, to efficiently find you its minimum. References Cursory run-throughs:[1] BFGS on Wikipedia[2] Quasi-Newton methods on Wikipedia[3] Newtonâ€™s method on Wikipedia Advanced references on BFGS:[4] J. Nocedal and S. Wright. Numerical Optimization (Chapter 6). Springer, 2nd edition, 2006.[5] Oxford University lecture notes by R. Hauser[6] MIT 18.335 lecture notes by S. G. Johnson The original papers:[7] C. G. Broyden, â€œThe convergence of a class of double-rank minimization algorithmsâ€, J. Inst. Math. Appl. 6, 76â€“90 (1970)[8] R. Fletcher, â€œA new approach to variable metric algorithmsâ€, Comp. J. 13, 317â€“322 (1970)[9] D. F. Goldfarb, â€œA family of variable-metric methods derived by variational meansâ€, Math. Comp. 24, 23â€“26 (1970)[10] D. Shanno, â€œConditioning of quasi-Newton methods for function minimizationâ€, Math. Comp. 24, 647â€“656 (1970)
1 day agoÂ·4 min read A greedy alternative for your Hyperparameter Tuning This story will show you an alternative Random Search algorithm that might be useful for your Hyperparameter Tuning. I will cover the main concept behind it and give a detailed example of the algorithm. What is a greedy algorithm? In informatics, an algorithm is used to solve problems systematically. There are many concepts on how to design an algorithm. A greedy concept is one of the most basic ones. A solution is defined stepwise and the greedy algorithm makes his decision (to solve the problem at a certain step) based on the current best available solution. Letâ€™s go with an example and take a look at Figure 1. You can see a tree with weighted edges. The goal is to minimize the overall costs to get from node A to node F. The correct answer is A-C-E-F. However a greedy algorithm would choose A-B-D-F. Either choose node B or choose node C at the beginning and a greedy algorithm would obviously choose node B over node C. I have to admit that this a rather unfavorable example for a greedy algorithm but it shows the weakness of such an algorithm. A greedy algorithm finds a solution but there is no guarantee itâ€™s the overall best solution. The concept of Deep Random Search In Random Search you simply test a random combination of hyperparameters that lay in a specified range. The algorithm takes random guesses. In Deep Random Search (DRS) you do the same but after a specified amount of guesses you update the ranges of your hyperparameters. The following example demonstrates the algorithm. Figure 2 shows a simple Random Search in two dimensions, A and B. The initial ranges in level 1 are [A0;A1] and [B0;B1]. After a specified amount of guesses the range of each dimension is cut in half and now you have four (only in 2D) quadrants. In n dimensions you would get 2 to the power of n quadrants when cutting each dimension in half. The best solution found so far is located in the first quadrant. This is the one we take to level 2. In level 2 we first have to specify the new ranges (shown in Figure 3) and then do the same again. The guesses taken in level 2 are indicated in red (the blue ones belong to level 1). The new best solution is located in the second quadrant. And again, this is the quadrant we take to level 3, the last level. As before, after specifying the new ranges a Random Search is performed (guesses taken in level 3 are indicated in green). Now we can identify the best solution again which represents the solution of our Deep Random Search. Discussion A Deep Random Search is greedy multilevel Random Search. In each Level a Random Search is performed and the ranges of the dimensions are updated towards the best known solution. The update of the ranges is the greedy part of DRS. The best solution could be located in a different quadrant then the best known solution and now DRS canâ€™t find the overall best solution anymore. This is the weakness of DRS. However DRS can help to find new good solutions and new ranges for your hyperparameters for further investigations and analyzes. Also you can try different methods for updating the ranges. For example you can very the amount of cuts you perform in each level (one for each dimension in this example). Or you define a new method for determining the quadrant that is taken to the next level. In this example we took the quadrant which includes the best known solution. As an alternative you could define a performance index for each quadrant that is influenced by every guess in a quadrant. The amount of guesses you take each level and the amount of levels are general parameters of DRS.
Soner YÄ±ldÄ±rÄ±m 2 days agoÂ·4 min read Pandas is a widely-used data analysis and manipulation library for Python. It provides numerous functions and methods to provide robust and efficient data analysis process. In a typical data analysis or cleaning process, we are likely to perform many operations. As the number of operations increase, the code starts to look messy and harder to maintain. One way to overcome this issue is using the pipe function of Pandas. What pipe function does is to allow combining many operations in a chain-like fashion. In this article, we will go over examples to understand how the pipe function can be used to produce cleaner and more maintainable code. We will first do some data cleaning and manipulation on a sample dataframe in separate steps. After that, we will combine these steps using the pipe function. Letâ€™s start by importing libraries and creating the dataframe. The dataset contains information about a marketing campaign. It is available here on Kaggle. The first operation I want to do is to drop columns that have lots of missing values. The code above drops the columns with 40 percent or more missing values. The value we pass to the thresh parameter of dropna function indicates the minimum number of required non-missing values. I also want to remove some outliers. In the salary column, I want to keep the values between the 5th and 95th quantiles. We find the lower and upper limits of the desired range by using the quantile function of numpy. These values are then used to filter the dataframe. It is important to note that there are many different ways to detect outliers. In fact, the way we have used is kind of superficial. There exist more realistic alternatives. However, the focus here is the pipe function. Thus, you can implement the operation that fits best for your task. The dataframe contains many categorical variables. If the number of categories are few compared to the total number values, it is better to use the category data type instead of object. It saves a great amount of memory depending on the data size. The following code will go over columns with object data type. If the number of categories are less than 5 percent of the total number of values, the data type of the column will be changed to category. We have done three steps of data cleaning and manipulation. Depending on the task, the number of steps might be more. Letâ€™s create a pipe that accomplish all these tasks. The pipe function takes functions as inputs. These functions need to take a dataframe as input and return a dataframe. Thus, we need to define functions for each task. You may argue that what the point is if we need to define functions. It does not seem like simplifying the workflow. You are right for one particular task but we need to think more generally. Consider you are doing the same operations many times. In such case, creating a pipe makes the process easier and also provides cleaner code. We have mentioned that the pipe function takes a function as input. If the function we pass to the pipe function has any arguments, we can pass it to the pipe function along with the function. It makes the pipe function even more efficient. For instance, the remove_outliers function takes a column name as argument. The function removes the outliers in that column. We can now create our pipe. It looks neat and clean. We can add as many steps as needed. The only criterion is that the functions in the pipe should take a dataframe as argument and return a dataframe. Just like with the remove_outliers function, we can pass the arguments of the functions to the pipe function as an argument. This flexibility makes the pipes more useful. One important thing to mention is that the pipe function modifies the original dataframe. We should avoid changing the original dataset if possible. To overcome this issue, we can use a copy of the original dataframe in the pipe. Furthermore, we can add a step that makes a copy of the dataframe in the beginning of the pipe. Our pipeline is complete now. Letâ€™s compare the original dataframe with the cleaned to confirm it is working. The pipeline is working as expected. Conclusion The pipes provide cleaner and more maintainable syntax for data analysis. Another advantage is that they automatize the steps of data cleaning and manipulation. If you are doing the same operations over and over, you should definitely consider creating a pipeline. Thank you for reading. Please let me know if you have any feedback.
Aditya Bhattacharya 2 days agoÂ·8 min read In my previous TDS article I described about the Machine Learning variant of Deep Hybrid Learning and how easily it can be applied for image data. If you are someone who is new to the concept of Deep Hybrid Learning, I would highly recommend you to have a look at the session recording of my past session at GIDS AI/ML 2020 conference. The recording video can also be directly accessed from my youtube channel. In this article, we will see how Deep Hybrid Learning can be applied for time series data and whether it is as effective as it is with image data. But before we begin, in case if you are very new to time series data, I would recommend looking at my previous posts related to time series data analysis: If you are new to TensorFlow and wondering how to apply TensorFlow for time series forecasting, this article from my website can be really helpful. The article does give very detailed code walkthrough of using TensorFlow for time series prediction. In this article also, I will take a similar approach of providing a very detailed approach for using Deep Hybrid Learning for Time Series Forecasting in 5 simple steps. For this post, I am going to use the same Sunspot Data from Kaggle like my previous article. As mentioned there, the data can be easily downloaded from my GitHub project TimeSeries-Using-TensorFlow. I will encourage everyone to use google colab notebooks where the modules required are already installed and the infrastructure is ready for use. Now, letâ€™s get started! Step 1 â€” Downloading and loading the data The data can be downloaded using a simple command in google colab â€“ !wget â€” no-check-certificate https://raw.githubusercontent.com/adib0073/TimeSeries-Using-TensorFlow/main/Data/Sunspots.csv -O /tmp/sunspots.csv Once the download is complete, we can use pandas to load the data into dataframes. # Loading the data as a pandas dataframe df = pd.read_csv(â€˜/tmp/sunspots.csvâ€™, index_col=0) df.head() We can take a glance at the data, which looks like this: Now, ideally we should do an extensive Exploratory Data Analysis (EDA) to understand the trend, seasonality of the data. But for this case, to keep things simple, we will do a visual inspection for the data. Step 2 â€” Preparing the data In this step we have to transform the loaded data and process it so that it can be passed as an input to the Deep Hybrid Learning model after which we can start the training process. We can consider time series forecasting as a sequential machine learning regression problem, and the time series data is converted into a set of feature values and the corresponding true or target value. Since regression is a supervised learning problem, we need the target value, in which the lagged time series data becomes the feature values like this: We will follow a window or buffer approach in which we would have to consider an appropriate window size. Then we would move the window from left to right of the sequence or the series data. We will consider the value immediately to right of the window frame as target or the true value. So, each time-step we will shift or move the window so as to get a new row of features values and target value pairs. In this way we form the training data and training labels. In a similar way, we form the test and the validation dataset, which is typically required for a machine learning prediction model. Also, remember that for a predictive model, having a wider observation window and a narrow prediction window can give better results. Next, for the train-test-validation split ratio we will have to figure that out based on the size of the data. For this example I have used a split ratio of 0.8 and based on the seasonality of the data, we have taken a window size of 60. But these variables are all hyper-parameters, which requires some tuning to get the best possible results. The code for the same is as follows: Next, we will prepare a data generator that prepares the training and testing data for us. Now, we have our processed data ready for feeding this into our model. Step 3 â€” Building the DHL Model and Training We will use a simple version of a Deep Hybrid Learning architecture for this problem. As discussed we will use the Deep Learning variant with Late Fusion technique. The model architecture looks like this: Here we have used a combination model of 1D CNN for extracting the initial sequential features and then combined with 2 LSTM layers for the feature extraction part and finally passed the same into tradional DNN Fully Connected Layers to produce the final output. The code for the model architecture looks like this: Next we would need to choose other hyper-parameters like learning rate, optimizer and loss function. I am not covering the intuition behind selection of these values in this post so as to keep it simple. But feel free to comment or drop me a note in this if you are curious to find out more about this. Step 4 â€” Model Evaluation Next we would see how to evaluate our model. But first, after the training process, it is always a better idea to plot the model loss curves to see whether the model is actually learning. From the model loss curves, we do see clear presence of over-fitting. At the end of the article I will give some hint of how to handle this to make the model better, but we can see the model loss is decreasing with more training time, which is a good indication that the model is learning. Now, for the model evaluation we would need to pick a metric. In a future article, I will include about various model evaluation metrics for time series data. But for this case, we will use MAE as the metric. The MAE value that we got is roughly around 40. Which is not bad, but a little high for this case. And the reason why the model error is more is because of the initial over-fitting that we saw. Step 5â€” Visualizing the model results As a final step, letâ€™s visualize the results that we are getting over the test data and letâ€™s inspect if at all the model is close for predicting good results. From the first plot, we can see that the predicted values does follow a similar seasonal pattern and trend as the actual values, but the peaks are not as high as the actual ones. Also, since time series forecast should be ranged prediction not a single point estimate, we will use the error rate to form the confidence interval or the confidence band. We can see the the error bands are wide, which means the model is not very much confident and might have some prediction error. The code for visualization is as follows: And thus we do have a Deep Hybrid Learning model for time series forecasting and we have used TensorFlow for forming the model and implementing the flow. But if you are wondering how can the result be improved, I have the following recommendations for you: Also sometimes, a simpler model may give better result. In my post Time Series Forecasting using Deep Learning with TensorFlow I got much better results just by using a simple Deep Neural Network. Now, unlike with image data, we saw that with time series data, Deep Hybrid Learner was not significantly better than a conventional Deep Learning, Machine Learning or Statistical methods. But, yes after doing thorough hyper parameter tuning, I am sure the results would be much better! Thus, this brings us to the end of this article. The full working code and notebook can be obtained from here. Keep following me: https://medium.com/@adib0073 and my website: https://www.aditya-bhattacharya.net/ for more! Please feel free to reach for any query or comment!
Dec 15, 2018Â·7 min read Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines. Researchers and enthusiasts alike, work on numerous aspects of the field to make amazing things happen. One of many such areas is the domain of Computer Vision. The agenda for this field is to enable machines to view the world as humans do, perceive it in a similar manner and even use the knowledge for a multitude of tasks such as Image & Video recognition, Image Analysis & Classification, Media Recreation, Recommendation Systems, Natural Language Processing, etc. The advancements in Computer Vision with Deep Learning has been constructed and perfected with time, primarily over one particular algorithm â€” a Convolutional Neural Network. Introduction A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics. The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area. Why ConvNets over Feed-Forward Neural Nets? An image is nothing but a matrix of pixel values, right? So why not just flatten the image (e.g. 3x3 image matrix into a 9x1 vector) and feed it to a Multi-Level Perceptron for classification purposes? Uh.. not really. In cases of extremely basic binary images, the method might show an average precision score while performing prediction of classes but would have little to no accuracy when it comes to complex images having pixel dependencies throughout. A ConvNet is able to successfully capture the Spatial and Temporal dependencies in an image through the application of relevant filters. The architecture performs a better fitting to the image dataset due to the reduction in the number of parameters involved and reusability of weights. In other words, the network can be trained to understand the sophistication of the image better. Input Image In the figure, we have an RGB image which has been separated by its three color planes â€” Red, Green, and Blue. There are a number of such color spaces in which images exist â€” Grayscale, RGB, HSV, CMYK, etc. You can imagine how computationally intensive things would get once the images reach dimensions, say 8K (7680Ã—4320). The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction. This is important when we are to design an architecture which is not only good at learning features but also is scalable to massive datasets. Convolution Layer â€” The Kernel Image Dimensions = 5 (Height) x 5 (Breadth) x 1 (Number of channels, eg. RGB) In the above demonstration, the green section resembles our 5x5x1 input image, I. The element involved in carrying out the convolution operation in the first part of a Convolutional Layer is called the Kernel/Filter, K, represented in the color yellow. We have selected K as a 3x3x1 matrix. The Kernel shifts 9 times because of Stride Length = 1 (Non-Strided), every time performing a matrix multiplication operation between K and the portion P of the image over which the kernel is hovering. The filter moves to the right with a certain Stride Value till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride Value and repeats the process until the entire image is traversed. In the case of images with multiple channels (e.g. RGB), the Kernel has the same depth as that of the input image. Matrix Multiplication is performed between Kn and In stack ([K1, I1]; [K2, I2]; [K3, I3]) and all the results are summed with the bias to give us a squashed one-depth channel Convoluted Feature Output. The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer. Conventionally, the first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would. There are two types of results to the operation â€” one in which the convolved feature is reduced in dimensionality as compared to the input, and the other in which the dimensionality is either increased or remains the same. This is done by applying Valid Padding in case of the former, or Same Padding in the case of the latter. When we augment the 5x5x1 image into a 6x6x1 image and then apply the 3x3x1 kernel over it, we find that the convolved matrix turns out to be of dimensions 5x5x1. Hence the name â€” Same Padding. On the other hand, if we perform the same operation without padding, we are presented with a matrix which has dimensions of the Kernel (3x3x1) itself â€” Valid Padding. The following repository houses many such GIFs which would help you get a better understanding of how Padding and Stride Length work together to achieve results relevant to our needs. vdumoulin/conv_arithmetic A technical report on convolution arithmetic in the context of deep learning - vdumoulin/conv_arithmetic github.com Pooling Layer Similar to the Convolutional Layer, the Pooling layer is responsible for reducing the spatial size of the Convolved Feature. This is to decrease the computational power required to process the data through dimensionality reduction. Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model. There are two types of Pooling: Max Pooling and Average Pooling. Max Pooling returns the maximum value from the portion of the image covered by the Kernel. On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel. Max Pooling also performs as a Noise Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction. On the other hand, Average Pooling simply performs dimensionality reduction as a noise suppressing mechanism. Hence, we can say that Max Pooling performs a lot better than Average Pooling. The Convolutional Layer and the Pooling Layer, together form the i-th layer of a Convolutional Neural Network. Depending on the complexities in the images, the number of such layers may be increased for capturing low-levels details even further, but at the cost of more computational power. After going through the above process, we have successfully enabled the model to understand the features. Moving on, we are going to flatten the final output and feed it to a regular Neural Network for classification purposes. Classification â€” Fully Connected Layer (FC Layer) Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space. Now that we have converted our input image into a suitable form for our Multi-Level Perceptron, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique. There are various architectures of CNNs available which have been key in building algorithms which power and shall power AI as a whole in the foreseeable future. Some of them have been listed below: GitHub Notebook â€” Recognising Hand Written Digits using MNIST Dataset with TensorFlow ss-is-master-chief/MNIST-Digit.Recognizer-CNNs Implementation of CNN to recognize hand written digits (MNIST) running for 10 epochs. Accuracy: 98.99% â€¦ github.com
David B. Clear Nov 11Â·4 min read  Whatâ€™s a hypnic jerk? Hypothesis 1: Your body twitches as daytime motor control is overridden by sleep paralysis Hypothesis 2: Your brain thinks youâ€™re a monkey falling off a tree Whatâ€™s clear either way Ahhâ€¦ sleep. How nice. You turn off the lights. You close your weary eyes. You sigh. You relax. Your breathing slows down. Your mind begins to wander off, fading into the nightly oblivion. Thenâ€¦ You stumble, trip, fall. Your body jolts. Your leg kicks. Your heart pounds. Huh? What happened? Did you mistakenly fall asleep on a trapdoor? Nope. You simply experienced a hypnic jerk. Why Do We Sleep? The hours spent sleeping may be the most important for your brain elemental.medium.com Whatâ€™s a hypnic jerk? A hypnic jerk, or sleep start, is a phenomenon that occurs when your body transitions from wakefulness to sleep. It involves a sudden involuntary muscle twitch and is frequently accompanied by a falling or tripping sensation. Itâ€™s that strange muscle spasm that happens when youâ€™re lying in bed, trying to sleep, and are suddenly jolted awake because you feel like you stumbled over something. Hypnic jerks are common and benign. But what causes them? Well, no one really knows. Itâ€™s still a mystery. However, researchers have come up with several hypotheses that may explain them, with the following two being the most popular. Hypothesis 1: Your body twitches as daytime motor control is overridden by sleep paralysis How is it that a bedfellow of yours doesnâ€™t wake up pummeled and bruised if you have a dream about a boxing match? Is it because theyâ€™re having a complementary dream where theyâ€™re blocking all your jabs, hooks, and other punches? Nope. The person sharing the bed with you doesnâ€™t get pummeled because when youâ€™re asleep, your body is paralyzed. This is due to something called REM sleep atonia, which prevents you from acting out your dreams. REM atonia works by inhibiting your motor neurons. It does so by raising the bar on the amount of electricity the brain must send down a motor neuron to trigger a movement. So, for instance, the little bit of electricity that your brain sends to your finger to make it move when youâ€™re awake is no longer enough when youâ€™re under REM atonia. When youâ€™re asleep, your body is paralyzed. This is due to something called REM sleep atonia, which prevents you from acting out your dreams. Now, the thing is that there is no single on/off switch in your body that inhibits all your motor neurons at once. Instead, the subsystems of your brain that handle sleep need to wrestle control from the subsystems that handle wakefulness. And sometimes, during this wrestling match, some motor neurons are fired randomly, causing your body to twitch. Hypothesis 2: Your brain thinks youâ€™re a monkey falling off a tree Imagine youâ€™re a monkey and the last rays of sunlight have just disappeared behind the green forest canopy. Itâ€™s getting dark, and you say to yourself: time for sleep. Your brain begins to ooze some melatonin into your bloodstream and you yawn. Drowsy, you settle down on a comfortable tree branch. Your eyelids become heavy and your breathing slows. The outside world begins to fade. Sounds become distant. At this point, the subconscious part of your brain takes over. â€œPerfect,â€ it says, â€œtime to boot up the dream images.â€ Your brain initiates the dream procedure, and just when youâ€™re about to nod off completely, it notices that all your muscles have suddenly and unexpectedly relaxed. â€œHoly Banana!â€ your brain screams panic-stricken, â€œMayday! Mayday! Weâ€™re in freefall! Dammit! Wake up! Wake up! Shit, crap! Brace for impaaaact!â€ As youâ€™re probably aware, we humans descend from primates who lived and slept on trees. This means that weâ€™ve inherited some monkey brain routines that no longer serve any purpose. Among them, according to the monkey-fall hypothesis, is a reflex that jolts you awake when youâ€™re falling from a tree. You see, when a monkey is unexpectedly soaring through the air, its muscles no longer have to prop it up and so they go limp. Confusingly, however, your muscles also go limp when youâ€™re sleeping. So, when you drift off into sleep and your muscles relax a little too fast, your groggy brain sometimes misinterprets this for falling off a tree. As a result, your brain freaks out and triggers a reflex that startles you awake in an attempt to prepare for an imminent crash onto the forest floor. Little does your brain know, in its sleepy stateâ€”and that you no longer live in trees. Whatâ€™s clear either way Hypnic jerks are involuntary muscle contractions that occur during the transition from wakefulness to sleep. Theyâ€™re most likely to occur if youâ€™ve been gulping down too much coffee, have been stressed or sleep-deprived, or did some vigorous exercise before going to bed. About 70% of people have experienced them. Even so, they are not well understood. Either way, hypnic jerks are benign and nothing to worry about. The worst that can happen is probably an occasional kick against the shin of whoever is sharing the bed with you.
The MIND Foundation 6 days agoÂ·8 min read Written by Christoph Benner for the MIND Blog. â€˜This connection, itâ€™s just a lovely feelingâ€¦ this sense of connectedness, we are all interconnected.â€™ - Depressed patient after experiencing a therapeutic dose of psilocybin. In the ever-accelerating world in which we live, many people feel increasingly lost and left behind. Despite omnipresent social media, rates of anxiety and depression have been rising for years. As a potential antidote against this loss of connection to oneself and others, psychedelics are experiencing a renaissance in science and medicine: They can provide an opportunity to identify misguided life decisions and weave new, meaningful patterns into our social webs. Just how are psychedelic substances able to do this? In the following paragraphs, I will argue that an underlying principle of psychedelics is connection â€” or even reconnection. This reconnection works on different levels: the biological, the psychological, the societal, and the ecological. The Biological Level Letâ€™s do a little magic trick. Please adopt a relaxed, comfortable position and do the following: Think about your favorite food. Imagine the texture, the different flavors, the colors, the smell. Try to completely dwell in the delicious experience this food provides you with. Pretty cool trick, isnâ€™t it? Your brain created a kind of virtual reality without the need for special goggles! This virtual reality, the food you were thinking about ten seconds ago, is generated by neurons in the brain connecting with each other. With this example in mind, letâ€™s take a look at how considering the â€˜wiringâ€™ of neurons can help us understand depression. Depression is associated with a change in neuronal connectivity, among other things, in brain areas belonging to what neuroscientists call the default mode network (DMN). This network â€” as the name suggests â€” is active in our default mode: when we daydream, think about ourselves, or let our minds wander. On the other hand, when we are engaged in a task that requires focusing on a specific external stimulus, the DMNâ€™s activity is dampened and other brain areas become more active. It is this fluidity in functional connectivity that makes for a healthy mind. In depressed patients, however, fluidity is reduced to rigidity. Cognition shifts dramatically towards constant negative thinking patterns, particularly about oneself. Likewise, trying to imagine anything pleasant â€” like oneâ€™s favorite food â€” becomes difficult when cognition is stuck in such a negative loop. Fascinatingly, psychedelics are able to loosen the rigidity within the DMN and allow brain regions to reconnect with each other in a way that resembles a healthy dynamic. In fact, psychedelic compounds seem to selectively increase neuronal connectivity at the molecular level. This means that they may be able to reconnect neuronal pathways that have been lost during a long period of depression. One can see how this effect on neuronal connectivity translates to increased well-being by looking atâ€¦ The Psychological Level In one of their publications, the research team headed by Dr. Robin Carhart-Harris roughly defined the ego as â€œa sensation of possessing an immutable identity or personality; most simply, the ego is our â€˜sense of selfâ€™â€. For each of us, the sense of self encapsulates a range of mental processes: the feeling of having a body, remembering past experiences, feeling emotions, or planning the future. Having an ego associated with these mental processes is usually not a problem. But disorders like depression arise when the ego takes the wheel on the highway of cognitive life, steering towards negative cognition at every opportunity. This illusory and false identification of (negative) thoughts with the sense of self is like mistakenly identifying a picture of an object with the object itself, as beautifully illustrated by RenÃ© Magritteâ€™s famous painting â€œThe Treachery of Imagesâ€: To put it simply: Just as there is no pipe in the picture, there is no self or ego in a thought. Carhart-Harris and his team further postulate that the function of the DMN correlates with the function of the ego. He writes: â€œSpecifically, we propose that within-default mode network (DMN) resting-state functional connectivity (RSFC) and spontaneous, synchronous oscillatory activity in the posterior cingulate cortex (PCC), particularly in the alpha (8â€“13 Hz) frequency band, can be treated as neural correlates of â€˜ego integrity.â€™â€ What that means is that our sense of self is associated with the activity in a functional neuronal network (i.e. the DMN), held together by alpha band activity (8â€“13 Hz) and forming a well-orchestrated pattern (synchronous oscillatory activity). As explained above, a depressed brain is partly characterized by a too tightly-bound DMN that translates into the psychological burden these patients carry. In other words, depressed patients suffer from an ego that is too dominant. What happens, then, if psychedelics temporarily cut the threads that hold the ego together? A depressed patient who underwent a psilocybin-assisted psychotherapy session gives the answer: â€œThis connection, itâ€™s just a lovely feelingâ€¦ this sense of connectedness, we are all interconnected.â€ (male, aged 52). As a possible explanation for this increased feeling of connectedness, psychedelics seem to bring the brain into a state of higher entropy, defined as an increase in possible connections between regions. They seem to induce a collapse of the usual pattern of activity within the DMN, and thereby the subjectively experienced ego (see the image below). One consequence of this seems to be a greater feeling of connection with the environment. This environment could be other people (society) or the surrounding nature (ecology). The Societal Level Timothy Learyâ€™s quote â€œturn on, tune in, drop outâ€ was the famous triad of the 1960â€™s psychedelic counter-culture that led many people into the novel realm of the psychedelic experience. While the statementâ€™s political implications are controversial, recent research illuminates why it may have been so appealing in combination with psychedelics. Recent psychological research has now shown that LSD changes social cognition by enhancing openness, trust, empathy, prosocial behavior, the desire to be with other people, and perceived closeness to others. Thus, it is not surprising that people who shared this previously unknown, deeply changed perception of themselves with others also engaged in discussions about how a new, better society might be established. The tragedy was that the discovery of LSD was only 30 years old, and psychedelic rituals didnâ€™t come with a harm reduction manual. The promised liberating effect on the mind was so alluring that many people took psychedelics in an irresponsible manner, which led to the political denunciation of psychedelic compounds by the administration of Richard Nixon. How that ended is well-documented: Scientific investigation into the medical applications of psychedelics was shut down for several decades. Now, there are signs that the use of psychedelics in medical and recreational settings is slowly being decriminalized in the United States. We should not miss the opportunity to discuss how to implement these tools for personal and societal development in our culture before the wave of decriminalization reaches Europe. This way, we can try to avoid the lack of harm reduction that hampered the responsible integration of psychedelics into society in previous generations. Perhaps even more importantly, the safe introduction of the psychedelic experience into society will allow us to reconnect toâ€¦ The Ecological Level How are psychedelics and ecological concern connected? Some believe that psychedelic compounds might increase connectedness to nature by dissolving ego boundaries, resulting in the inclusion of nature in self-identification. This effect is described in the following patient report: â€œBefore [the psychedelic experience] I enjoyed nature, now I feel part of it. Before I was looking at it as a thing, like TV or a painting. [But] youâ€™re part of it, thereâ€™s no separation or distinction, you are it.â€ Evidence is growing for the theory that psychedelics increase connectedness to nature. In a small study involving patients with treatment-resistant depression, nature-relatedness and authoritarianism were shown to be increased and decreased, respectively, lasting for up to 12 months after psilocybin administration. Additionally, in a large-scale general population online study, participants reported that their use of psychedelics increased their self-identification with nature, which was in turn associated with pro-environmental behavior. Furthermore, another survey study found that related attitudes and beliefs, like the personality trait of â€œopennessâ€ and liberal political views, were positively associated with the ingestion of psychedelics, although the magnitude of this effect should not be overestimated. It is a matter of ongoing discussion whether the relationship between nature-relatedness and psychedelics is causal or correlational. However, preliminary findings indicate that in addition to the positive effects on healthy individuals, exposure to nature within depression treatment might significantly increase the success rate. A thorough and more in-depth review about psychedelics and nature relatedness can be found elsewhere. Connecting the dots In the increasing number of studies on psychedelics from the last two decades, connection is a recurring theme. Psychedelics facilitate new connections between neurons, which translates into higher connectivity between certain regions in the brain. This effect might represent the core of psychedelicsâ€™ antidepressant effects, and potentially their general therapeutic potential. In Aldous Huxleyâ€™s novel Island, a utopian society builds its ecological beliefs on using psychedelic mushrooms. Likewise, the meaningful and ethically informed use of psychedelics could help people reconnect to their social and ecological environments. If we continue to carefully and critically examine the scientific advances and legislative changes regarding psychedelics, we can edge closer to the vision of the MIND Foundation: to build a healthier, more connected world. The MIND Foundation for Psychedelic Research aims to create a healthier, more connected world through research and education. Learn more or become part of our mission. References appear in the original article.
Emily Mullin 5 days agoÂ·7 min read For decades, scientists have been trying to forge a direct connection from the human brain to external devices to allow people to control machines with their minds. And over the years, thereâ€™s been incremental success. Known as brain-computer interfaces, these devices have allowed paralyzed people to control robotic arms and a mouse on a computer screen just by thinking about it. But these interfaces require clunky setups and trained engineers to supervise their use. They canâ€™t yet be used at home to help people in their everyday lives. This year, neuroscientists got closer to that reality. Here are some of the notable advancements in brain implants we saw in 2020. A startup achieved the largest-ever electrical recording of cortical activity In July, startup Paradromics said in a research paper that it achieved the largest-ever electrical recording of neural activity from the cerebral cortex, the thin protective layer that covers the brain. The Austin, Texas-based company has developed a brain chip with 65,000 electrode channels â€” tiny conductors that carry neural signals into and out of the brain (current brain chips have just hundreds). The company also says its chip has the highest-ever data rate of any neural recording system. In an interview with Future Human, CEO Matt Angle, PhD, said the data rate â€œis about one DVD per second.â€ In sheep, the device allowed scientists to observe brain activity in response to sound stimuli. The paper has yet to be peer-reviewed. Angle thinks his companyâ€™s device is a vast improvement over the Utah array, the brain chip thatâ€™s most commonly used in human clinical trials of brain implants. The Utah array has about 100 pin-like electrode channels or probes that nestle into the brain tissue. But the size of the probes can cause inflammation and tissue scarring around the implant over time, which affects its ability to record neural signals. As a result, these implants only last a few years at most. The Utah array also connects to a heavy metal pedestal that sits atop the head and attaches to a computer via a thick wire. People who have gotten these implants canâ€™t use a brain-computer interface setup at home, only in research labs. By contrast, Paradromicsâ€™ implant contains microwires that are much smaller than the pins of the Utah array, so Angle expects it to last longer and do less damage to brain tissue. The company is developing a wireless device that could be used at home in daily life. Angle says he wants to first test the system in people with paralysis to provide them with a means of fast and reliable communication. The company estimates that around 165,000 people in the United States could benefit from such a system. Neuralink demoed its brain implant in a pig This August, in a much-anticipated live-streamed demonstration that involved three pigs, Elon Musk shared an update on his secretive company Neuralink. To show off the Link device, he brought out three pigs that were Neuralink test subjects. One pig had been implanted with the companyâ€™s newly unveiled brain chip, and a screen showed spikes of activity corresponding to neurons firing in real time. A second pig, which seemed perfectly healthy, previously had a Link but had gotten it removed. Musk said that the pig demonstrated the â€œreversibilityâ€ of the implant. A third pig, which never had an implant, acted as a control. Musk also unveiled the Link itself: a coin-sized implant that fits into the skull and lays against the cortex. â€œItâ€™s like a Fitbit for your skull with tiny wires,â€ he said. Attached to it are 1,024 threadlike, flexible electrodes. The implant is designed to wirelessly transmit these recorded signals to an app that can run on a smart device. During the demo, Musk claimed that the device will be able to do everything from cure deafness to treat mental illness. These applications, however, are far from becoming reality. The companyâ€™s first application of the technology will be to help people with severe spinal cord injury control computers and mobile devices directly with their brains. The company still imagines, though, that the Link could be used for a variety of different purposes depending on its placement in the brain. In the visual and auditory cortices, it could help process vision and sound; in the somatosensory cortex, touch; and in the motor cortex, movement. In a 2019 company update, Musk said the first human subjects could be implanted with the device as early as 2020. That hasnâ€™t happened yet, but during the August demo, he said the company is in talks with the U.S. Food and Drug Administration to begin clinical trials soon. An implant inserted in the vein allows paralyzed people to type with their minds In October, Future Human wrote about a type of brain-computer interface that is inserted and threaded through the jugular vein in the neck instead of implanted directly into the brain. The Stentrode is a flexible mesh tube that resembles a heart stent and is designed to capture nearby brain activity using tiny electrodes on its surface. It sends those brain signals to a second device embedded in the chest, which in turn wirelessly transmits that data to a computer or phone and converts it into computer commands. The company that developed the system, Silicon Valley-based Synchron, has implanted it in two people with paralysis caused by amyotrophic lateral sclerosis, or ALS. After months of learning how to use the system, the two patients were able to do things like send emails, write Word documents, surf the web, and do online banking using just their thoughts. Synchron is hoping to be the first commercially available implantable brain-computer interface. The company thinks its device will be safe since itâ€™s based on heart stent surgery, a common procedure thatâ€™s been performed for decades. Robert Kirsch, PhD, a brain-computer interface expert and chair of biomedical engineering at Case Western Reserve University, told Future Human in October that Synchronâ€™s device is an elegant approach to simple keyboard tasks but may not be able to carry out more complicated tasks since it doesnâ€™t pick up as many brain signals as devices that rest in or on the brain. A brain implant provided artificial vision in monkeys In a step toward restoring vision to blind people, scientists used a brain-stimulating implant in monkeys that allowed them to â€œseeâ€ shapes without the use of their eyes. The high-resolution neuroprosthesis, developed by scientists at the Netherlands Institute for Neuroscience, contains more than 1,000 electrodes and sits on top of the visual cortex â€” the part of the brain that receives and processes information relayed from the retina. The researchers, led by Pieter Roelfsema, PhD, wanted to test whether it was possible to create interpretable images by delivering electrical stimulation through these electrodes. They implanted the device in two macaque monkeys and found that the monkeys could recognize lines, moving dots, and letters using their artificial vision. The findings were published in the journal Science in December. Most efforts to restore vision have focused on stimulating cells in the retina using a pair of glasses outfitted with a camera. The Argus II, made by Sylmar, California-based Second Sight, was the first such device to come onto the market in 2011. But the Argus II and other similar devices are limited in the amount of vision they can restore. They also donâ€™t work for many types of blindness caused by damage to the optic nerve, the main nerve that transmits visual information from the retina to the brain. The implant developed by Dutch researchers interfaces directly with the brain, bypassing damage in the optic nerve. Second Sight is currently conducting human clinical trials of a similar brain implant. Its device, a modified version of the Argus II, sits on the surface of the visual cortex. At least six blind patients have received the experimental device, called the Orion. In November 2019, one of those patients, Jason Esterhuizen, spoke with OneZero about how the device has improved his life.
Jane Garnett, LMFT Dec 17Â·8 min read Twenty minutes late, Matt (whose name was changed for privacy) stumbles into my therapy office, dives onto my green couch, stretches out like a Freudian pro, and buries his face in his hands. Matt, a renowned New York wellness entrepreneur, had found me through the intersection of Burning Man and the plant medicine communities â€” where most of my clients come from. This niche has found me organically and inevitably; Iâ€™ve had a private practice as a marriage and family therapist for 13 years, and for 10 of those, Iâ€™ve been on my own parallel personal journey of exploration both as a burner and a psychonaut (sailor of the mind). â€œStupid. Stupid. Just stupid!â€ Matt groans. â€œWhat?â€ I ask. â€œI came to you to talk about psychedelics, but now you know my dirty secret. I canâ€™t believe I came in today. I havenâ€™t even slept. Fucking cocaine.â€ Matt had come to me to talk about his experience on ibogaine, a natural psychoactive medicine derived from the West African shrub iboga and increasingly used by patients trying to kick drug addictions. It clearly hadnâ€™t worked for him, and it occurs to me that he might still be high on coke. And if heâ€™s high, itâ€™s unethical to treat him. But at this moment, other issues take precedence: the shame spiral heâ€™s in, the fact that weâ€™re already 25 minutes into a 50-minute session, and the reality that I couldnâ€™t move his huge body off my sofa if I tried. Above all, a more universal question is nagging me: As more people turn to psychedelic-aided therapy, why are so many forgoing the necessary follow-up to process these experiences? That follow-up, known as psychedelic integration, is how we metabolize the supranormal phenomena weâ€™ve experienced while tripping and fold it back into â€œnormalâ€ life. Integration takes real time and is likely to be bumpy. Old traumas can suddenly surface hours or days after a trip; we can be suddenly flooded by unmet needs or pain we thought weâ€™d left in the past. The Case for Psychedelics for Depression Is Getting Stronger A day after Oregon legalizes psilocybin, a new study adds to a growing body of literature that psychedelics can beâ€¦ elemental.medium.com This is something I know firsthand. Ayahuasca helped me become a better mother, a better ex-wife, and a better therapist. The â€œmedicineâ€ reflected back to me the power of compassion and how to cultivate it. Iboga, a medicine that originated in Gabon, showed me my own darkness and reconnected me to my creativity. Psilocybin has added depth and perspective to my meditation practice. And Iâ€™m still processing my one and only LSD trip that I had a little over a year ago. I know that without proper resources in place to guide them through the aftermath, people often struggle to reconcile life-changing â€œmedicine experiencesâ€ with their actual lives. That is, a wide gulf lies between what we â€œseeâ€ on psychedelics and what we do with what we saw. Mind the gap, as we say in London, where Iâ€™m from. Psychedelics are proliferating both in and out of therapeutic settings; they have transcended hippiesâ€™ â€œtuning in and dropping outâ€ and become increasingly accepted as mental health treatments. As the Multidisciplinary Association for Psychedelic Studies (MAPS) website puts it, â€œWith both MDMA and psilocybin on the precipice of approvals as mainstream medicines, and several leading universities opening dedicated psychedelic research facilities, the story of the last 10 years has been one of profound breakthrough.â€ Psychedelic wellness solutions are featured in Michael Pollanâ€™s bestseller How to Change Your Mind, Gwyneth Paltrowâ€™s The Goop Lab, and Anderson Cooperâ€™s segment on the John Hopkins study of psilocybin for mental health. The best thing about psychedelics may also be the worst: Psychedelics tear down your defenses fast. A study by Scientific American shows a 223% rise in LSD use among 35- to 39-year-olds between 2015 and 2018. That startling number only proves what Iâ€™m seeing in my own psychotherapy practice in Los Angeles: a striking influx of people who have taken psychedelics for therapeutic purposes. Too many of them, Iâ€™ve found, are seeking help weeks or months or years after life-changing experiences on ayahuasca, psilocybin, or iboga. And Iâ€™m increasingly concerned. So many patients are struggling. A therapeutic trip can trigger a dramatic shift â€” and thatâ€™s the point. People break addictions, heal traumas, and mend relationships immediately after psychedelic experiences. But over time, many become jaded, disappointed, confused, and destabilized. The best thing about psychedelics may also be the worst: Psychedelics tear down your defenses fast. People often finish a trip and leap into life-changing decisions â€” divorcing their spouses, leaving their jobs, or conceiving children. Many people donâ€™t regret those decisions, but inevitably, after time has passed, some do. Could Psychedelics Heal the World? Drug trips, under controlled conditions, break down the barriers between people and bring users closer to nature elemental.medium.com Which brings us back to my patient Matt, the wellness entrepreneur who canâ€™t stop snorting cocaine. â€œWhat did iboga show you about yourself?â€ I ask him. Considered the â€œgrandfather of psychedelicsâ€ (ayahuasca is the â€œgrandmotherâ€), iboga is a traditional West African root bark used in low doses to retain alertness while hunting and in high doses to cause near-death experiences for the purpose of spiritual awakening. Administered legally in sobriety clinics throughout Mexico, iboga is also one of the most powerful addiction interrupters we know of. It successfully got Matt off coke a couple of times, but he avoided the after-care protocols, including therapy and group support, and his abstinence didnâ€™t stick. He had planned to take iboga again, he tells me, except two weeks ago, he discovered heâ€™s a heart attack candidate, so itâ€™s no longer safe. My own 48-hour iboga experience was a mixture of torture and revelation. I relived the shadows of my childhood, hatred, and rage over my parentsâ€™ divorce and my fatherâ€™s addictions â€” all ugly emotions my upbringing had trained me to hide. Iboga revealed a split in me: the part of me that was on board with my life and the part of me that wanted out. It was the most confronting psychedelic experience Iâ€™ve ever had â€” wildly disorienting, exhausting, nauseating, and full of all the traumas I hadnâ€™t been able to look at square in the face despite years of therapy. When it finally ended, I was overwhelmed with gratitude, as if Iâ€™d been given a permission slip to break out of the old family narratives and rewrite my future. We can open our hearts, be in community, and rip the lid off our failing civilization. But then we return home, and the real work begins: We must integrate. Experience has taught me what to do after a trip, and privilege has allowed me to do those things. I have the resources to receive therapy, get a massage, or do a meditation retreat. I have the know-how and education to keep researching. Iâ€™ve gotten to know the cutting-edge thinkers of the psychedelic community through conferences, networking, and reading (not that any of that guarantees a lack of turbulence). Even among those of us who have the material resources and time to process our experiences, many of us donâ€™t have the communal models of indigenous communities. In plant medicine circles in the U.S., in contrived ceremonial settings, we may get a glimpse of what life could look like. We can open our hearts, be in community, and rip the lid off our failing civilization. But then we return home, and the real work begins: We must integrate. Matt turns to me, his face locked in unexpressed emotion. I know that look: the dam before it bursts. Then he starts sobbing so hard, he canâ€™t speak and canâ€™t stop. He covers his heart with his hand, his whole body shaking. â€œIt showed me this,â€ he says. And for a minute, I get to see it: the excruciating vulnerability that is so often at the center of personal chaos. Matt has a vulnerable heart, figuratively and literally under threat of attack. Iboga has helped expose this reality, but what should he do about it? I outline a treatment plan for Matt that doesnâ€™t involve iboga. To quit cocaine and find more healthy coping mechanisms, he would most likely need to enter a sobriety program or curate a team of mental health professionals to help him stay clean. Most challengingly, Matt would need to commit to sobriety. Even then, there would be no guarantee of success. Psychedelics are immensely helpful in getting under our defenses and providing a simplicity of vision that adults canâ€™t normally access. In this sense, they are more powerful than the greatest therapists. But when they wear off, thereâ€™s not only whatâ€™s been exposed but the rest of life to deal with also: the to-do lists and taxes and physical needs. As Buddhist practitioner and mindfulness expert Jack Kornfield put it in the title of his book: â€œAfter the ecstasy, the laundry.â€ Or, in Mattâ€™s case, rehab. For anyone new to psychedelic healing, my advice is to plan for integration before even starting the journey. Iâ€™d recommend thinking carefully through three categories: set, setting, and support. Set: Mindset is all-important. Having a clear and positive intention can be very helpful. It makes you more of an empowered participant in your journey, inviting in more purpose as well as a sense of a home base to return to â€” which could be a mantra of some kind or a question that you lead with. Music also makes a huge difference to a journey. If you are selecting the music yourself, choose tunes that speak to your heart and relax your nervous system. Setting: Who you have your experience with and where is important. Pick a location that feels safe and quiet and that, ideally, you have a sense of connection to. If youâ€™re sensitive to other peopleâ€™s energy, do not do psychedelics in large groups. Being in or near nature can feel very supportive even if it just means being close to a plant or a tree. If youâ€™re journeying away from home, make sure you have a solid plan of how to get there and back. (Needless to say, it should not involve you driving.) Support: Research the shaman or therapist who will act as your guide. Make sure thereâ€™s someone else youâ€™re checking in with on either side of your experience â€” a good friend or a wisdom figure in your life. Itâ€™s always valuable to have another perspective and not make your shaman or therapist your only source of feedback and or authority. Make sure you have time carved out on the back end of a journey to go slow and talk with a counselor if you can. Journaling, meditation, and spending time in nature all help with processing psychedelic downloads as well as allowing new neural pathways to be reinforced. A trip is like any voyage: It requires preparation, time for proper digestion. For further information and support, InnerSpace Integration hosts a network of resources along with psychedelic integration circles. Tam Integration offers a collection of trip sitter manuals and guides. For those who want to go deeper, Psychonautdocs.com has curated a wealth of essays and studies on a range of psychedelics. Most importantly, hold off on big decisions after life-altering experiences. A trip is like any voyage: It requires preparation, time for proper digestion, and a willingness to not only surrender to the experience but to allow the necessary time to change your life.
Ashley Laderer 6 days agoÂ·5 min read  Are SAD lamps for everyone? So, should you buy a SAD lamp this winter? It goes without saying that these next few months are going to be rough. Covid-19 cases are rising to record highs, weâ€™re running on months and months of social isolation, and new lockdowns and restrictions are sweeping across the nation. Add in the already daunting shorter daylight hours, colder temperatures in many parts of the country, and lonely holidays, and we have ourselves a recipe for a doozy of a winter, including winter blues that are sure to cause mental health to take a hit. So, how can one find relief? Could artificial light in the form of SAD lamps â€” lights that are often used to treat people who have seasonal affective disorder â€” provide a glimmer of hope for everyone this winter? Maybe. But first, letâ€™s look at the basics of SAD and SAD lamps. SAD, sometimes referred to as seasonal depression, is more than just the winter blues. Someone must meet the criteria for a depressive disorder, including their functioning being impaired, to be diagnosed with SAD, according to Lindsay R. Standeven, MD, assistant professor of psychiatry and behavioral sciences at Johns Hopkins School of Medicine. SAD Is Serious Seasonal affective disorder is more than just the winter blues elemental.medium.com These depressive symptoms in the winter months can include feeling sad most days, not enjoying things you usually enjoy, and feeling irritable, Standeven says. Additionally, unique features of SAD include increased appetite, particularly carbohydrate cravings, and â€œprofound fatigue.â€ Experts believe SAD is caused by the reduction in daylight hours, which throws off hormones like melatonin and serotonin, contributing to the change in energy levels and mood. Enter SAD lamps, readily available to buy on the internet (no prescription required) and used to treat the condition during the shorter days throughout winter, often in conjunction with therapy and medication. How exactly do these lamps, often marketed as â€œhappy lights,â€ work? The mechanism goes beyond just staring at a light and feeling better. (In fact, you shouldnâ€™t stare directly at these lights.) There are cells in the retina that contain a pigment called melanopsin, according to Paul Desan, MD, PhD, a psychiatrist at Yale Medicine and assistant professor of psychiatry at Yale School of Medicine. These are special brightness-detecting cells that project and communicate directly with the hypothalamus and other parts of the brain that affect your hormones, energy, and emotions. â€œIn most cases, the lamps arenâ€™t going to hurt, but if you donâ€™t really have seasonal affective disorder, theyâ€™re not necessarily going to help.â€ SAD lamps with a brightness of 10,000 lux (typical room lights are between 50 and 200 lux) can stimulate these cells and affect hormones in a positive way. Doctors recommend that people with SAD use the lamps as early as possible in the morning for about 30 minutes. You sit about 12 to 18 inches from the light and go about your morning routine â€” reading the news, having your coffee, and checking emails. Results are seen relatively quickly when using SAD lamps. Standeven says people will typically see improvement in mood within a couple weeks. Research shows that even just one week of light therapy is enough to make a difference. Are SAD lamps for everyone? Even though SAD lamps are traditionally for those with diagnosed seasonal depression, Desan suggests that under the right circumstances, they could also benefit people who are â€œjustâ€ experiencing the winter blues â€” especially during this pandemic winter, when they will be indoors more, sleeping in later, and exposed to much less natural light. If you work from home, for example, youâ€™re at risk of being exposed to even less natural sunlight. â€œUsually people drive to work at 7:30 or 8 a.m., and theyâ€™re exposing themselves to a piece of bright daylight. But if instead you just sleep in until you get online for your first meeting of the day at 9 a.m., youâ€™re going to be sending a different signal to your body,â€ Desan says. Every Covid-19 Vaccine Question Youâ€™ll Ever Have, Answered Clear guidance on everything you want to know about the vaccine (and then some) elemental.medium.com Using a SAD lamp first thing in the morning for 30 minutes can counteract this and signal your body to wake up, which may improve fatigue and your mood throughout the day. However, not every expert agrees that these lamps are for everyone. While thereâ€™s good evidence that light therapy is effective for people with SAD, the lamps may have no or very little effect on people who donâ€™t suffer from seasonal affective disorder, according to Standeven. A 2020 meta-analysis of 19 studies found that light therapy is more effective than placebo in reducing depression symptoms in people with SAD. However, thereâ€™s no sufficient evidence to conclude if they are also effective for those without SAD who are dealing with the winter blues. Standeven recommends other coping tips to ward off the winter blues this year, such as braving the cold and getting out for a walk to expose yourself to natural light. â€œThereâ€™s no reason you canâ€™t be out walking and getting some exercise in whatever weather it is,â€ she says. The combination of light and exercise will give you a mood and energy boost. Exercise in general is helpful as well. Standeven also urges people to stay social virtually, such as by Zooming with friends and family, to help combat the social isolation weâ€™ll be dealing with for even longer. Activating the Vagus Nerve Might Lower Your Covid-19 Risk While physical distancing and masks are crucial, social interaction could calm the immune system and turn downâ€¦ elemental.medium.com So, should you buy a SAD lamp this winter? This shouldnâ€™t be the only question youâ€™re asking yourself. First, take a step back and recognize that if youâ€™re feeling poorly and considering getting a light that promises to make you feel better, it might be time to reevaluate your mental health this winter. If you decide to try out a SAD lamp and use it correctly, it isnâ€™t going to cause harm, but the lamp also wonâ€™t make your winter blues magically disappear. â€œIn most cases, the lamps arenâ€™t going to hurt, but if you donâ€™t really have seasonal affective disorder, theyâ€™re not necessarily going to help,â€ Standeven says, particularly if used as the only tool to help you feel better. Itâ€™s more important and effective to go back to the basics: getting enough sleep, exercising, and eating a balanced diet. One important caveat: If youâ€™re not feeling well mentally this winter, itâ€™s possible that you actually have SAD â€” especially if low motivation or low energy are getting in the way of your daily functioning. â€œNobody should be treating their depression on their own,â€ Desan says, and if youâ€™re not sure if your winter blues are actually seasonal affective disorder, you should be evaluated. The bottom line: Under the right circumstances, a SAD lamp can make your winter look just a little brighter â€” but donâ€™t expect it to be a miraculous light switch for your mental health.
Lazarina Stoy Dec 16Â·5 min read Okay, okay. Even the premise of this article might ruffle some feathers. But there is a study that reports that dog owners are much happier in comparison to cat owners, or any other type of pet owners. This was possible as a result of the General Social Survey, which in 2018, for the first time quantified matters related to pet ownership. One of which being happiness. Dog owners are about twice as likely as cat owners to say theyâ€™re very happy, with people owning both falling somewhere in between. Dog people, in other words, are slightly happier than those without any pets. Those in the cat camp, on the other hand, are significantly less happy than the pet-less. As a true cynic, I was close to discarding this one, but I looked a bit deeper, hoping to see there is no validation of this data. Sadly, though, I was proved wrong. Another study from 2016 on dog and cat owners, affirmed the yielded greater happiness ratings for dog owners relative to cat people, presented early in this story. But this time, the authors attributed the differences to differences in personality, claiming that dog owners tended to be more agreeable, more extroverted, and less neurotic than cat owners. While this may appear as interesting, perhaps even a bit controversial, breakthroughs in neuroscience might offer a potential explanation of why this might be the case. While, as I cat owner myself, I feel the urge to protest against this very studyâ€¦ Instead wishing to propose the argument that cats are just misunderstood, equally affectionate as dogs, and so onâ€¦ As a researcher, I am obliged to propose a hypothesis that has little to do with personal affection and more to do with habits. After researching the work of Dr. Andrew Huberman, I am convinced that the habits that dog owners form when taking care of their pets set them on a path similar to the one challenging symptoms of depression, thus: making them better capable to handle mild depression (if they sustain those habits) making them less likely to experience severe depression; and enhancing their self-reported happiness. Before I begin there are two disclaimers I want to make: Neuroscience on the relationship between brain and body, and how it affects our perception of depression Throughout his research on neuroscience, Dr. Andrew Huberman and his lab have made some remarkable discoveries on how the brain works. He has also made an effort in educating the public about his work. One such instance is his appearance as a guest on a podcast with Lewis Howes on YouTube. In this specific episode, he talked about the relationship between the mind and the body, as well as how influencing the perceptions in the body, influences the mind, and vice-versa. There he was asked by the host: Can a person make it, so they are never get depressed? â€¦ Is there a way that we can ever defend ourselves against negative stressors and emotions? His answer drew on highly important concepts: These things are incredibly important because if someone falls beyond their baseline, it is much harder to introduce tools that can assist them to get out of a depressive state. As a result, because the brain controls and body and the body controls the brain, people lose the ability to intervene should such tools are not introduced earlier. What are these tools? What are these tools, you might ask? Well, they are simple, yet powerful habits, proven to make a dent in the perception of the brain of depressive states, such as: Getting up early Getting some sunlight early in the day Exercising, or simply moving These, as Dr. Andrew Huberman says, are tools grounded in excellent science. For instance, Dopamine is released early in the day, which is anti-depressive When you get up early, receive sunlight and get some movement throughout the day, you time your sleep better If the brain-body relationship is caught early and built in the form of habits and routines (and before a potential depressive state occurs), the same will allow a person to intervene more effectively, should such a state occur. As the scientist says: We can stay out of depression. But we have to keep depression at bay by doing things regularly. How does this relate to dog owners? Well, owning a dog pretty much takes care of the three things Dr. Huberman described there, doesnâ€™t it? On getting up early, dog owners have a visual motivation and trigger of getting up early every morning. This also takes care of the â€˜receiving sunlight early in the dayâ€™ aspect of the routine. On exercising, dog owners typically walk an average of 300 minutes per week, compared to 168 minutes for people who donâ€™t have dogs, according to the ASPCA. That is about 43 minutes per day in light-to-moderate exercise. Is it possible that that is the reason behind dog ownersâ€™ self-reported happiness? Could routines and habits related to taking care of their dog evoke mechanisms that better enable them to combat symptoms of depression? Iâ€™d like to believe it could be, based on the insights I got from Dr. Huberman, and neuroscience research on the topic of the brain and body and how our routines can set us on a better mental health path. While those habits can be incorporated into the routines of cat owners, too, only the luckiest of them will be able to share the time for self-care with their furry friends. Maybe that is where true happiness lies â€” in the mutual feeling of care for one another through shared rituals between the dog owner and the dog.
Dec 13Â·7 min read A splitting headache that continues non-stop. A weakness in the muscles and being unable to walk a small distance. Feeling abnormally sleepy at random periods of the day. For many individuals with glioblastoma (a type of brain tumor), these symptoms were generally dismissed as the body feeling generally unwell. Even when non-cancer patients think of these symptoms, a condition as deadly as glioma would not be the first thing to come to mind. Unfortunately, these subtle symptoms can transform into a rapidly-progressing disease with minimal chance of survival. 52% of all primary brain tumors are glioblastoma, and 17% of all tumors in the brain are a glioblastoma (whether the tumor is primary or metastatic doesnâ€™t matter). Despite the tumor more commonly occurring in individuals between ages 45â€“70, the median survival rate for brain cancers is around age 64, which is devastating. GBM is evidently becoming a larger problem, and research is imperative to reduce the progression of this disease. As scary as this sounds, recent breakthroughs are beginning to change the way we look at glioblastoma. Interested to learn more? Letâ€™s take a look at this seemingly deadly cancer and recent studies about it. A Brief Introduction to the Brain To understand the nature of glioblastoma, we require a general understanding of the different regions in the brain. Despite the brain being known for being overly complex, the two most important sections to note are the cerebrum and the cerebellum. Cerebrum The cerebrum refers to the anterior part of the brain thatâ€™s located towards the front of the skull, and is much larger compared to the cerebellum. The cerebrum can be divided into many subsections with varied function, but the purpose of the cerebrum includes recognizing external surroundings, problem-solving, and coordinating movement. With regards to a brain tumor, most cases (regardless of whether itâ€™s benign or malignant) will occur in the cerebrum. Tumors in the cerebrum are also referred to as supratentorial since they are present above the tentorium, which is a fissure that divides the cerebrum from the cerebellum. The cerebrum also contains ventricles with cerebrospinal fluid, which is responsible for protecting the brain while providing bouyancy. Cerebellum While the cerebrum is above the tentorial line, the cerebellum is below this fissure and also neighbors the brain stem. While the cerebrum is split into several sections, the cerebellum has two important subsets to note: the cerebellar cortex and cerebellar nuclei. The cerebellar cortex consists of three layers: The outermost layer is made of the axons and dendrites of cerebellar neurons, indicating it contains a majority of the cerebellumâ€™s neurons. The middle layer contains Purkinje cells, which are a type of flask-shaped neuron native to the cerebellum. The primary function of these cells is the emission of GABA (gamma-aminobutyric acid), a neurotransmitter that regulates nerve impulses throughout the brain and controls motor function. The innermost layer has granular cells, which are the smallest type of neuron in the brain. These cells send impulses to the Purkinje cells in the cerebellum, which move into the outermost molecular layer throughout the cerebellar cortex. These granule-cell â€” Purkinje-cell synapses are super important for the brain, and they use glutamate as a neurotransmitter. On the other hand, the cerebellar nuclei is the innermost section of white matter of the cerebellum. This subsection will receive inhibitory signals from Purkinje cells (usually from the cerebellar cortex) and is responsible for communicating such signals throughout the brain. Within this inner white matter, the four types of deep cerebellar nuclei are: Dentate Emboliform Globase Fastigii Tumors are rarely found in the cerebellum, but these infratentorial tumors (since the cerebellum is below the tentorium) can also be located in the brain stem and other neighboring structures. Where and How Glioblastoma Starts Now that we have a general understanding of the brainâ€™s anatomy, letâ€™s look at the nature of a glioblastoma. More often than not, GBMs are supratentorial tumors and are also a type of astrocytoma, which is a brain tumor originating from astrocyte cells in the brain. In the central nervous system (CNS), astrocytes are a type of glial cell that perform a number of functions throughout the brain and spinal cord. Some of their key purposes are supporting synapses (which relay electrical signals between neurons), controlling the blood-brain barrier, and regulating blood flow throughout the brain. Depending on the type of astrocyte, they will either be located throughout the CNS or primarily in white matter, a characteristic of more fibrous glia. Though astrocytes are the primary cause of glioblastoma, the reason behind their sudden abnormality remains a mystery. Scientists have discovered that a number of mutations within astrocytes have likely caused the development of cancerous cells in the brain, but the cause of these mutations isnâ€™t well-known either. In 2018, researchers discovered that a mutation in the most commonly-used protein in astrocytes led to consequences that were shockingly similar to that of a brain tumor. The protein, GFAP (glial fibrillary acidic protein), is a type of cytoskeleton protein that allows for astrocytes to remain in a star-shaped structure. The leader of this study and a neuroscience professor at the University of Wisconsin-Madison, Su-Chun Zhang, led this study and began her research by growing adult astrocytes from patients with extensive neurodegeneration and converting them into stem cells. When observed more closely, the astrocytes contained excessive tangles of the GFAP protein and seemed to be taking a toll on these cellsâ€™ functionality. When the converted stem cells experienced gene editing to correct the incorrectly folded GFAP protein, the cells were good as new! This discovery is astronomical for determining the causes of neurodegeneration brought on by cancerous cells, and gives us significant insight on potentially stopping this rapidly-progressing malignancy. Recent Breakthroughs in Glioblastoma Research In a world of rapidly-progressing technology, there have been several breakthroughs in glioblastoma research and working towards stopping it altogether. Here are some instances of notable research conducted by scientists on brain tumors: In July 2020, a recent study was published on improving the current radiation therapy used for glioblastoma. Instead of directly targeting the tumor (which is the norm for most radiation therapy), the researchers in this study wanted to target a specific metabolic pathway that worked towards repairing damaged DNA. The senior study author, Daniel Wahl, M.D., PhD., explains that preventing the repair of DNA after radiation therapy will inhibit the cancerous cells from returning since the genetic information to generate these cancerous cells will be entirely destroyed by attacking a specific metabolic pathway. In March 2019, research revealed that instead of targeting the glioblastoma cells in a brain tumor, drug options should be attacking stem cells that divide relatively slowly to eliminate further progression of the cancer. Researchers believed that while radiation therapy and chemotherapy are effective at stopping the rapidly-multiplying cancerous astrocytes, the glioblastoma stem cells are the real culprits that are constantly allowing this malignant brain tumor to return immediately. The study eventually concluded that the use of a synthetic drug Gboxin was the most effective at reducing the accumulation of tumors in the brains of mice. In June 2019, a team of researchers found a number of previously undiscovered biomarkers in glioblastoma using the Cancer Genome Atlas. The discovered biomarkers are revolutionary to personalizing treatment options for patients since they determine which types of patients will heal fastest from chemotherapy or radiation therapy. To look for these biomarkers, research was started by generating patient-derived tumors in mice and providing different treatment options to these animals. Based on the results and general health of these mice, the scientists looked at gene transcripts to determine which genes were allowing mice to experience positive effects associated with their treatment. Key Takeaways Glioblastoma is a malignant brain tumor and is considered one of the deadliest brain cancers. This condition originates from mutated astrocytes found throughout the central nervous system, which begin growing at an uncontrollable rate. In the brain, the two critical sections to note are the cerebrum and cerebellum. While tumors in the cerebrum are called supratentorial, tumors in the cerebellum are referred to as inratentorial. Glioblastoma is allegedly caused by the misfolding of the GFAP protein in astrocytes, which is responsible for their structure. A single mutation in this protein causes astrocytes to exhibit behavior similar to that of cancerous cells. There is a lot of notable research in finding treatment for glioblastoma, most of which concerns changing current treatment methods.
Tara Haelle Aug 17Â·13 min read  An unprecedented disaster Understanding ambiguous loss A winding, uncharted path to coping in a pandemic It was the end of the world as we knew it, and I felt fine. Thatâ€™s almost exactly what I told my psychiatrist at my March 16 appointment, a few days after our childrenâ€™s school district extended spring break because of the coronavirus. I said the same at my April 27 appointment, several weeks after our stateâ€™s stay-at-home order. Yes, it was exhausting having a kindergartener and fourth grader doing impromptu distance learning while I was barely keeping up with work. And it was frustrating to be stuck home nonstop, scrambling to get in grocery delivery orders before slots filled up, and tracking down toilet paper. But I was still doing well because I thrive in high-stress emergency situations. Itâ€™s exhilarating for my ADHD brain. As just one example, when my husband and I were stranded in Peru during an 8.0-magnitude earthquake that killed thousands, we walked around with a first aid kit helping who we could and tracking down water and food. Then I went out with my camera to document the devastation as a photojournalist and interview Peruvians in my broken Spanish for my hometown paper. Now we were in a pandemic, and Iâ€™m a science journalist who has written about infectious disease and medical research for nearly a decade. I was on fire, cranking out stories, explaining epidemiological concepts in my social networks, trying to help everyone around me make sense of the frightening circumstances of a pandemic and the anxiety surrounding the virus. I knew it wouldnâ€™t last. It never does. But even knowing I would eventually crash, I didnâ€™t appreciate how hard the crash would be, or how long it would last, or how hard it would be to try to get back up over and over again, or what getting up even looked like. How to Live When Your Mind Is Governed by Fear Psychiatrist and habit change specialist Dr. Jud Brewer explains how anxiety masquerades as helpful elemental.medium.com In those early months, I, along with most of the rest of the country, was using â€œsurge capacityâ€ to operate, as Ann Masten, PhD, a psychologist and professor of child development at the University of Minnesota, calls it. Surge capacity is a collection of adaptive systems â€” mental and physical â€” that humans draw on for short-term survival in acutely stressful situations, such as natural disasters. But natural disasters occur over a short period, even if recovery is long. Pandemics are different â€” the disaster itself stretches out indefinitely. â€œThe pandemic has demonstrated both what we can do with surge capacity and the limits of surge capacity,â€ says Masten. When itâ€™s depleted, it has to be renewed. But what happens when you struggle to renew it because the emergency phase has now become chronic? By my May 26 psychiatrist appointment, I wasnâ€™t doing so hot. I couldnâ€™t get any work done. Iâ€™d grown sick of Zoom meetups. It was exhausting and impossible to think with the kids around all day. I felt trapped in a home that felt as much a prison as a haven. I tried to conjure the motivation to check email, outline a story, or review interview notes, but I couldnâ€™t focus. I couldnâ€™t make myself do anything â€” work, housework, exercise, play with the kids â€” for that whole week. Or the next. Or the next. Or the next. I know depression, but this wasnâ€™t quite that. It was, as Iâ€™d soon describe in an emotional post in a social media group of professional colleagues, an â€œanxiety-tainted depression mixed with ennui that I canâ€™t kick,â€ along with a complete inability to concentrate. I spoke with my therapist, tweaked medication dosages, went outside daily for fresh air and sunlight, tried to force myself to do some physical activity, and even gave myself permission to mope for a few weeks. We were in a pandemic, after all, and I had already accepted in March that life would not be â€œnormalâ€ for at least a year or two. But I still couldnâ€™t work, couldnâ€™t focus, hadnâ€™t adjusted. Shouldnâ€™t I be used to this by now? â€œWhy do you think you should be used to this by now? Weâ€™re all beginners at this,â€ Masten told me. â€œThis is a once in a lifetime experience. Itâ€™s expecting a lot to think weâ€™d be managing this really well.â€ It wasnâ€™t until my social media post elicited similar responses from dozens of high-achieving, competent, impressive women I professionally admire that I realized I wasnâ€™t in the minority. My experience was a universal and deeply human one. An unprecedented disaster While the phrase â€œadjusting to the new normalâ€ has been repeated endlessly since March, itâ€™s easier said than done. How do you adjust to an ever-changing situation where the â€œnew normalâ€ is indefinite uncertainty? â€œThis is an unprecedented disaster for most of us that is profound in its impact on our daily lives,â€ says Masten. But itâ€™s different from a hurricane or tornado where you can look outside and see the damage. The destruction is, for most people, invisible and ongoing. So many systems arenâ€™t working as they normally do right now, which means radical shifts in work, school, and home life that almost none of us have experience with. Even those who have worked in disaster recovery or served in the military are facing a different kind of uncertainty right now. Life Is Now a Game of Risk. Hereâ€™s How Your Brain Is Processing It. Americans are faced with more risk than ever. Understanding how the brain navigates this new reality can buildâ€¦ elemental.medium.com â€œI think we maybe underestimate how severe the adversity is and that people may be experiencing a normal reaction to a pretty severe and ongoing, unfolding, cascading disaster,â€ Masten says. â€œItâ€™s important to recognize that itâ€™s normal in a situation of great uncertainty and chronic stress to get exhausted and to feel ups and downs, to feel like youâ€™re depleted or experience periods of burnout.â€ Research on disaster and trauma focuses primarily on whatâ€™s helpful for people during the recovery period, but weâ€™re not close to recovery yet. People can use their surge capacity for acute periods, but when dire circumstances drag on, Masten says, â€œyou have to adopt a different style of coping.â€ â€œHow do you adjust to an ever-changing situation where the â€˜new normalâ€™ is indefinite uncertainty?â€ Understanding ambiguous loss Itâ€™s not surprising that, as a lifelong overachiever, Iâ€™ve felt particularly despondent and adrift as the months have dragged on, says Pauline Boss, PhD, a family therapist and professor emeritus of social sciences at the University of Minnesota who specializes in â€œambiguous loss.â€ â€œItâ€™s harder for high achievers,â€ she says. â€œThe more accustomed you are to solving problems, to getting things done, to having a routine, the harder it will be on you because none of that is possible right now. You get feelings of hopelessness and helplessness, and those arenâ€™t good.â€ Thatâ€™s similar to how Michael Maddaus, MD, a professor of thoracic surgery at the University of Minnesota, felt when he became addicted to prescription narcotics after undergoing several surgeries. Now recovered and a motivational speaker who promotes the idea of a â€œresilience bank account,â€ Maddaus had always been a fast-moving high achiever â€” until he couldnâ€™t be. â€œI realized that my personal operating system, though it had led to tremendous success, had failed me on a more personal level,â€ he says. â€œI had to figure out a different way of contending with life.â€ That mindset is an especially American one, Boss says. â€œOur culture is very solution-oriented, which is a good way of thinking for many things,â€ she says. â€œItâ€™s partly responsible for getting a man on the moon and a rover on Mars and all the things weâ€™ve done in this country that are wonderful. But itâ€™s a very destructive way of thinking when youâ€™re faced with a problem that has no solution, at least for a while.â€ That means reckoning with whatâ€™s called ambiguous loss: any loss thatâ€™s unclear and lacks a resolution. It can be physical, such as a missing person or the loss of a limb or organ, or psychological, such as a family member with dementia or a serious addiction. â€œIn this case, it is a loss of a way of life, of the ability to meet up with your friends and extended family,â€ Boss says. â€œIt is perhaps a loss of trust in our government. Itâ€™s the loss of our freedom to move about in our daily life as we used to.â€ Itâ€™s also the loss of high-quality education, or the overall educational experience weâ€™re used to, given school closures, modified openings and virtual schooling. Itâ€™s the loss of rituals, such weddings, graduations, and funerals, and even lesser â€œrituals,â€ such as going to gym. One of the toughest losses for me to adapt to is no longer doing my research and writing in coffee shops as Iâ€™ve done for most of my life, dating back to junior high. â€œThese were all things we were attached to and fond of, and theyâ€™re gone right now, so the loss is ambiguous. Itâ€™s not a death, but itâ€™s a major, major loss,â€ says Boss. â€œWhat we used to have has been taken away from us.â€ Just as painful are losses that may result from the intersection of the pandemic and the already tense political division in the country. For many people, issues related to Covid-19 have become the last straw in ending relationships, whether itâ€™s a family member refusing to wear a mask, a friend promoting the latest conspiracy theory, or a co-worker insisting Covid-19 deaths are exaggerated. Ambiguous loss elicits the same experiences of grief as a more tangible loss â€” denial, anger, bargaining, depression, and acceptance â€” but managing it often requires a bit of creativity. A winding, uncharted path to coping in a pandemic While there isnâ€™t a handbook for functioning during a pandemic, Masten, Boss, and Maddaus offered some wisdom for meandering our way through this. Accept that life is different right now Maddausâ€™ approach involves radical acceptance. â€œItâ€™s a shitty time, itâ€™s hard,â€ he says. â€œYou have to accept that in your bones and be okay with this as a tough day, with â€˜thatâ€™s the way it is,â€™ and accept that as a baseline.â€ But that acceptance doesnâ€™t mean giving up, he says. It means not resisting or fighting reality so that you can apply your energy elsewhere. â€œIt allows you to step into a more spacious mental space that allows you to do things that are constructive instead of being mired in a state of psychological self torment.â€ Expect less from yourself Most of us have heard for most of our lives to expect more from ourselves in some way or another. Now we must give ourselves permission to do the opposite. â€œWe have to expect less of ourselves, and we have to replenish more,â€ Masten says. â€œI think weâ€™re in a period of a lot of self discovery: Where do I get my energy? What kind of down time do I need? Thatâ€™s all shifted right now, and it may take some reflection and self discovery to find out what rhythms of life do I need right now?â€ She says people are having to live their lives without the support of so many systems that have partly or fully broken down, whether itâ€™s schools, hospitals, churches, family support, or other systems that we relied on. We need to recognize that weâ€™re grieving multiple losses while managing the ongoing impact of trauma and uncertainty. The malaise so many of us feel, a sort of disinterested boredom, is common in research on burnout, Masten says. But other emotions accompany it: disappointment, anger, grief, sadness, exhaustion, stress, fear, anxiety â€” and no one can function at full capacity with all that going on. Recognize the different aspects of grief The familiar â€œstagesâ€ of grief donâ€™t actually occur in linear stages, Boss says, but denial, anger, bargaining, depression, and acceptance are all major concepts in facing loss. Plenty of people are in denial: denying the virus is real, or that the numbers of cases or deaths are as high as reported, or that masks really help reduce disease transmission. Anger is evident everywhere: anger at those in denial, anger in the race demonstrations, anger at those not physically distancing or wearing masks, and even anger at those who wear masks or require them. The bargaining, Boss says, is mostly with scientists we hope will develop a vaccine quickly. The depression is obvious, but acceptanceâ€¦ â€œI havenâ€™t accepted any of this,â€ Boss says. â€œI donâ€™t know about you.â€ Sometimes acceptance means â€œsaying weâ€™re going to have a good time in spite of this,â€ Boss says, such as when my family drove an hour outside the city to get far enough from light pollution to look for the comet NEOWISE. But it can also mean accepting that we cannot change the situation right now. â€œWe can kick and scream and be angry, or we can feel the other side of it, with no motivation, difficulty focusing, lethargy,â€ Boss says, â€œor we can take the middle way and just have a couple days where you feel like doing nothing and you embrace the losses and sadness youâ€™re feeling right now, and then the next day, do something that has an element of achievement to it.â€ â€œOur new normal is always feeling a little off balance, like trying to stand in a dinghy on rough seas, and not knowing when the storm will pass.â€ Experiment with â€œboth-andâ€ thinking This approach may not work for everyone, but Boss says thereâ€™s an alternative to binary thinking that many people find helpful in dealing with ambiguous loss. She calls it â€œboth-andâ€ thinking, and sometimes it means embracing a bit of the irrational. For the families of soldiers missing in action in Vietnam that Boss studied early in her career, or the family members of victims of plane crashes where the bodies arenâ€™t recovered, this type of thinking means thinking: â€œHe is both living and maybe not. She is probably dead but maybe not.â€ â€œIf you stay in the rational when nothing else is rational, like right now, then youâ€™ll just stress yourself more,â€ she says. â€œWhat I say with ambiguous loss is the situation is crazy, not the person. The situation is pathological, not the person.â€ An analogous approach during the pandemic might be, â€œThis is terrible and many people are dying, and this is also a time for our families to come closer together,â€ Boss says. On a more personal level, â€œIâ€™m highly competent, and right now Iâ€™m flowing with the tide day-to-day.â€ Itâ€™s a bit of a SchrÃ¶dingerâ€™s existence, but when you canâ€™t change the situation, â€œthe only thing you can change is your perception of it,â€ she says. Of course, that doesnâ€™t mean denying the existence of the pandemic or the coronavirus. As Maddaus says, â€œYou have to face reality.â€ But how we frame that reality mentally can help us cope with it. Look for activities, new and old, that continue to fulfill you Lots of coping advice has focused on â€œself-care,â€ but one of the frustrating ironies of the pandemic is that so many of our self-care activities have also been taken away: pedicures, massages, coffee with friends, a visit to the amusement park, a kickboxing class, swimming in the local pool â€” these activities remain unsafe in much of the country. So we have to get creative with self-care when weâ€™re least motivated to get creative. â€œWhen weâ€™re forced to rethink our options and broaden out what we think of as self-care, sometimes that constraint opens new ways of living and thinking,â€ Masten says. â€œWe donâ€™t have a lot of control over the global pandemic but we do over our daily lives. You can focus on plans for the future and whatâ€™s meaningful in life.â€ For me, since I missed eating in restaurants and was tired of our same old dinners, I began subscribing to a meal-kit service. I hate cooking, but the meal kits were easy, and I was motivated by the chance to eat something that tasted more like what Iâ€™d order in a restaurant without having to invest energy in looking through recipes or ordering the right ingredients. Okay, Iâ€™ve also been playing a lot of Animal Crossing, but Maddaus explains why it makes sense that creative activities like cooking, gardening, painting, house projects â€” or even building your own imaginary island out of pixels â€” can be fulfilling right now. He references the book The Molecule of More, which explores how dopamine influences our experiences and happiness, in describing the types of activities most likely to bring us joy. â€œThere are two ways the brain deals with the world: the future and things we need to go after, and the here and now, seeing things and touching things,â€ Maddaus says. â€œRather than being at the mercy of whatâ€™s going on, we can use the elements of our natural reward system and construct things to do that are good no matter what.â€ Those kinds of activities have a planning element and a here-and-now experience element. For Maddaus, for example, it was simply replacing all the showerheads and lightbulbs in the house. â€œItâ€™s a silly thing, but it made me feel good,â€ he says. Focus on maintaining and strengthening important relationships The biggest protective factors for facing adversity and building resilience are social support and remaining connected to people, Masten says. That includes helping others, even when weâ€™re feeling depleted ourselves. â€œHelping others is one of those win-win strategies of taking action because weâ€™re all feeling a sense of helplessness and loss of control about whatâ€™s going on with this pandemic, but when you take action with other people, you can control what youâ€™re doing,â€ she says. Helping others could include checking in on family friends or buying groceries for an elderly neighbor. Begin slowly building your resilience bank account Maddausâ€™ idea of a resilience bank account is gradually building into your life regular practices that promote resilience and provide a fallback when life gets tough. Though it would obviously be nice to have a fat account already, he says itâ€™s never too late to start. The areas he specifically advocates focusing on are sleep, nutrition, exercise, meditation, self-compassion, gratitude, connection, and saying no. â€œStart really small and work your way up,â€ he says. â€œIf you do a little bit every day, it starts to add up and you get momentum, and even if you miss a day, then start again. We have to be gentle with ourselves and keep on, begin again.â€ After spending an hour on the phone with each of these experts, I felt refreshed and inspired. I can do this! I was excited about writing this article and sharing what Iâ€™d learned. And then it took me two weeks to start the article and another week to finish it â€” even though I wanted to write it. But now, I could cut myself a little more slack for taking so much longer than I might have a few months ago. I might have intellectually accepted back in March that the next two years (or more?) are going to be nothing like normal, and not even predictable in how they wonâ€™t be normal. But cognitively recognizing and accepting that fact and emotionally incorporating that reality into everyday life arenâ€™t the same. Our new normal is always feeling a little off balance, like trying to stand in a dinghy on rough seas, and not knowing when the storm will pass. But humans can get better at anything with practice, so at least I now have some ideas for working on my sea legs. This story has been translated into Portuguese, which you can read here.
Dana G Smith Dec 16Â·6 min read At the beginning of the pandemic, doctors in France prepared for an influx of psychiatric patients with Covid-19, creating special units in hospitals to care for people with mental health problems who contracted the novel coronavirus. People with psychiatric disorders were presumed to be at an increased risk for infection because of potential difficulties complying with protective measures, limited access to health care, close living conditions for those residing in psychiatric wards, and high rates of comorbidities like diabetes and cardiovascular disease. But to the doctorsâ€™ surprise, the units remained largely empty, even during the most severe stage of the pandemic. This observation led the doctors to ask whether psychiatric drugs could be offering some protection against the coronavirus. Sure enough, they found that 10 of the 18 most commonly prescribed psychiatric drugs â€” antidepressant, antipsychotic, and anti-anxiety medications â€” have known antiviral properties, including against the coronaviruses SARS and MERS. Four different types of therapies â€” the antiviral drug remdesivir, convalescent plasma from recovered patients, monoclonal antibodies, and the steroid dexamethasone â€” are currently authorized by the U.S. Food and Drug Administration (FDA) to treat severe Covid-19. But three of these therapies are expensive and must be administered intravenously, while the fourth, dexamethasone, can actually backfire and suppress the immune system if given prematurely. What is still desperately needed, physicians say, is a safe, cheap oral medication that could be prescribed to people with mild Covid-19 to ease their symptoms, help fight the infection early on, and prevent their decline into more severe disease. Ideally, this would be a drug thatâ€™s already approved for another purpose, which would speed up the authorization process and get the medication to patients as soon as possible. Some experts are now saying that psychiatric drugs could fit the bill. â€œWe donâ€™t have anything that can be rolled out on a large scale for outpatients to prevent people from getting into hospital,â€ says Ilan Schwartz, MD, PhD, an infectious disease physician at the University of Alberta. â€œHaving a therapy thatâ€™s already widely available, ideally itâ€™s off-patent so itâ€™s cheap, and for which thereâ€™s already a lot of safety data â€” something with those attributes that could be used against Covid-19 would really be fantastic.â€ What is still desperately neededâ€¦ is a safe, cheap oral medication that could be prescribed to people with mild Covid-19 to ease their symptoms, help fight the infection early on, and prevent their decline into more severe disease. The antidepressant fluvoxamine may be exactly that. A recent study published in the Journal of the American Medical Association found that people who were given fluvoxamine within seven days of developing Covid-19 symptoms were less likely to have a drop in their oxygen levels and be hospitalized than people who received a placebo. â€œFor the fluvoxamine group, we had 80 individuals and none of them had clinical deterioration. But for the placebo group, which was 72 people, six of them deteriorated,â€ says Angela Reiersen, MD, an associate professor of psychiatry at Washington University who led the study. â€œThat was a statistically significant difference, so it suggests that the fluvoxamine may be preventing the clinical deterioration in terms of a drop in the respiratory function.â€ At first glance, it makes little sense that an antidepressant targeting chemicals in the brain would have any impact on a viral infection. Fluvoxamine is classified as a selective serotonin reuptake inhibitor (SSRI), which means it increases levels of serotonin in the brain and body. However, it turns out the drug also activates a protein called the sigma-1 receptor (sigmaR1) that is involved in many fundamental cellular processes â€” ones that the coronavirus hijacks in order to replicate and spread throughout the body. There is now mounting evidence that manipulating sigmaR1 could have both antiviral and anti-inflammatory effects against Covid-19. â€œWe didnâ€™t [test fluvoxamine] because of it being an SSRI or an antidepressant,â€ Reiersen says. â€œThe reason we looked at fluvoxamine was because of its sigma-1 receptor activity.â€ Back in April, an international group of scientists screened for all of the places where SARS-CoV-2 proteins and human cellular proteins interacted. Then, in a follow-up study published in October, they tested whether getting rid of each of those human proteins affected the virusâ€™s ability to infect and replicate in the cell. From the two studies, sigmaR1 emerged as a key location where the virus binds to and manipulates human cells and one that is crucial for the virusâ€™s survival. SigmaR1 is a complicated human protein involved in many fundamental cellular processes, such as energy regulation and the production and transport of other proteins. Scientists arenâ€™t exactly sure why the virus is targeting it. Itâ€™s not where the virus enters the cells â€” thatâ€™s the now infamous ACE2 receptor. But once inside, it appears that the virus somehow uses sigmaR1 to turn the cell into a virus-producing factory. â€œ[SigmaR1 has] been linked to a lot of different pathways, and the question is, which one of those pathways is the one connected to viral infection? More work is needed to flesh that out,â€ says Nevan Krogan, PhD, a professor in the department of cellular and molecular pharmacology at the University of California San Francisco who led the research. â€œA viral protein will spend more of its time trying to hijack proteins that are involved in many different pathways and processes so they get a bigger bang for the buck. In that regard, I think it is probably a smart move for the virus to attack this receptor. But the exact details of why the manipulation is beneficial, we donâ€™t know.â€ â€œWe didnâ€™t [test fluvoxamine] because of it being an SSRI or an antidepressant. The reason we looked at fluvoxamine was because of its sigma-1 receptor activity.â€ Kroganâ€™s team next looked for existing drugs that act on sigmaR1 and tested whether those medications interfere with the coronavirusâ€™s ability to infect human cells in a dish. They identified several psychiatric drugs, including the same antidepressant used in the clinical trial, fluvoxamine, as well as some antipsychotic medications, that had antiviral properties against SARS-CoV-2, presumably because of their effect on sigmaR1. In a final step, the researchers retrospectively analyzed data from people who were hospitalized for Covid-19 and had also been given an antipsychotic. The people who had taken an antipsychotic that acted on sigmaR1 (a class of drugs called â€œtypicalâ€ antipsychotics) were significantly less likely to be ventilated than people who received an â€œatypicalâ€ antipsychotic that did not affect the protein. â€œWeâ€™re not saying people [with Covid-19] should be on antipsychotics. This was, in my mind, more of a proof of principle,â€ Krogan says. â€œBut, in my opinion, sigmaR1 is important for infection without a doubt â€” weâ€™ve genetically shown that â€” and known sigmaR1 modulators have antiviral effects.â€ A few prior studies have looked at the role of sigmaR1 in viral infections. For instance, research from 2013 showed that the protein is important for infection with the hepatitis C virus, and decreasing levels of the protein interfered with the virusâ€™s ability to infect cells. â€œAt this point in the pandemic, I think we need to be very skeptical about embracing therapies without much more robust data.â€ Reiersen, the Washington University psychiatrist, was tipped off to the proteinâ€™s role in another aspect of infections â€” inflammation. A 2019 study showed that mice lacking sigmaR1 were more likely to die from hyper-inflammation, while mice given fluvoxamine to activate the protein were more likely to survive. When news emerged in March that people with Covid-19 were dying because of the hyper-inflammatory cytokine storms, Reiersen launched a clinical trial to see if the drug might be helpful. â€œI thought maybe we could use fluvoxamine to try to prevent the clinical deterioration that might be related to the inflammation,â€ she says. Schwartz, the infectious disease physician, says that the initial clinical trial data for fluvoxamine are promising and that he expects a larger phase 3 study will follow. â€œIâ€™m quite encouraged by the fact that there are some quite serious scientists that have looked at the data quite carefully and decided that there is merit going forward,â€ he says. But, he adds, â€œat this point in the pandemic, I think we need to be very skeptical about embracing therapies without much more robust data.â€
Most people do exactly the wrong thing during a bout of sleepless nights Markham Heid Dec 3Â·5 min read  The power of sleep debt When â€œdo nothingâ€ fails Stress and worry are major insomnia triggers, and so itâ€™s hardly a surprise that the pandemic has set off a wave of lost sleep. Earlier this year, research in the journal Sleep Medicine found that the emergence of SARS-CoV-2 caused a 37% jump in the incidence of clinical insomnia. Even before the pandemic, insomnia was commonplace. Each year, about one in four adults develops acute insomnia, which is defined as a problem falling asleep or staying asleep a few nights a week for a period of at least two weeks. Thatâ€™s according to a 2020 study in the journal Sleep. Fortunately, that study found that most people â€” roughly 75% â€” recover from these periods of short-term insomnia. But for others, the problem persists for months or years. â€œA bad night of sleep can be a one-and-done, it can be a couple of nights for a couple of weeks, or it can turn into a chronic problem,â€ says Michael Perlis, PhD, first author of the Sleep study and director of the Behavioral Sleep Medicine Program at the University of Pennsylvania. A Simple Insight to Help Worriers Rein in Anxious Thoughts Teaching an anxious brain to picture happier scenes or scenarios helps against the inner dialogues that fuel dailyâ€¦ elemental.medium.com One of the reasons that acute insomnia turns into chronic insomnia, Perlis says, has to do with a common mistake people make after a night or two of poor sleep. Even among those who have struggled for years with insomnia, many continue to employ this same counterproductive strategy â€” a strategy that is based on a fundamental misunderstanding of how sleep works. On the other hand, Perlis says that one of the very best remedies for insomnia is also one of the simplest, and it works because it prevents people from making that mistake. â€œDo nothing. Thatâ€™s what I tell people whoâ€™ve had a bad night of sleep, or two or three,â€ he says. â€œBut itâ€™s the hardest nothing youâ€™ll ever do. And Iâ€™ll explain why.â€ The power of sleep debt Perlis says that the common, insomnia-perpetuating error that most people commit is that they try to make up for lost sleep; they take naps, they go to bed early, and they sleep in late. â€œAll of this contributes to sleep dysregulation, which is a recipe for long-term insomnia,â€ he explains. When he tells people to â€œdo nothing,â€ he means that they should not try to make up for lost sleep. Instead, they should stick to their usual sleep-wake routine even on days when theyâ€™re exhausted and dying for a nap or a sleep-in. â€œI tell people to be awake in the service of sleep,â€ he says. â€œIf you build up enough sleep debt, sooner or later that will be enough to force you into deep and prolonged sleep. The ship will right itself.â€ (To be clear, naps can be fine for healthy sleepers, but for those with insomnia, they can make matters worse.) Sleep debt is such a powerful anti-insomnia force that sleep therapists and clinics often employ a technique known as sleep restriction. This involves limiting the time a person is allowed to spend in bed each night to just six or seven hours â€” sometimes less â€” which augments the bodyâ€™s need for sleep. â€œSleep is a homeostatic process, which means that for every hour youâ€™re awake, youâ€™re increasing the pressure [the body feels] to balance that with sleep,â€ Perlis says. Eventually, if a person doesnâ€™t relieve that pressure by taking naps or sleeping in late, the bodyâ€™s homeostatic need for sleep will overwhelm whatever is keeping that person awake at night. â€œIf you build up enough sleep debt, sooner or later that will be enough to force you into deep and prolonged sleep. The ship will right itself.â€ Other sleep doctors echo Perlisâ€™ advice â€” and his warnings. â€œPatients ask all the time, â€˜What happens if my sleep is thrown off? How much do I need to make up?â€™ But thatâ€™s not how sleep works,â€ says Michael Grandner, PhD, director of the Sleep and Health Research Program at the University of Arizona College of Medicine in Tucson. â€œSleep is not like a bank account where if you pull money out and then put money in, it will all balance out.â€ Grandner once worked with Perlis at the University of Pennsylvania. In support of his old colleagueâ€™s recommendation to â€œdo nothing,â€ he offers a useful analogy. â€œIf you have no appetite at dinner, you wouldnâ€™t fix that by eating snacks all day long,â€ he says. â€œThat would create a cycle where youâ€™re eating at the wrong times and you donâ€™t have any hunger when itâ€™s actually time to eat.â€ Today Feels Impossible. Hereâ€™s How Science Suggests We Cope. Uncertainty can hijack our planning machinery and weaponize it against us elemental.medium.com Similarly â€” and due to some related biological systems â€” he says that naps and other attempts to make up for lost sleep can reduce the â€œsleep hungerâ€ a person feels in bed at night. This lack of sleepiness often leads to another night of poor sleep, which leads to more compensatory efforts to make sleep up the next day, and all of this disrupts the cycles and processes that govern healthy sleep. Grandner says that an important element of the â€œdo nothingâ€ approach is the maintenance of a stable sleep-wake schedule, which helps align the bodyâ€™s circadian clocks and rhythms. This means going to bed and getting up at roughly the same times (give or take 30 minutes) each day â€” including on weekends. â€œSleep is highly programmable,â€ he says. â€œYou can train it like you train a dog, but in both cases you have to be consistent.â€ â€œDoing nothing doesnâ€™t mean ignoring the problem,â€ he adds. â€œIt really means staying the course and not overcorrecting after a few bad nights.â€ â€œDo nothing. Thatâ€™s what I tell people whoâ€™ve had a bad night of sleep, or two or three.â€ When â€œdo nothingâ€ fails For many insomniacs â€” especially those who experience only acute and sporadic bouts of problem sleep that are brought on by stress â€” Perlisâ€™ â€œdo nothingâ€ advice will do the trick. But if a personâ€™s insomnia is severe and entrenched, thereâ€™s another remedy that surprisingly few problem sleepers try despite its sky-high rates of success. That remedy is cognitive behavioral therapy for insomnia, or CBT-I. CBT-I is a form of individualized psychotherapy that has become the â€œgold standardâ€ for people with chronic insomnia, and that more than a decade of research has shown to be highly effective. In its clinical practice guidelines, the American College of Physicians recommends CBT-I as its â€œfirst-lineâ€ treatment for chronic insomnia. â€œCBT-I is magic,â€ Perlis says. Like other forms of psychotherapy, CBT-I is tailored to each individualâ€™s situation and challenges. It usually combines a handful of interventions that target a personâ€™s thoughts, behaviors, and sleep routines, and it requires a sleep specialistâ€™s oversight, he explains. â€œThe problem for people with insomnia is that when they experience it, they play the short game, not the long game,â€ he adds. The short game involves trying to catch up on lost sleep, and prioritizing feeling better fast over more durable remedies. The long game may require more work. But the payoff can be a lifetime of better sleep.
Angela Lashbrook Dec 17Â·6 min read Last week, some Goodreads users received a disappointing message: The popular book tracking website is disabling access to its API for users who havenâ€™t used the product in more than 30 days. The company says it â€œplans to retire these toolsâ€ altogether and that, as of December 8, it will no longer issue new keys. Itâ€™s unclear when or if Goodreads will close off its API to active users. â€œWhen I found out, I was pretty upset,â€ says Karen Ellett, a software developer in South Carolina who uses the Goodreads API to power a private tool that tracks book series. The tool, which she had hoped to eventually release for other people to use, keeps track of new releases in book series she reads, which is a function Goodreads doesnâ€™t currently offer. When a new book gets added to the series, Ellettâ€™s tool updates automatically, so she doesnâ€™t have to go looking for it on her own when sheâ€™s ready to dive back into the series. Since sheâ€™s read 172 books this year, itâ€™s not easy for her to mentally juggle all the new additions she wants to get to on her own. â€œIâ€™ve put so many hours into developing this tool not just for myself, but with an eye towards it being utilized by other people. Iâ€™d say I was probably about 70â€“80% done, and now thereâ€™s just no point,â€ she says. As Goodreads is a stagnant product that has barely improved its functionality and features since it was acquired by Amazon in 2013, thousands of readers with basic coding skills use the Goodreads API to power their own better features and tools. On a thread about the change for Goodreads developers, one user says the Discord book recommendations bot he was in the process of building suddenly stopped working. Another says his tool, which analyzes statistics related to the authors on a Goodreads userâ€™s â€œreadâ€ list, will be shut down, nullifying countless hours of work he put into the feature. â€œItâ€™s pretty infuriating.â€ Ellett still uses the API daily, so her access to the API hasnâ€™t been shut down â€” yet. She heard about it from a friend who forwarded the email to her. Many Goodreads API users complain that communication from Goodreads has been terrible, with people only hearing about the change from intermittent users whose access was suddenly terminated. In a statement to Debugger, Goodreads confirmed it plans to retire â€œthe current versionâ€ of its API tools. â€œWhile we assess the value of APIs to determine how to support in the future, we continue to support active API users who meet our terms of service,â€ it says. Matthew Jones is a moderator for r/Fantasy, a fantasy-book-focused subreddit with 1.2 million members. He helps run the subredditâ€™s annual Stabby Awards in which members of the subreddit choose their favorite fantasy reads of the year. Jones was almost done with a Goodreads API-based tool that would have improved his ability to manage the Stabby Awards, but when he returned from vacation, his access to the API had been revoked. He spent much of 2020 working on the tool, he says. â€œItâ€™s pretty infuriatingâ€ how Goodreads has managed communication of the API change, he says. Support has been disappointing for years, making it obvious to API users that the company doesnâ€™t care about them, but â€œIâ€™ve never heard of a service that just completely cut off their members like that. No warnings. No end of life email. Just a â€˜sucks to be youâ€™ notice that everything youâ€™ve been working on is instantly obsolete,â€ he says. The subreddit will have to go back to what has been done in previous years â€” using Google forms and â€œverifying everything by hand.â€ Goodreads has little information about the impending changes on its website or developers discussion board, but one user on the forum says Goodreads support told them that it â€œwill not continue to support API endpoints moving forward.â€ Good e-Reader, a blog dedicated to covering e-reader and digital book industry news, estimates that the shutoff will be complete within the next couple of months. â€œThis is a serious blowâ€ to developers whose tools rely on Goodreads API, writes Good e-Reader editor-in-chief Michael Kozlowski. Those affected by Goodreads pulling the plug on its API say theyâ€™re baffled as to what Goodreads and, by extension, Amazon have to gain by limiting access to its datasets. â€œI donâ€™t understand the motivation since maintaining their APIs was so low-lift for them. But Amazon has a reputation for being ruthless and disfavorable to cooperation,â€ says Stephanie Wilkinson, an engineer whose popular tool, Yonderbook, will cease functioning when the API changes take effect. â€œGoodreads has been frozen in time since 2013 when Amazon acquired them.â€ Yonderbook is an excellent example of what untold numbers of readers have to lose when Goodreads kills its API. The tool analyzes Goodreads usersâ€™ reading statistics and allows them to see which of the books on their lists are available through their local library on Overdrive. It also connects to Bookmooch, where users swap books for free. If youâ€™re anything like me and have hundreds of books on your TBR (to-be-read) list, it would take a very long time to manually search Overdrive to see which books you can check out for free from your local library. Connecting a readerâ€™s Goodreads list to Overdrive makes it much easier for people to use their local libraries and find their next free read. Joe Alcorn, who runs the book website Readng, blogged about the change over the weekend. Goodreads has failed to innovate, he argues, and is pivoting to squashing competition instead. â€œThe sad thing is it really only hurts the hobbyist projects and Goodreads users themselves. Anybody seriously attempting to compete with Goodreads is well aware of the Amazon-shaped elephant in the room and is likely prepared,â€ he writes. â€œItâ€™s the users and the hackers that this move will harm, and if anything it further reinforces the need for viable alternatives.â€ Amazon Has Gone From Neutral Platform to Cutthroat Competitor, Say Open Source Developers For open-source developers, AWS has gone from a neutral platform to a cutthroat competitor onezero.medium.com Itâ€™s difficult to know exactly how many people this change will affect. The Goodreads developers discussion board has 2,602 members, a tiny percentage of the 45 million monthly users on the site. But these are some of the companyâ€™s most dedicated and passionate users â€” people who love reading so much, they want to take what Goodreads has to offer and improve upon it on their own time. Goodreadsâ€™ core ideas and gigantic pool of users were the primary reason Amazon acquired Goodreads in 2013, says Venkatesh Shankar, PhD, the director of research at Texas A&M Universityâ€™s Center for Retailing Studies. But â€œwithout a self-sustaining strategy, the site and app have lagged in usability and usefulness. Other social cataloging sites like Open Library, LibraryThing, The Reading Room, Libib, and BookLamp, each with its own limitations, offer alternatives.â€ Amazon may be exploring ways to monetize Goodreadsâ€™ 90 million users instead of continuing to allow API access, he says. People I spoke with indicated that the change would compel them to seek other tools rather than moving their book activity into the inner Goodreads ecosystem. â€œIt could be that [Amazon] just doesnâ€™t want anyone innovating. Which sucks if they donâ€™t plan on giving us the functionality themselves,â€ says Ellett. â€œI think by doing this, by pushing people to other sources, Goodreads runs the risk of hurting themselves.â€ â€œThere are a ton of new book apps coming on the scene. People have been frustrated with the lack of features in Goodreads,â€ says Wilkinson. â€œIt seems like instead of improving Goodreads to stay competitive, they are choosing to cut off access.â€ Up to now, many of these competitor websites â€” such as StoryGraph, Italic Type, and Readng â€” have remained relatively small. Many less techy users like myself stick with Goodreads out of habit while those with more programming skills are making their own sites and tools that depend on the Goodreads API. But as this community of dedicated readers finds the tools theyâ€™ve perfected no longer work, theyâ€™ll look to other APIs (such as that offered by the Internet Archiveâ€™s book project, Open Library), transition to another one of the current competitors, or even start their own. Amazon might be hoping that by drawing the curtain around its product, it will be bringing its more creative users in with it. But Amazon and Goodreads may have just landed the last blow on this group of readers that sends them on their way to create and strengthen their own communities.
Tal Martsiano Jun 26Â·4 min read The following article explains how to handle custom protocol in different operation systems and launching applications using custom browser protocols. Introduction As most of you already know, most of the operation systems today knows how to handle a call to a very specific list of protocols. Examples, All operation system will automatically suggest your default browser for http or https protocols. there for, running https://<website-url> from your command prompt can even open your browser. Providing mailto://<some-address> URL will automatically opened you mail application. Providing callto://<phone-number> URL may open you caller application if you have one More pre-defined protocols can be found here You can also define you own protocol handlers if you wish. Itâ€™s one of the nicest way to make your browser to run a local application. Motivation Enrich your environment with more protocols or ever replace current protocol with you own implementation can provide a lot of granularity to your products.Image you are writing a parental control application and you wish to replace any http/https call with your own application as a proxy.Or, imagine you would like to make sure an internal application is running in the background from you website. If you asked â€œhow can I run an exe file on my computer from html or javascript in your siteâ€ than here you will find the answer. Windows Windows OS stores its protocols in it registry, the registry structure look like this: you can add your protocol following these instructions: Windows + R key (or Start menu + cmd prompt) Type regedit and click Enter. Search for HKEY_CLASSES_ROOT directory and follow the below to add your protocol Add directory name as you protocol name in HKEY_CLASSES_ROOT.(this is the protocol name later be used) Add an anonymousÂ¹ entry typed REG_SZ with the title of your applications.(It will be shown in any popup of the browser, should be user friendly) Add an entry named URL Protocol typed REG_SZ without any value. Add directory named shell in you new protocol directory Add directory named open in the shell directory above Add and entry named command in the open directory above. Add an anonymous entry typed REG_SZ with the path of your application executable Done! Now you can open your browser and run your-protocol-name://<some parameters> and your application will immediately be executed. Copy-Paste Example Open Windows Shell (as administrator) and run these lines Run my-prot:// and see notepad is being opened. Note: For more information regarding custom protocol you can refer here Linux Linux OS stores its protocols in a files/directory hierarchy over the disk, which looks like this: You can add your own protocol following these instructions: Create an empty properties file named <my-proc>.desktop under ~/.local/share/applications/ directory add the following text into you file, modify the names accordingly: Open ~/.local/share/applications/mimeapps.list and add line under [Default Applications] sections with the following text:(If the section doesnâ€™t exist add this line anyway) From linux terminal run: Done! Now you can open your browser and run your-protocol-name://<some parameters> and your application will immediately be executed. Copy-Paste Example Open one of you text editors and create a file named my-terminal.desktop in ~/.local/share/applications/ and paste this: In linuxâ€™s terminal run: Provide this URL my-terminal:// to your browser and see linuxâ€™s terminal being opened. Security Risks As youâ€™ve seen in the examples above, any browser or external application can run your registered application (handler) under custom protocol. The registered application must handle the varied amount of data and verify the input is correct as it may get input from an un-trusted sources. Make sure your application line / path to binary isnâ€™t exposed to any command injections, example, customer can provide the following input as an argument: In this example, your application will have abcde as an input and your operation system may translate the above to another execution line named format-disk. To summarize, Using custom protocols handler is a very nice way to run applications from browser or website, its commonly used if you need an engine/server to be run locally on the host machine for your site. Enjoy! Please assist with sharing this article, share you gratitude by comment/applaud and feel free to share your note so we can approve the article. Â¹ Anonymous entry is a registry entry with empty name
Erik Engheim 14 hours agoÂ·15 min read If you donâ€™t know any assembly programming or perhaps donâ€™t know much coding at all then RISC-V may be one of the better assembly languages to start with. Sure there are way more x86 assembly tutorials out there. More people who could help you. But x86 is a beast with over 1500 different instructions. RISC-V in contrast was made specifically to be easy to teach while pragmatic enough to actually allow the implementation of high performance microprocessors. If you need a smoother start or donâ€™t know anything about microprocessors, you could read my: How Does a Modern Microprocessor Work? If you need something easier and more â€œfunâ€ then you could being with various games built around assembly programming: Learn Assembly Programming the Fun Way. However if you want a learn a real assembly language used on real hardware, you should stick around here. Getting Setup and Started To make this as easy as possible I will use this online RISC-V interpreter at Cornell University. You can see a screenshot below. The leftmost orange circle shows a simple program with only a single instruction adding two numbers contained in the registers x1 and x2 before storing result in register x3. Registers are named memory cells inside the microprocessor. The microprocessor cannot perform operations directly on data stored in memory (RAM). It needs to get data into a register before it can perform an operation. On the right side you can see a list of registers. The second column named Register show the name of the register. You can see in the first column we can set the initial value for a register. Encircled in orange, you can see I have set the initial values of register x1 an x2 to be 3 and 4 respectively. When I run my program by hitting the green Run button these to registers will be added and the result stored in register x3. I have already run the program, so you can see the Decimal column shows 7 for register x3. You can copy paste in a larger program and try to run that. In this case the input is placed in register x1. It is a number to count down from. Put e.g. 14 in the register by editing the Init Value column. You can see the count down happening by looking at the Decimal column for the registers or the Memory. The countdown happens at the second row in the memory for address 0x04. Before running you should set the CPU to 2 Hz. That means two instructions are executed each second. If the CPU runs faster you will not easily see what is going on. Afterwards you can hit the green Run button. While the program runs you can see it prints out in the box you see below, which line of code is being executed. To Run the program again you need to hit the blue Reset button. Now you know the basics of how to insert a program, input data and run it. So let us start talking about the RISC-V microprocessor and how it is programmed. RISC-V Register Usage Let us talk about the different parts of a RISC-V microprocessor to better understand how it is programmed. In my introduction to microprocessors, I mentioned that in modern RISC processors you can only perform operations on numbers inside registers, not on numbers in memory. RISC-V is no exception. It has 32 general purpose registers named x0 to x31. The first register x0 is special in that it is always zero. Doesn't matter what value you copy into it. It will always produce zero if you try to read from it. It may seem strange, but it is very practical. There are a lot of operations where you need the number zero. By having a register always be zero, you donâ€™t have to copy the value 0 into it. For those familiar with Unix, it serves a bit of the same purpose as /dev/null. It s a place to send values you want to discard. All registers have an alias in parenthesis which help remind the code writer what the register is for. The alternative name for x0 is zero. That means the two lines below are equivalent: The latter part is more readable as it reminds you what the register is being used for. Let us look at some of the other registers: ra return address. sp stack pointer. gp global pointer. tp thread pointer t0 to t6 temporary registers. s0 to s11 saved registers. a0 to a7 function arguments. This naming relates to conventions used for RISC-V code generated by a compiler for a high level language. Say you write some pointless C code like this: What would this translate into in RISC-V assembly code? We will not not go through the code as such but simply talk about the register usage. In line 07 we store the value 4 in variable a. We will need a register to store that value. Because registers could be used by the calculate function called in the line 08 right below, we need to make sure a is preserved? How do we do that? A common practice in the past has been to store variables to memory. In particular to a location we call the stack. But memory is slow on modern processors so we want to avoid accessing memory as much as possible. The RISC-V solution is to come up with a convention. If you place the value of a in one of the s registers s0 to s6 which corresponds to x8, x9 and x18 to x27 then these are saved registers. Called functions must promise not to change these or if they do, save them and restore them before returning. But every register cannot be an s register otherwise we cannot use any register without saving them. The solution to that is the t registers which are temporary. The tmp variable inside calculate can be store in e.g. t0 because it does not need to be preserved. The next problem is how do we pass arguments to functions? The old conventions frequently used on x86 Intel processors was to put arguments in memory on the stack. But again unnecessary memory access is bad. The RISC-V solution is to use the a registers a0 to a6 to store function arguments. x and y argument is passed in a0 and a1 respectively. This corresponds to x10 and x11. The result of the calculation is returned in a0. When we call calculate in line 08 we want to return and be able to continue executing line 09. The convention here is to store the return address in the ra register, which corresponds to x1. Again if you have CISC assembly code background, e.g. from x86 you would have been used to placing this on the stack. Of course eventually we run out of registers. There are conventions for how we store data in memory then, but I will not cover them here. RISC-V Instructions Before we look at concrete instructions it can be useful to look at the common pattern used with RISC-V instructions. If you look at the code below you will see that pretty much all of them take 3 arguments. These arguments are used in a very similar fashion across most instructions. Actually scratch that. We donâ€™t call it arguments in assembly code. Arguments are for functions. Assembly instructions have operands, and the the type of instruction is the opcode. Thus the whole line below is an instruction: While the ADDI part is the opcode and x2, x0 and 1 are the operands. Or more accurately ADDI is a mnemonic, a short word which works as an alias for the actual numeric opcode, used in the machine code. For practical reasons I will mix this terminology a bit. Anyway a typical RISC-V instruction is on this format: In programming terms we could think about this as: The opcode specifies some function f to perform on the two source registers rs1 and rs2 and produce a result which is stored in the destination register rd. For the ADD and SUB instruction this is often written as: Instead of covering every instruction separately, I will use this notation in comments next to instructions. Comments in RISC-V assembly begin with hash symbol #, just like in script languages. Sadly I was not aware of this until writing this very line. Thus a bunch of my previous RISC-V writing is wrong, because I used ; which was commonly used in other assembly languages. Our RISC-V interpreter will not be happy with semi-colons though for comments. It wants hash symbols. Anyway I will use comments to explain specific instructions: Why Three Operands? If you are used to higher level languages, it is not common for functions to take a fixed number of arguments. However assembly code instructions are encoded in a fixed width format of 32-bits. To make it easier for the decoders inside the CPU to figure out what an instruction does, it helps to have different parts of the instruction being in fixed locations. The diagram below shows what the different bits in the 32-bit word encoding an instruction is used for. If you look at the diagram you can see the RISC-V designers have tried to regularize things to to make the job for the decoder easy. Let me explain a bit how. The rd register e.g. spans the same bit positions, bit-7 to bit-11 regardless of form used. The second argument rs1 or begins in the same location in every case. Pseudo Instructions You can however write a bunch of RISC-V instruction which donâ€™t take three arguments, but a lot of these are what we call pseudo-instructions. That means they donâ€™t correspond directly to a machine code instruction. Instead they are simply a shorthand for another instruction. For instance take the NEG instruction: This takes the content of x4 and makes it negative and stores that in x2. Register cannot store a - symbol so what actually happens is that we get the two-complement. Before I explain that let me just show what NEG is shorthand for: Now you can also see some of the advantage of the zero register. It makes it easier to create a lot of these pseudo instructions. Unfortunately our simulator doesnâ€™t support very many of these pseudo instructions, so you will have to write stuff out on the long form. Twoâ€™s Complement To explain how we represent negative numbers it may be useful to work in decimal numbers first and consider tenâ€™s complement first. Computers always work with numbers with some fixed number of digits. Let us assume we work with a decimal computer where registers only work on 2-digit decimal numbers. Look at the calculation below: How did we get that? Consider the how we do subtraction more in detail: We carry from the hundreds position, but there isnâ€™t anything there. Hence we get this wrap around effect. But we can exploit this to have a representation for negative numbers. That means 99 would mean -1. 98 would mean -2 and so on. We can show that this works consistently: Twoâ€™s complement is exactly the same idea but for binary numbers. For four bit numbers 1111 would mean -1. While 1001 would mean -2 and so on. Loading and Storing Data To load data from memory into registers or store data in registers to memory we use the L and S instructions. You need to use different suffixes to indicate what you are loading: LB - Load Byte. LW - Load Word. SB - Store Byte. SW - Store Word. On RISC-V a word means 32-bits, while a byte obviously means 8-bits. These instructions take three operands but the third one is an immediate value. In assembly programming an immediate value is the same as a constant. It is encoded with the instruction rather than being stored in memory or inside a register. Some examples of usage: Addresses, Jumps and Labels For a RISC-V program we know every instruction takes 32-bits. That corresponds to 4 bytes. Thus in a program the first instruction will typically be at address 0, the second at 4, the third at 8 and so on. Let us write out an earlier program with addresses in the first column to make it easier to see how jumps work. First let us clarify a jump or branch is. If you want to repeat part of your code several times over you need to make a jump to a previous instruction. The microprocessor keeps track of the next instruction to execute with a special register called the Program Counter (PC). It gives the address in bytes of the next instruction to execute. Since each instruction is 4 bytes long, the PC gets incremented by 4 each time. Let us look at an example: The BEQ instruction means branch or jump if registers are EQual. If x2 = x4 then the program counter (PC) will be updated to: That means jumping 3 instructions forward. That means we skip the next two instructions. Let us look at the count down program from earlier. The first column contains the instruction address: You can copy this to the simulator without the address column. What you see here is that branching is relative to where you are. On line 12, we want to check if x1 is still larger than zero to see if we should continue our countdown. If it is we want to jump to line 04 where we use SUB to subtract 1 from x1. However we don't write BLT zero, x1, 4. Instead we specify -8. That is because jumps are relative. We jump two instructions backwards. This is in fact quite practical because it means we could move our program to a different location in memory and it would still work. However more importantly it saves a lot of space. You only got 32-bits to encode an instruction. Each register requires 5 bits to encode and the branch specifies two registers which eats up 10-bits. Then the opcode and function eat up 10-bits. That leaves 12-bits to specify an address to jump to. The maximum number you get with 12-bits is 4096 (Â²Â¹Â²). Thus if your program was larger than 4 KB you couldnâ€™t perform jumps. With relative addressing this is not a problem. You can jump 2048 bytes backwards or forwards in the program. Most for-loops, while-loop and if-statement will not be larger than that. However there is one problem with relative addressing. It is awkward for the programmer to write. This is where address labels save us: You can simply label the location you want to jump. Here we use the label loop. Use a colon to indicate this is a label. The assembler will use the label to calculate what offset needs to be used to jump to the given label. Thus depending on what instruction is using the loop label, a different offset will be calculated. Ok let us look at different types of jumps. An instruction which makes an unconditional jump start with a J. Jumps which are conditional start with a B for Branch. Branching â€” Conditional Jumps For conditional branching we compose the mnemonic from a B and a two or three letter combination describing the condition such as: EQ = EQual. NE â‰  Not Equal. LT < Less Than. GE â‰¥ Greater or Equal. Let us look at a few examples of what that would translate to: Unconditional Jumps Often we need to jump around in code without checking if a condition is true or false. Examples of this is: Calling a function. That means setting a registers to function inputs and doing a unconditional jump to location in memory where instructions for function reside. Returning from a function. When we are done executing code in a function we need to return to instruction after call-site. But often you simply need to make unconditional jumps to deal with different conditions. Jump and Link â€” JAL The JAL instruction can be used for both calling functions or just make a simple unconditional jump. JAL makes a relative jump just like the conditional branch instructions. However the provided register argument is not use for comparisons but to store return address. If you don't want to store the return address you can simply provide the zero register x0. The convention used with RISC-V is that the return address should be stored with the return address register ra which is x1. Say you got some C code with call like this: In assembly this would be: Jump and Link Register â€” JALR This is really the same instruction but with the difference that we use an offset from register? What is the point of that? In JAL there is simply not enough space to encode a full 32-bit address. That means you cannot jump anywhere in the code if you are in a larger program. But if you use an address contained in a register, you can jump to any address. Other than that JALR works the same as JAL in that it will also store the return address in rd. While we could use a regular ADDI function to set the rs1 register that is impractical. We want an address relative to the program counter (PC). Fortunately we have a special instruction AUIPC which stands for: Add Upper Immediate to PC. The explanation may be hard to read, but it basically just means that we store the upper 20-bits of the offset to some label in the upper 20-bits of the rd register. The JALR offset can be 12-bits wide, so together (20 + 12 = 32) they make up a 32-bit address. JAL uses a 20-bit address, so if you need to call a function foobar further away than that we use a combination of AUIPC and JALR like this: Code Examples for Next Time This got longer than I expected, so we will have to get into some more code examples next time. This however should give you a lot of the important basics. In the mean time, I highly recommend this instruction reference card: James Zhu RISC-V Reference
Erik Engheim Dec 19Â·17 min read By now it is pretty clear that Appleâ€™s M1 chip is a big deal. And the implications for the rest of the industry is gradually becoming clearer. In this story I want to talk about a connection to RISC-V microprocessors which may not be obvious to most readers. Let me me give you some background first: Why Is Appleâ€™s M1 Chip So Fast? In that story I talked about two factors driving M1 performance. One was the use of massive number of decoders and Out-of-Order Execution (OoOE). Donâ€™t worry it that sounds like technological gobbledegook to you. This story will be all about the other part: Heterogenous computing. Apple is aggressively pursued a strategy of adding specialized hardware units, I will refer to as coprocessors throughout this article: GPU (Graphical Processing Unit) for graphics and many other tasks with a lot of data parallelism (do the same operation on many elements at the same time). Neural Engine. Specialized hardware for doing machine learning. Digital Signal processing hardware for image processing. Video encoding in hardware. Instead of adding a lot more general purpose processors to their solution, Apple has started adding a lot more coprocessors to their solution. You could also use the term accelerator. This isnâ€™t an entirely new trend, my good old Amiga 1000 from 1985 had coprocessors to speed up audio and graphics. Modern GPUs are essentially coprocessors. Googleâ€™s Tensor Processing Units are a form of coprocessors used for machine learning. What is a Coprocessor? Unlike a CPU, a coprocessor cannot live alone. You cannot make a computer by just sticking a coprocessor into it. Coprocessor as special purpose processors which do a particular task really well. One of the earliest examples of a coprocessors was the Intel 8087 floating point unit (FPU). The humble Intel 8086 microprocessor could perform integer arithmetic but not floating point arithmetic. What is the difference? Integers are whole numbers like this: 43, -5, 92, 4. These are fairly easy to work with for computers. You could probably wire together a solution to add integer numbers with some simple chips yourself. The problem starts when you want decimals. Say you want to add or multiply numbers such as 4.25, 84.7 or 3.1415. These are examples of floating point numbers. If the number of digits after the point was fixed, we would call it fixed point numbers. Money is often treated this way. You usually have two decimals after the point. You can however emulate floating point arithmetic with integers, it is just slower. That is akin to how early microprocessors could not multiply integers either. They could only add and subtract. However one could still perform multiplication. You just had to emulate it will multiple additions. For instance 3 Ã— 4 is simply 4 + 4 + 4. It is not important to understand the code example below, but it may help you understand how multiplication can be performed by a CPU only by using addition, subtraction and branching (jumping in code). If you do want to understand microprocessors and assembly code, read my beginner friendly intro: How Does a Modern Microprocessor Work? In short, you can always achieve more complex math operations by repeating simpler ones. What all coprocessor do is similar to this. There is always a way for the CPU to do the same task as the coprocessor. However this will usually require repetition of multiple simpler operations. The reasons we got GPUs early on, was that repeating the same calculations on millions of polygons or pixels was really time consuming for a CPU. How Data is Transmitted to and from Coprocessors Let us look at the diagram below to get a better sense of how a coprocessor work together with the microprocessor (CPU), or general purpose processor, if you will. We can think of green and light blue busses as pipes. Numbers are pushed through these pipes to reach different functional units of the CPU (drawn as gray boxes). The inputs and outputs of these boxes are connected to these pipes. You can think of the inputs and outputs of each box as having valves. The red control lines are used to open and close these valves. Thus the Decoder, in charge of the red lines, can open valves on two gray boxes to make numbers flow between them. This lets us explain how data is fetched from memory. To perform operations on numbers we need them in registers. The Decoder uses the control lines to open the valves between the gray Memory box and the Registers box. This is how it specifically happens: Things like mouse, keyboard, the screen, GPU, FPU, Neural Engine and other coprocessors are equal to the Input/Output box. We access them just like memory locations. Hard drives, mouse, keyboard, network cards, GPU, DMA (direct memory access) and coprocessors all have memory addresses mapped to them. Hardware is accessed just like memory locations by specifying addresses. What exactly do I mean by that? Well let me just make up some addresses. If you processor attempts to read from memory address 84 that may mean the x-coordinate of your computer mouse. While say 85 means the y-coordinate. So to get a mouse coordinates you would do something like this in assembly code: For a DMA controller there might might be address 110, 111 and 113 which as special meaning. Here is an unrealistic made up assembly code program using this to interact with the DMA controller: Everything works in this manner. You read and write to special memory addresses. Of course a regular software developers never sees this. This stuff is done by device drivers. The programs you use only see virtual memory addresses where this is invisible. But the drivers will have these addresses mapped into its virtual memory addresses. I am not going to say too much about virtual memory. Essentially we got real addresses. The addresses on the green bus will get translated from virtual addresses to real physical addresses. When I began programming in C/C++ in DOS, there was no such thing. I could just set a C pointer to point straight to a memory address of the video memory and start writing straight to it to change the picture. Coprocessors work the same way as this. The Neural Engine, GPU, Secure Enclave and so on will have addresses you communicate with. What is important to know about these as well as something like the DMA controller is that they can work asynchronously. That means the CPU can can arrange a whole bunch of instructions for the Neural Engine or GPU which it understands and write these into a buffer in memory. Afterwards it informs the Neural Engine or GPU coprocessor about location of these instructions, by talking to their IO addresses. You donâ€™t want the CPU to sit there and idle waiting for the coprocessor to chew through all the instructions and data. You donâ€™t want to do that with the DMA either. That is why usually you can provide some kind of interrupt. How does an Interrupt Work? Various cards you stick into your PC, whether they are graphics cards or network cards will have assigned some interrupt line. It is kind of like a line that goes straight to your CPU. When this line get activated, the CPU drops everything it is holding to deal with your interrupt. Or more specifically. It stores in memory its current location and the values of its registers, so it can return to whatever it was doing later. Next it looks up in a so called interrupt table what to do. The table has an address of a program you want to run when that interrupt is triggered. As a programmer you donâ€™t see this stuff. To you it will appear more like callback functions which you register for certain events. Drivers typically handle this at the lower level. Why am I telling you all these nerdy details? Because it helps develop an intuition about what is going on when you use coprocessors. Otherwise it is unclear what communicating with a coprocessor actually entails. Using interrupts allow lots of things to happen in parallel. An application may fetch an image from the network card, while the CPU is interrupted by the computer mouse. The mouse has been moved and we need the new coordinates. The CPU can read these and send them to the GPU, so it can redraw the mouse cursor in the new location. When the GPU is drawing the mouse cursor the CPU could begin processing the image retrieved from the network. Likewise with these interrupts we can send complex machine learning tasks to the M1 Neural Engine to say identify a face on the WebCam. Simultaneously the rest of the computer is responsive because the Neural Engine is chewing through the image data in parallel to everything else the CPU is doing. The Rise of RISC-V Back in 2010 at UC Berkley the Parallel Computing Laboratory saw the development towards heavier use of coprocessors. They saw how the end of Mooreâ€™s Law meant that you could no longer easily squeeze more performance out of general purpose CPU cores. You needed specialized hardware: Coprocessors. Let us reflect momentarily on why that is. We know that the clock frequency cannot easily be increased. We are stuck on close to 3â€“5 GHz. Go higher and Watt consumption and heat generation goes through the roof. However we are able to add a lot more transistors. We simply cannot make the transistors work faster. Thus we need to do more work in parallel. One way to do that is by adding lots of general purpose cores. We could add lots of decoders and do Out-of-Order Execution (OoOE) as I have discussed before: Why Is Appleâ€™s M1 Chip So Fast? Transistor Budget: CPU Cores or Coprocessors? You can keep playing that game and eventually you have 128 general cores like the Ampere Altra Max ARM processor. But is that really the best use of our silicon? For servers in the cloud that is great. One can probably keep all those 128 cores busy with various client requests. However a desktop system may not be able to effectively use more than 8-cores on common desktop workloads. Thus if you go to say 32 cores, you are wasting silicon on lots of cores which will sit idle most of the time. Instead of spending all that silicon on more CPU cores, perhaps we can add more coprocessors instead? Think about it this way: You got a transistor budget. In the early days, maybe you had a budget of 20 000 transistors and you figured you could make a CPU with 15 000 transistors. That is close to reality in the early 80s. Now this CPU could do maybe 100 different tasks. Say making a specialized coprocessor to one of these tasks cost you 1000 transistors. If you made a coprocessor for every task you would get to 100 000 transistors. That would blow your budget. Transistor Abundance Change Strategy Thus in early designs one needed to focus on general purpose computing. But today, we can stuff chips with so many transistors, we hardly know what to do with them. Thus designing coprocessors has become a big thing. A lot of research goes into making all sorts of new coprocessors. However these tend to contain pretty dumb accelerators which needed to be babied. Unlike a CPU they cannot read instructions which tells them all the steps to do. They donâ€™t generally know how to access memory and organize anything. Thus the common solution to this is to have a simple CPU as a sort of controller. So the whole coprocessor is some specialized accelerator circuit controlled by a simple CPU, which configures the accelerator to do its job. Usually this is highly specialized. For instance, something like a Neural Engine or Tensor Processing Unit deal with very large registers that can hold matrices (rows and columns of numbers). RISC-V Was Tailored Made to Control Accelerators This is exactly what RISC-V got designed for. It has a bare minimum instruction-set of about 40â€“50 instructions which lets it do all the typical CPU stuff. It may sound like a lot, but keep in mind that an x86 CPU has over 1500 instructions. Instead of having a large fixed instruction-set, RISC-V is designed around the idea of extensions. Every coprocessor will be different. It will thus contain a RISC-V processor to manage things which implements the core instruction-set as well as an extension instruction-set tailor made for what that co-processor needs to do. Okay, now maybe you start to see the contours of what I am getting at. Appleâ€™s M1 is really going to push the industry as whole towards this coprocessor dominated future. And to make these coprocessors, RISC-V will be an important part of the puzzle. But why? Canâ€™t everybody making a coprocessor just invent their own instruction-set? After all that is what I think Apple has done. Or possibly they use ARM. I have no idea. If somebody knows, please drop me a line. What is the Benefit of Sticking with RISC-V for Coprocessors? Making chips have become a complicated and costly affair. Building up tools to verify your chip. Run tests programs, diagnosis and a host of other things requires a lot of effort. This is part of the value of going with ARM today. They have a large ecosystem of tools to help verify your design and test it. Going for custom proprietary instruction-sets is thus not a good idea. However with RISC-V there is a standard which multiple companies can make tools for. Suddenly there is an eco-system and multiple companies can share the burden. But why not just use ARM which is already there? You see ARM is made as a general purpose CPU. It has a large fixed instruction-set. After pressure from customers and RISC-V competition ARM has relented and in 2019 opened its instruction-set for extensions. Still the problem is that it wasnâ€™t made for this from the onset. The whole ARM toolchain is going to assume you got the whole large ARM instruction set implemented. That is fine for the main CPU of a Mac or an iPhone. But for a coprocessor you donâ€™t want or need this large instruction-set. You want an eco-system of tools that have been built around the idea of a minimal fixed base instruction-set with extensions. Nvidia using RISC-V Based Controllers Why is that such a benefit? Nvidiaâ€™s use of RISC-V offers some insight. On their big GPUs they need some kind of general purpose CPU to be used as a controller. However the amount of silicon they can set aside for this, and the amount of heath it is allowed to produce is minimal. Keep in mind that lots of things are competing for space. The small and simple instruction-set of RISC-V makes it possible to implement RISC-V cores in much less silicon than ARM. Because RISC-V has such a small and simple instruction-set it beats all the competition, including ARM. Nvidia found they could make smaller chips by going for RISC-V than for anybody else. They also reduced watt usage to a minimum. Thus with the extension mechanism you can limit yourself to adding only the instructions crucial for the job you need done. A controller for a GPU likely needs other extensions than a controller on an encryption coprocessor e.g. RISC-V Machine Learning Accelerator (ET-SOC-1) Esperanto Technologies is another company that found a value in RISC-V. They are making an SoC, called ET-SOC-1, which is slightly larger than the M1 SoC. It has 23.8 billion transistors compared to the 16 billion on the M1. Instead of four general purpose Firestorm cores, it as four RISC-V cores called ET-Maxion. These are suited for doing general purpose stuff like running a Linux operating system. But in addition to this it has over 1000 specialized coprocessors called ET-Minion. These are RISC-V based coprocessors which implement the RISC-V vector extension. What is the significance of that? These instructions are particularly well suited for processing large vectors and matrices which modern machine learning is all about. You may be looking at the number of cores in disbelief. How can the ET-SOC-1 have so many more cores than the M1? It is because a Firestorm core is meant to deal with typical desktop workloads which cannot easily be parallelized. Hence lots of tricks have to be pulled to attempt to run code in parallel which is not trivial to parallelize. That eats up a lot of silicon. ET-Minion cores in contrast deal with problems which are trivial to parallelize, and these core can thus be really simple, cutting down on the amount of silicon needed. The key takeaway from ET-SOC-1 is that producers of highly specialized coprocessors are seeing a value in building coprocessors based on RISC-V. Both ET-Maxion and ET-Minion cores will be licensable from Esperanto Technologies. That means in theory Apple (or anybody else) could license ET-Minion cores and put a ton of them on their M1, to get superior machine learning performance. ARM Will Be The New x86 Ironically we may see a future where Macs and PCs are powered by ARM processors. But where all the custom hardware around them, all their coprocessors will be dominated by RISC-V. As coprocessor get more popular more silicon in your System-on-a-Chip (SoC) may be running RISC-V than ARM. Read more: RISC-V: Did Apple Make the Wrong Choice? When I wrote the story above, I had not actually fully grasped what RISC-V was all about. I though the future would be about ARM or RISC-V. Instead it will likely be ARM and RISC-V. ARM Commanding an Army of RISC-V Coprocessors General purpose ARM processors will be at the center with an army of RISC-V powered coprocessors accelerating every possible task from graphics, encryption, video encoding, machine learning, signal processing to processing network packages. Prof. David Patterson and his team at UC Berkeley saw this future coming and that is why RISC-V is so well tailored to meet this new world. We are seeing such a massive uptake and buzz around RISC-V in all sorts of specialized hardware and micro-controllers that I think a lot of the areas dominated by ARM today will go RISC-V. Imagine something like Raspberry Pi. Now it runs ARM. But future RISC-V variants could offer a host of variants tailored for different needs. There could be machine learning microcontrollers. Another can be image processing oriented. A third could be for encryption. Basically you could pick your own a little micro-controller with its own little flavor. You may be able to run Linux on it and do all the same tasks, except the performance profile will be different. RISC-V microcontrollers with special machine learning instructions will train neural networks faster than the RISC-V microcontroller with instructions for video encoding. Nvidia has already ventured down that path with their Jetson Nano, shown below. It is a Raspberry Pi sized microcontroller with specialized hardware for machine learning, so you can do object detection, speech recognition and other machine learning tasks. RISC-V as Main CPU? Many ask: Why not replace ARM entirely with RISC-V? While others claim that this would never work because RISC-V has a â€œpuny and simpleâ€ instruction-set which cannot deliver the kind of high performance that ARM and x86 offers. Yes, you could use RISC-V as the main processor. No, performance is not stopping us from doing it. Just like with ARM, we just need somebody to make a high performance RISC-V chip. In fact it may already have been done: New RISC-V CPU claims recordbreaking performance per watt. It has been a common misconception that complex instructions give higher performance. RISC workstations disproved that back in the 90s as they destroyed x86 computers in performance benchmarks. How did Intel beat the RISC workstations in the 90s: Is It Game Over for the x86 ISA and Intel? In fact RISC-V has a lot of clever tricks up its sleeve to get high performance: The Genius of RISC-V Microprocessors. In short, there is no reason why your main CPU couldnâ€™t be a RISC-V processor, but this is also a question of momentum. MacOS and Windows already runs on ARM. At least in the short term, it seems questionable that either Microsoft or Apple will spend the effort on doing yet another hardware transition. Share Your Thoughts Let me know what you think. There is a lot going on here which is hard to guess. We see e.g. now there are claims of RISC-V CPUs which really beats ARM on watt and performance. This also makes you wonder if there is indeed a chance that RISC-V becomes the central CPU of computers. I must admit it has not been obvious to me why RISC-V would outperform ARM. By their own admission, RISC-V is a fairly conservative design. They donâ€™t use much instructions which have not already been used in some other older design. However there seems to be a major gain from pairing everything down to a minimum. It makes it possible to make exceptionally small and simple implementations or RISC-V CPUs. This again makes it possible to reduce Watt usage and increase clock frequency. Hence the last word on RISC-V and ARM is not yet said.
7 hours agoÂ·11 min read Create a full and robust API with Get, Post, Put, and Delete capabilities. Table of Contents: 1. Introduction to APIs: What is an API? An API, or Application Programming Interface, can be simply defined as an interaction between various software components. When a user clicks a button to see a list of their Facebook friends, likes on Instagram, or emails within an inbox, data is generally being exchanged in one way or another through a web service API. There are a number of different types of web service APIs such as: For the purposes of this article, we will be focusing specifically on the utilization of REST as an architectural principle. How do APIs work? APIs function as methods allowing for the transfer of data from the client-side, to an API server which interacts with a database of some sort. Data is generally retrieved based on a set of conditions, and then returned back to the client. One of the most commonly used examples for an API generally involve the stock market, companies and daily prices. While keeping the diagram above in mind, a user (client) may request the price history data for a particular company. The client-side would make an API call specifying a specific company, and the API server would retrieve that information from the database, and return it to the user. Note that the user never interacts with the database, only the API server. What is an API Call? An API call is an action in which an endpoint of a server is specified and request, and the server responds with the requested content. Take for example the process of logging into a social media website. A user would fill out their login information, and then click â€œloginâ€. Upon clicking this button, a number of API calls are made to authenticate the user and then retrieve the userâ€™s data. This in of itself is an example of an API call. Let us take a look at a simple example of a URL in which stock price data is being requested for a specific company. In the above example, you see the URL request broken down by section. We begin with an HTTPs protocol, followed by the name of a particular domain of interest. Following the best practices of a RESTful API, we then specify the path of the api as well as the version. We then specify the exact end point of interest. Notice that the endpoint follows a particular â€œfunnel downâ€ approach in which we narrow down the scope of the data as we go from the left to right. We first start with all of the companies, and then specifically selection Apple as the company of interest, and then specifically request the prices from that one company. This is a classic RESTful architectural pattern. When interacting with an API server, there are four main methods one can take: GET, POST, PUT, and DELETE. For a more detailed explanation, please visit my last article in which in discuss this in more detail in a Python setting. 2. Getting Software Installed: Installing Node and Mongo : In the following steps, we will be developing a Node.js API using Mongo as our database. Let us go ahead and get started. You can download Node.js directly from their website. Select the LTS option which is recommended for most users, and install that locally on your machine. You can confirm the installation by running â€œnodeâ€ on your command line. â€¦and you should see a welcome message from nodejs. If you do not, please re-run the installation of node. On the other hand the MongoDB Community Edition can be downloaded using this link. Select the version appropriate for your OS, and click download and install. You can confirm the installation of mongo by running either â€œmongoâ€ to access the client, or â€œmongodâ€ to start the database server. 3. Creating a Remote Mongo Database (AWS): There are a number of remote database options ranging from scalable relational database to object databases depending on the specific usecase. For the purposes of this tutorial, we will be deploying our API to an object database known as Mongo. A Mongo database can be used via the CLI we installed in the previous section, or via a GUI. To get started, you will need to navigate the to https://www.mongodb.com/. Create a new account by selecting the â€œStart Freeâ€ option in the main page. once you have signed up for a new account, navigate to the clusters section of the website and select â€œCreate a New Clusterâ€. For the Cloud Provider & Region, select AWS and change the region closest to you. When selecting a region, be sure to select one with with â€œFree Tierâ€ option. Keep all other settings such as Cluster Tier & Additional Settings as their defaults. Feel free to change the cluster name. Finally, click â€œCreate Clusterâ€. You will need to wait a few minutes while the cluster gets provisioned. Follow the tutorial that will auto-start at the end of the provisioning. This will walk you through creating the clusters credentials. Be sure to note the username, password, and collection. We will need those in a few minutes. Back on the main cluster page, find your new cluster and click the â€œConnectâ€ button: A new menu will appear with three options. Select the option called â€œConnect to MongoDB Compassâ€. Clicking this option will bring up another menu consisting of two steps. Step 1 is to download Compass which is a GUI interface allowing you to interact with the database. The alternative to this is using the CLI we installed in the previous section, however, it is highly recommended you use the GUI. Step 2 is to copy the connection string. Be sure to substitute <password> for your actual password you created in the tutorial. Save this connection string, we will need this in a few moments. With Atlas installed, you can now open the GUI, connected to your database using the connection string, and create new data for your database. Creating a Remote Node Server: Getting Started with Node.js: We can get stared by creating a new directory and then initializing npm (which was installed with Node). You will be asked to fill out a few items for the project such as the name, author, etc. These are completely optional, but recommended as best practice. Keep most items as their default values. At this point, the project is created and ready for development. I would recommend you open the directory with your favorite IDE such as IntelliJ, WebStorm, or VisualCode â€” all will function the same. We will be creating a few files within the project, each of which server a very specific function. Theoretically, we could group the contents of all of these files into server.js, however, decoupling the contents based on function is considered best practice when developing a node API, which is what we will complete here today. We beign with server.js which is the main file any incoming request will interact with. This then prompts the controller which determines the specific function to carry out based on the endpoint. The service then communications with the DAO (Data Access Object), which carries out a specific function in conjunction with the model and schema of our database â€” both of which will be governed using the mongoose library. To get stared, let us go ahead and install two libraries we will need: (1) Express to handle the backend web application framework of the API, and mongoose to help with the database interaction with Mongo. server.js The server.js file acts as the main â€˜gatekeeperâ€™ for all API calls. There are three main functions we will set up within this file: Managing access is commonly done through what is known as a CORS policy. This can be configured using the CORS library which allows you to specify the domains that you do and do not wish to grant access to. For the purposes of this tutorial, we will grant access to all domains by listing the â€œ*â€ in the code below. Once access is determined and granted, the file then passes the API request to controller which will determine the type of request by determining the endpoint. In order to access the controller, we must â€˜requireâ€™ it. Finally, we will connect to the remote Mongo database using the Mongoose library. Paste the connection from the previous section that was copied from the MongoDB website. book-controller.js The controller is responsible for outlining the endpoints of your API that users can navigate to. For example, one particular endpoint we can create is www.website.com/api/v1/books which should return a list of all books. Perhaps we could even narrow down the scope and return a single book by specifying the bookâ€™s ID. For that, we could use an endpoint of www.website.com/api/v1/books/123, in which 123 is the bookâ€™s ID or primary key. We could also set up an endpoint to query by author. In addition to retrieving values, we can also specify other CRUD operations here such as deleting, posting, and updating our database entries. book-service.js The book-service file is responsible for connecting the book-controller to the book-dao (Database Access Object). Within this file we simply create a connection by creating a new constant and connecting it to the dao function. In the case of finding all books: book-dao.js The book-dao file is responsible for connecting the data access object to the database objectâ€™s model. It is here that we can now use Mongoose operations such as find() and findById(id). For example, we can find all book values using: book-schema.js The book-model file is responsible for handling the model and its association to the predefined schema in the file book-schema. One of the benefits of defining a schema and connecting it to the model is ensuring that all entries will be identify in their format. Within this file we define the attributes of our Book object in the sense that a book can have a title and author which are of the type String. A publication date which is of the type Date. An edition in the form of an integer. And finally a type which must be of one of three available options of type String. Running the Node Server: Depending on your IDE of choice, there may be some built in capabilities to run your server directly from your menu bar. If not, you can always run this via the command line. Navigate to the folder containing the server.js file. In your web browser, navigate to http://localhost:3000/api/v1/books which should now display a list of books in JSON format. With that, the server is now ready for deployment on Heroku. Go ahead and add and commit all your code thus far to GitHub. 5. Deploying your API to Heroku There are many servers where one can deploy their API, one of which is Heroku. If you have not done so already, navigate to www.heroku.com and create a free new account. Within the apps menu, click on â€œNewâ€ and then â€œCreate new appâ€. Assign the application a name and region, and click â€œcreateâ€. You will then be guided to the applicationâ€™s menu. Within the application, navigate to the â€œDeployâ€ section. Next, select your deployment method of choice. You are free to deploy your API via the Heroku CLI, or via the GitHub integration. Once the application has been deployed, click on the â€œOpen Appâ€ option at the top of the menu and you will be directed to the API where you can now navigate to the API endpoints we prepared. Conclusion Within this article we covered the basic concepts of RESTful APIs including their characteristics, structure, and architecture. We then provisioned and deployed a mongo database using MongoDB and AWS. Next, we prepared a best-practice Node server with full CRUD operations to manage our database of books. Finally, we deployed the server on Heroku allowing us to access our API from anywhere. The code for this project can be cloned from by GitHub account.
Nov 29Â·6 min read I have worked for different companies which allowed me to experience different strategies when it comes to take an idea and release it to the public as a final product. Whether you are part of a small or big project, I believe any developer would benefit a lot from understanding the entire process of what it takes to deliver software or application. Analysis The first thing that triggers a product or software development is the idea or a solution for a specific problem. After that, you normally look at the market for existing solutions if any. If you find one you take a closer look at them to see if they lack something you can contribute to or if you can just go ahead and build something better to compete in the market. I call this part â€œresearch the ideaâ€ and in a software development cycle it is called analysis and it is the step where you define the scope and the project itself. You measure the risks, define a timeline that will take you to the final result, define or anticipate issues or opportunities, and plan for things as well as come up with the requirements for the project. This step may even determine if the project should go forward or not as well as how. Documentation This phase is to come up with a way to document the project solution and requirements and it must contain everything needed during development and provide few checks: Economic, Legal, Operations, Technical, and Schedule, more or less. You must define the costs related to the development of the project, go over copyright and patents to avoid any legal conflict around the idea and product. The delivery schedule is a big one especially if you have a sales, marketing, and social media team and they need to create content and be ready for launch to promote the product. Design The design phase is not just about designing the interface of the product itself but anything related to it. It can be the overall system architecture, how the data look like, where it will be stored, and how the data flows in the system. You also define the functionality of each module and all related logic as well as how these modules talk to each other and yes, you also design the interface of the software as well. Coding After the design phase is the coding phase where you analyze the idea the documentation, requirements, and specifications and start coding things by following a coding plan, the product schedule, timeline, and roadmap. Anything that turns out to be more complex and deviates from the original plan should be communicated. Things may change as a result. I often see plan B being applied where you find an MVP version of the feature or the delivery and implement that and come back to it later on where you further improve the feature after much detailed research. The show must go on and it is hard to remove a wagon after a train is in motion. The coding is done maybe by following an agile development technique where features are delivered in sprints, are planned in sprint plannings, and get daily Engineer updates in daily stand-ups. The development team keeps a backlog of features and bugs to distribute among them and address them per sprint which usually takes 2 weeks. Testing When the code is done it is the testing phase. I am not talking about unit tests as those should happen during the coding phase whether you use a Test Driven Development technique or not. The test phase is for QA and E2E(sometimes). These tests do not happen after 100% of the things are coded. They happen as different parts are completed. Anything that is found to be faulty or deserves improvement is sent back to be fixed by the engineers. The goal is to not introduce new features but to check that what was coded follows the requirements and does what it is supposed to do. The E2E is created to automate the user flow in a step by step pattern to mimic how the user would use the product. Deployment If everything is coded, tested, and seems to be right it then gets deployed but it does not mean that developers and testers job is complete. The QA then tests things in production as the production and development environments are different and again, anything found to be broken is also sent to be fixed by the developers. At this point, the user will start interacting with the product and sometimes things come up and this is when customer support comes in. These people understand how the product works because they got trained as things were being built or at the end. These people will guide the users, through the product in case of any problem or if the user is stuck in some issue that is preventing them from using the product where anything perceived to be a problem is turned into issues that are sent to the developerâ€™s backlog to be checked by the engineering team and gets fixed if necessary. The customer support may even be the developers as well. Some companies use the concept of having developers â€œon-callâ€ for any user-related issues. Normally small companies do that and these engineers stay on call even during non-business hours. Maintenance After launch, there is the maintenance phase, the final phase of the cycle. This phase includes bug fixes like those reported after launch, software upgrades, and any new feature enhancements. The development cycle is circular so if any new thing, version, or complex update needs to be done it goes from phase 1 again until it is delivered. Observation One thing to notice is that the coding phase is often small. There is a lot of planning and support time dedicated to delivering a product. I worked in companies where we took 2 and a half years to deliver a product as well as others that took 3, 6, or 9 months depending on the product type. No matter the time it takes to deliver software, they all follow or try to follow a software development cycle. Red Flag A red flag would be a place where the coding time is the largest phase and normally these are startups that experiment, test, and come up with requirements as things are being coded and designed. These environments tend to be very stressful to work at as things may change as you are coding them, meaning, you start a sprint with a set of requirements and by the end of the sprint the design and requirements may change which may mean that the developers need to allocate extra time to address these changes. Conclusion It should not matter the size of the project, whether it is a side project or a freelance project. You should always try to follow a plan and get good it. The steps will narrow your focus and allow you to deliver a product in junks which will keep you on track and satisfied as you go. I implement these steps fully or partially in my deliveries which allows me to finish a side project, give a detailed plan, pricing, and timeline for a freelance project client, communicate well with the VP, managers, and project owners at work. Watch me code and create things by visiting beforesemicolon.com or subscribing to my YouTube channel. Blog & Youtube Channel | Web, UI, Software Development & Developer's Career Tips Blog & Youtube Channel | Web, UI, Software Development & Developer's Career Tips Blog & Youtube Channel | Web, UI, Software Development & Developer's Career Tipsbeforesemicolon.com
Nov 26Â·7 min read â€œCSS is not easy â€œ is probably one of the biggest complaints in the web development community. I strongly believe that beyond learning the fundamentals, CSS requires creativity and the ability to visualize things. CSS is powerful and the more you dig in, the more of a magician you become. When it comes to essentials, these are what I consider to be the top 15 things to master at first. HTML I know, I know! This is a CSS post, but CSS becomes way easier when you learn all the basics of HTML. CSS is butter for HTML and they always go together. Oftentimes, to accomplish the style you want you may want to structure your HTML differently but you should not sacrifice structure and semantics for styling. The better you get with CSS the better is your HTML structure and a great web developer knows how to get the UI right without introducing too much unnecessary HTML, compromise accessibility, or make content un-manageable. Box Model The way I approach things on the page is by thinking that everything is a box. The CSS box model is pretty much one of the top secrets to better learn CSS. The margin is how you distance the box apart, the border is the box walls, the padding is the space between the box content and the walls and then you have your content which can be other boxes or text. One thing that is often left out when talking about the box model is the outline which is the line that sits between the border and the margin which is also stylable. Specificity & Cascading flow This is pretty much what defines CSS because CSS stands for Cascading Style Sheets. The number one issue when writing CSS is having the style be overwritten by some other style either third-party or yours. Many tools have emerged promising fixing this by isolating your style block but the best way to fix this is to understand it. Learn how specific you need to be, in which order your style is applied by the browser, how to override third-party styles, the issue around the !important flag, etc. Try to follow the CSS flow and how it works and often times it requires you to come up with a CSS structure that works for you and sticking to it. Selectors, Combinators, Pseudo Class & Pseudo Elements I made a video about all the Selectors, Combinators, Pseudo Class, and Elements and there is so much magic in learning these CSS details. The selectors help you target HTML, the combinators help you combine and target with patterns. The pseudo-classes help you target state and the pseudo-elements help you target or create specific parts of your HTML. It is magical when you master these concepts and so important in becoming a CSS magician. Variables Variables pretty much take CSS to the Next Level. CSS already has some useful functions and needed the variables to help make it even more powerful. Variables introduce the idea of sharing and scoping values and knowing your way around how to use variables will take your CSS style to the next level. I use variables on SCSS but SCSS is a pre-processor which means once the CSS reaches the browser it no longer exists. With CSS variables you can manipulate them with Javascript which makes it for amazing features like toggling dark and light mode on your website. Display and Positioning The CSS display and position properties allow you to control how the elements are positioned in and outside the content flow. Any CSS developer should master these two and everything that derives from using them. Positioning things remain one of the biggest challenges when it comes to CSS and because of that, you should not neglect your way around the display and position properties. Responsive Design & Fluid Layout Being able to adapt your website to any size and being able to change the layout to adapt to the available space is a superpower on its own. I normally would recommend focusing on CSS media queries but CSS has come a long way which means you can make responsive pages without using media queries by using the display grid and/or flexbox and controlling font with CSS clamp. Responsive and fluid design goes beyond knowing your way around display flex and grid. It requires a developer to understand device and viewport scale, how different tags behave under available space constraints, HTML meta tags, the right distance in between elements given the viewport, different device sizes, etc. It is all about the experience and so many other things that factor into it. Image & Background Two things that go hand in hand are the <img/> tag and the CSS background and you must know when to use the image tag vs the CSS background. In particular, the CSS background is powerful and you can even use it to create text underlines and some amazing other background effects you can check on this playlist I put together for you. For me, it is one of the most fun parts of CSS and one you should definitely not ignore. Font Here something often devs neglects but I find it to be very important. You need to know how to optimize Fonts for your site and it often means how to load them, what other options are available, how to control them in order to really take advantage of a font to make things look great. It is hard to find a font that is just perfect so you need to know how to take it to the next level and this is something designers often manipulate to get just right so you need to do the same to get the page to match the designs. Color Color is pretty much everywhere. You use it for border, background, outline, etc. Most developers use plain hex color values or the color name but you can take CSS colors too many steps forward by using the RGBA to control the alpha of the color, HSLA to control the hue, saturation, and lightness on top of the alpha value. Color is not just about the values. CSS currentColor property is something to factor in as well. You can also add an additional 2 values to your hex color values to control the alpha of color like RGBA. For example, #29910d82 is an opaque green where the extra 82 controls the alpha value similar to the last value when using RGBA. Transition & Animation Transition is a way to control how CSS value goes from state A to B and animation is a way to animate something with many keyframes stop. Any webpage comes to life when the animation is used well. Animation and transition can be used to enhance the experience and create effects that will further enhance the interaction with the site. Transform Another thing you can use to enhance any website experience is introducing 3D effects. Transform is often something you need to animate and transition which makes transform go hand in hand with CSS animation and transition. I have never worked on a website that did not require me to transform something which is why I believe is something you really need to understand your way around it. Preprocessors I love SASS and I am yet to work in a place or project that did not use pre-processors. CSS Preprocessors allows you access to concepts like mixins, functions, and modularity with the ability to import and compose style. Preprocessors are powerful and let you use things in CSS you wish existed natively. There are also other preprocessors besides SASS like LESS & Stylus and they all promise about the same things with some syntax changes. Whichever you pick, it is important to introduce a preprocessor to your website styling routine as a very important tool to help you create a complex style with much less CSS. CSS Houdini CSS Houdini is a low-level API that lets you get access to the CSS engine which allows you to extend CSS with features you want. Instead of waiting for a specific feature to existing or be supported in the browser, you can introduce it yourself. It is much faster to parse as it does not wait for the browser render cycle. It exposes these things called Worklets which you can use to modularize CSS and remove the need for preprocessors for modular CSS code. It is a really advanced concept in a simple API that turns you into a magician like a Harry Houdini. CSS Architecture It is easy to introduce conflicts when writing CSS as your app grows bigger. Eventually, you need to make sense out of your CSS alone, and thatâ€™s when you need to adopt a technique to help you with it and remain consistent. It must be something you like or the team agrees with, here some examples: BEM â€” Block Element Modifiers OOCSS â€” Object-Oriented CSS SMACSS â€” Scalable Modular Architecture for CSS All these options are simple CSS structure style guides you can follow to better structure your selectors and style in general. You can even come up with your own to fit your liking. This is totally optional concept to adopt but it is worth mentioning in case you come across things like this out there. Conclusion Mastering CSS is a continuous process and requires a lot of practice. It just has no effect to master these concepts without continuously trying to use them and expose you to various projects where you can practice. I recommend small CSS projects just so you can sharpen your knowledge about particular areas like devs do on CodePen. Mastering CSS is about having fun and utilizing your creativity and your power to visualize things or bring them to life. I love CSS and there is nothing I enjoy writing about more. To watch me write CSS check my Youtube channel or my website. 10 HTML things to Master as an Experienced Web Developer10 Things to Master About Javascript Before You Call Yourself a Pro Blog & Youtube Channel | Web, UI, Software Development & Developer's Career Tips Blog & Youtube Channel | Web, UI, Software Development & Developer's Career Tips Blog & Youtube Channel | Web, UI, Software Development & Developer's Career Tipsbeforesemicolon.com
Experience in software engineering comes with the number of years you are in the profession. The statement is true to a certain extent. However, the number of years in a job does not make you gain the right experience by default. I have seen people behave amateur like, even after working in their job for a good number of years. They hardly learn the basics of software development. Their growth stagnates after an initial couple of years, but they do not understand what they are doing wrong. At the same time, I have worked with developers with just a couple of years of experience who show an incredible amount of growth potential. They possess the right attitude and know-how to avoid incompetent behavior. Based on certain traits developers exhibit, you can easily find out who is experienced and who is not. Letâ€™s dive into seven signs of an inexperienced programmer that every software engineer should be aware of to avoid making similar mistakes that can hinder their career progression. 1. Creates large pull requests Have you ever got a code review request with so many changes in it that you donâ€™t feel like reviewing it? Yes, thatâ€™s precisely what inexperienced developers do. They will bunch together a lot of changes in one single pull request. On top of that, they will expect you to prioritize their code review. I have seen this habit of creating big pull requests with many senior developers too. They will code for days without feedback. When you finally review their code, they would have already built the whole functionality around it. Thus any review comment you give necessitates significant changes. When I get such pull requests, my first reaction is to return it to the developer to break it down into smaller, logically divided PRs. I often just put comments in the first issue I find and send it back to the developer. If I feel incredibly generous, I will ask them to set up a call and review the code live. What you can do: Create smaller pull requests. As a good practice, never leave a day's work without checking it in. Never check-in code that does not compile or breaks the build. 2. Writes spaghetti code Inexperienced developers write the exact opposite of beautiful code. Their code will be all tangled and scattered across all over the place in the codebase. If you ever try to read the spaghetti code, you will constantly get lost in it. You will forget where you started, what you are looking for, and what exactly the code is trying to do. With experience, the developer should know how to plan their coding. Unless it is a straightforward functionality, put your understanding and the flow on a paper first. Do a dry run to visualize it end to end. Once you are crystal clear about the changes, then start on the implementation part. If you do not follow the above process, you will have pain reading your own code. It will be hard for yourself and the whole team to troubleshoot or enhance the piece of the puzzle you wrote as code. What you can do: Have a clear understanding of the feature before you start to implement it. You can ask as many questions as you want to have a clear idea of the requirement. Keep your code simple and well structured. Your teammates should be able to read the code and understand the intended use of it. 3. Tries to work on a lot of tasks at the same time Inexperienced developers do not know where to start a task, how to proceed, and when to call it done. They try to solve a lot of stuff at the same time. They have no clue how to chunk a big task into smaller logical divisions to make it easy for implementation. If you assign them a task, they will jump into coding immediately without verifying with you if they even understood the ask. Neither will they review their progress with you to make sure they are on track. They will get back to you only once they think they are done. By that time, you can only pray to have an accurate implementation for your requirement. Another sign of inexperience is that such developers put their hands in too many things simultaneously. They will pick up tasks from unrelated features, volunteer themselves to troubleshoot production issues and promise to help others in the team. In the end, these developers do not deliver any of the committed tasks in their entirety. This attitude might be well-intentioned most of the time, but the result is disastrous for the team. Ultimately the team loses a lot of time and has to complete all the tasks on a war footing. What you can do: Focus on shipping small. Break your assignments into smaller logical chunks. Get it clarified and then deliver the smallest possible block of working functionality. Take one task assignment at a time and complete it. Commit to a new task only when the previous task is delivered as requested. 4. Full of arrogance Arrogance is a dead giveaway of an inexperienced developer. They are so full of themselves that they do not understand what they are doing wrong. You give them feedback on their code or presentation; they will take it as a personal comment on their ability. Many freshers show off their arrogance, mostly due to their ignorance. They are fresh out of college and are yet to understand that the professional world is entirely different than what they learn in college. The smart ones actually stay quiet and show a keen interest in learning the ways of the corporate culture. It is not just the freshers â€” some arrogant developers already have several years behind them in the software industry. It might be due to some of their professional achievements, or maybe they have not yet worked with people smarter and talented than them. In either case, arrogant behavior shows a clear indication that such developers lack the right experience. Their ego blinds them from learning the right approach towards their career. Eventually, no one likes to work with an arrogant team member. Once the growth slows down, the arrogant developer blames others for their failure. What you can do: Be humble in your approach. Politeness goes a long way in building a successful career in software development. Treat everyone, irrespective of their designation, with respect. Refrain from getting into an argument over disagreements. 5. Does not learn from their mistakes I always consider the feedback mechanism as one of the most effective tools for a software developer. Feedback helps us to understand our shortcomings and how to improve on them. A smart developer knows how to use the feedback to enhance their productivity. You can easily spot an inexperienced developer based on their reaction to constructive feedback. They will never accept any improvement comment on their performance. They even take the code review comments personally. Many years back, we had a teammate who wrote me a lengthy email on how I should review the code. He was infuriated with the review comments I gave for his PR. His main contention was I should not worry about the coding standard as he knows how to code. He wanted me just to review if the code meets the functional requirement. If the developer feels insulted because of review comments, that is a clear sign that they have not learned anything from their experience. They continue to work year over year with an incompetent attitude and wonder why no one ever values their contribution. What you can do: Keep a positive attitude towards every feedback. You can choose which one to accept and which one to discard. But give an impartial review before you decide to discard it. Have an open mind to learn from your mistakes. No one is right all the time. Use learnings to improve your performance. 6. Spends work hours on personal tasks There are always a few team members whom you can find doing their personal work during office hours. They will be browsing through social media, scanning through online shopping sites, or playing games. We had a team member who used to trade in the stock market during office hours. It had an adverse impact on his delivery as he was always focused on how his day trade is doing. Other team members raised concerns with this behavior as they were the ones who had to put in extra effort to meet the deadline. When the manager warned the said developer, he mended his way for a few days but got back to trading again. Eventually, the company had to let him go due to this behavior. Such behavior is unethical and shows clear signs of the developerâ€™s inexperience. It would be best if you were sincere towards the profession that helps you earn your livelihood. What you can do: Limit your personal tasks during office hours to a bare minimum. You can always take permission from the manager if you have to take a couple of hours off to attend to unavoidable personal issues. You can use your breaks to check your social media. Take your lunch to the desk and do stock trading during lunchtime, if you want to. 7. Often runs behind hyped technology A given sign of an inexperienced developer is how they run behind the hyped technology. You will find them always talking about the next big thing. As soon as there is a new fad in the market, the developer will drop the previous one and jump on the latest bandwagon. Inexperienced developers also master the art of doing tutorials. No doubt, tutorials are useful learning tools. But just following tutorials without any practical use for it is a waste of time. It might give a false sense of accomplishment, but the actual test of knowledge is in using it for real-world usage. You will seldom see the developer using the hyped technology or the knowledge acquired from tutorials to implement anything new. They just do it to satisfy their ego. Also, many inexperienced developers fall into this trap due to their fear of missing out. What you can do: Spend your time and effort to learn technologies that you can actually use to implement something in your workplace or your personal project. Use the learnings from tutorials and perform the hands-on practice. You will learn more from implementing something on your own than following any tutorial. Final Thoughts Inexperience programmers bring down the productivity of the whole team due to their inefficiency. Their incorrect approach towards the job makes them miss the opportunity to grow in a highly rewarding software career. It is wise to know the self-destructing attitude early in the career and avoid them. The more you get into the habit of the behaviors mentioned above, the more difficult it becomes to come out of it in your profession's later years. Thanks for reading the article. I hope you can avoid the pitfalls and achieve the career growth you desire. You might also like to read the following articles: 101 Hilarious Programming Jokes in 3 Minutes Do you understand the binary system? No worries, these jokes will still tickle your funny bones. levelup.gitconnected.com 5 Practical Programming Languages You Can Code Using Emojis Find out how many of them you have heard of before. levelup.gitconnected.com Level Up Coding Thanks for being a part of our community! Subscribe to our YouTube channel or join the Skilled.dev coding interview course. Coding Interview Questions + Land Your Dev Job | Skilled.dev The course to master the coding interview skilled.dev
Nov 17Â·2 min read Dear Readers, Today, Iâ€™m thrilled to announce that the Marker family is growing with the addition of a new blog, The Mobilist, about the future of batteries, electric cars, and driverless vehicles. Every so often, we write about something that our readers clearly have an insatiable appetite for. Thatâ€™s exactly what happened recently, when Medium editor-at-large Steve LeVine wrote a series of stories chronicling the fast-moving, ever-changing, high-stakes mobility industry unfolding before our eyes. More than a year ago, Steve began writing for Marker about Teslaâ€™s million-mile electric car battery and the EV industryâ€™s woman problem. More recently, he wrote about stealth startup QuantumScape, its claim of a battery breakthrough, and the skepticism around whether there was any leap at all. He chronicled the apparent EV ambivalence of incumbents like GM and the opposite, headlong approach of daredevils like Nikola. Steveâ€™s able to parse through hype versus actual progress because heâ€™s not just an enthusiast on this subject â€” heâ€™s also an expert. To write his third book, The Powerhouse: America, China, and the Great Battery War, he got permission to embed for two years at the federal governmentâ€™s secretive Argonne National Laboratory, where a team of electrochemical geniuses were squirreled away, trying to build the worldâ€™s first super-battery. Every week on The Mobilist, Steve will report on the business, science, technology, and geopolitics of advanced batteries, electric vehicles, and driverless mobility. The blog will be a space where inventors, entrepreneurs, investors, policy hands, and anyone interested in understanding the future of electric and automated vehicles can track developments large and small, meet the people responsible for this new age, and learn about the changes it will bring. For those of you who already enjoy Steveâ€™s wonderful in-depth features that appear all over Medium â€” whether heâ€™s writing about the radical plan to save a sinking city at GEN, alien geopolitics at OneZero, or anything related to economics or business at his primary home, Marker â€” not to worry, heâ€™ll continue taking these big swings. Just make sure you donâ€™t miss any of them by following Steve LeVine on Medium and following the blog, The Mobilist. Marker is in the fortunate position of serving Medium subscribers. We believe in the unique value of our platform and the journalism we can deliver. Weâ€™re excited to be able to bring you The Mobilist. Danielle SacksEditor-in-chief, Marker
For many people, the only matrix worth knowing includes Keanu Reeves but theyâ€™d be missing out on great tools to navigate the complex world of running a business. It takes passion to start a company but this same fire can blind entrepreneurs to the bigger picture. They canâ€™t see problems obvious to outsiders because they are so focused on their vision. Management consultants have made a $250bn industry out of pointing these white elephants out. Yet you donâ€™t need to spend money on expensive consultants to make use of their simple matrices. They are widely published and taught at business schools across the world. A matrix is just a grid used to categorize companies and tasks to make sure the appropriate strategy is taken. Using a matrix wonâ€™t magically make you a billionaire but it could save you a lot of wasted time. Iâ€™m covering 5 of the most trusted matrices used in the strategy consulting industry: BCG Matrix Ansoff Matrix GE-McKinsey 9 Box Matrix SWOT Matrix Eisenhower Matrix The Boston Consulting Group (BCG) has powered the growth of many of the worldâ€™s largest companies. In the 1970s, they created their devilishly simple growth-matrix. By using memorable characters for each square, itâ€™s become a mainstay for 50 years. The primary function is to remind you to focus on the areas which are growing fast and dominating. On the Y-axis is the growth rate of the market and on the X-axis is the companyâ€™s market share. Different strategies should be used depending on which square you are in. If you havenâ€™t started a company yet think about companies and divisions of companies that interest you. Most likely these will be the stars and the dogs will mainly be in the bankruptcy section. Question mark â€” Being in this quadrant is a perplexing place to be. If the market is growing fast, why donâ€™t you have a bigger share? You need to strategize carefully here and decide whether or not you can increase your market share to capture the growth. When you are losing out to the competitors, it might be better to call it quits. Star â€” Everybody thinks their company is a star but most of them are wrong. The star companies are what venture capitalists dream about at night and why they are willing to pump money into early-stage companies. When it works (think Uber and Airbnb), the value generated is colossal. If you have an idea that can dominate a high-growth industry then invest, invest, invest! Dog â€” Nothing screams dead end more than a low share of a low growth industry. For some time you might keep going but the dominant players will muscle you out eventually. Itâ€™s best to divest these ideas and focus on the stars and cash cows. Cash cow â€” The perfect place to be after building your company up to reduce your stress. Once you can dominate a mature industry, your concern is keeping your market share high and reaping the rewards. If itâ€™s too early for retirement, you can funnel the profits into new potential stars. Think of Coca-Cola as a great example, the core product is mature so they use the profit to invest in other products. Igor Ansoff is known as the father of strategic management. Over his career, he was critical in the foundation of several management schools as well as advising over a hundred companies. His work inspired much of the industry to create the kinds of generic frameworks you see here. His matrix, the Ansoff matrix, is about correctly defining your growth path. The Y-axis represents whether the market is established already and the X-axis is whether the product exists. The risk increases as you move from the top left to the bottom right. Too often you see companies trying to penetrate their market but being distracted by tactics from the other parts of the matrix. Market penetration â€” This path isnâ€™t what weâ€™d traditionally consider a startup, itâ€™s not so much about revolution as tactical warfare to increase market share. Options available include decreasing prices, increasing promotion, or acquisitions. Market development â€” Truly great companies have taken products that are successful in one segment and widened their audience through clever marketing. Take Facebook which started aimed at college students and kept bringing in new demographics until it took over the world. Product development â€” Google is an expert at product development. Through their search engine, all internet users were their market but they increased reliance through tools such as Google Maps and the G-Suite of office features. Diversification â€” This is the entrepreneurâ€™s dream, targeting a new market with a totally new product which makes it the riskiest strategy. Berkshire Hathaway is a great example where they keep adding different industries to their portfolio rather than stick to one market or product they know well. Like BCG, McKinsey is one of the worldâ€™s leading consulting firms. General Electric also shouldnâ€™t need any introduction as a mainstay of the Dow Jones list of largest industrial companies in the USA. In the 1970s, McKinsey was working with GE to understand their unruly kaleidoscope of different businesses. GE needed to make tough decisions to avoid being swamped but was bombarded with irrelevant statistics. McKinsey simplified the process by focusing on two key metrics: business unit strength and industry attractiveness. This matrix is especially relevant for companies with multiple products so entrepreneurs can decide where to focus their efforts. As we go up in the matrix, the industry attractiveness increases. Unintuitively, they decided to make the business unit strength go backward so the further left of the grid, the stronger the business unit. Depending on where the business unit ended up there were 3 choices: Divest/harvest â€” When you arenâ€™t strong in an undesirable industry, itâ€™s time to exit unless itâ€™s still bringing in some profits. These areas should never be invested in when there are other options available. Selective/earnings â€” As in the traffic light system, these are the proceed with caution units. They need to be monitored the most carefully to see if they start veering towards the invest or divest sides. If you decide not to invest more, you can still use these areas as short-term plays for income to fund other areas of the business. Invest/grow â€” These are the fantasies when you are doing well in an attractive industry. These should be the focus of the company and where your investment is concentrated. This is the classic matrix most people will first think of but itâ€™s still relevant today. Itâ€™s easy to fall into the trap of unicorns and rainbows early on but paying attention to threats and weaknesses can bring some home truths. Itâ€™s worth completing this yourself and then asking others to do the same exercise for your business. You may find what you thought you knew about your business isnâ€™t a shared opinion. Strengths â€” The bit youâ€™ll love to fill out. What makes your company special and what are you good at. Weaknesses â€” Be honest here, there's no point in pretending your company is the best at everything. You have two choices: either accept your weaknesses and narrow your focus or fight them tooth and nail to broaden your appeal. Opportunities â€” What are your competitors weak at? What needs do customers have that no one is fulfilling? Itâ€™s worth thinking about wider trends here. For example, the massive shift to working from home was an opportunity Hopin seized and Slack was lackluster. Threats â€” What are your competitors good at and getting better at relative to you? Again the focus should be external. The tech industry needs to pay particular attention to regulations around privacy. The potential for further policies like GDPR could make business conditions more difficult in the future. Effective time management is one of the toughest skills for entrepreneurs as all kinds of tasks are thrown their way. The curse of many is believing only they can do something in a way that is acceptable which leads to a short road to burnout. Focusing on the high-value tasks protects your mental wellbeing and leads to the best outcomes for your business. This matrix was created by the 34th President of the United States, Dwight D. Eisenhower who as you can imagine had many competing tasks in 1953. While his legacy is outshone by his successor, John F. Kennedy, his matrix lives on and is still taught in workshops across the corporate world. Do â€” If there is something important and urgent to do then forget the rest of the list and get this down ASAP. The last day for tax returns is not the time to be dallying about. Delegate â€” This is the hardest task for many entrepreneurs who feel only they can do something well. By taking too much on, you burn yourself out for the important tasks. Learn to delegate. Decide â€” Important tasks that arenâ€™t urgent are usually forgotten. Get around this by scheduling time in advance to get onto it and only an important and urgent task can delay this. Delete â€” If something is not urgent or important, why is it even being done? Some say if it takes under 2 minutes to do it but this doesnâ€™t account for switching costs and the cumulative effect. Be brutal. This is an introduction to the 5 matrices, there is plenty more information out there if you want to use one in-depth with your own company. Test out the tools once with your business and see which matrix triggers the most questions in your mind. The objective here is to challenge your status quo mindset and this might be a bit uncomfortable. Good luck and I hope this will help you plan! Love this? Learn more about me and subscribe here
As far as audacious attacks on the importance of an elite university education go, Peter Thiel may take the crown. In 2010, he launched The Thiel Fellowship which encourages visionary students to drop out of college in return for his mentorship. Oh, and a little sum of $100,000 to get whatever venture inspires them off the runway. Amongst the alumni are the youngest person to produce nuclear fusion and the co-creator of the Ethereum. Itâ€™s been controversial, to say the least. Larry Summers, the former head of the US Department of the Treasury had this to say: â€œI think the single most misdirected bit of philanthropy in this decade is Peter Thielâ€™s special program to bribe people to drop out of college.â€ Yet in December 2020, Austin Russell became the second fellowship alumni to become a billionaire when his company Luminar went public. At just 25, he joins the exclusive club of the Facebook, Snapchat, and Stripe founders to get there before 30. He is the worldâ€™s youngest billionaire with Kylie Jenner currently disqualified. He is far less famous so itâ€™s time to illuminate the story of the prodigy. Story based on these sources. Nerd first, entrepreneur second It would be an outright lie to look at Austinâ€™s childhood and declare there were no signs he would achieve anything out of the ordinary. Look at what he worked out at such young ages; I can confirm at 28, I have done none of these things. At 2 years old, he memorized the periodic table. At 10 years old, he converted his Nintendo DS into a mobile phone. At 11 years old, he built a research lab in his family garage. At 13 years old, he patented technology to recycle sprinkler water. Austin was obsessed with learning new things and spent lots of his free time independently studying. After building his own supercomputers, he landed on his true passion. Lasers. The University of California gave him access to their laser institute when he was 14 impressed by his intellect. He focused on lidar technology which bounces lasers off objects to map the surroundings. Why this? Because he thought others were looking at it completely incorrectly. The technology is crucial in self-driving cars to make them as safe as possible. It doesnâ€™t sound right that a teenager was the one pushing forward such an obviously needed system and Austin would agree with you. â€œIt should not be a 16- or 17-year-old and then subsequently a 25-year-old that can build a business like this. Weâ€™ve been able to accelerate this because no one has really done this before.â€ Austin was a nerd with a mission who believed his ideas could make the world a safer place. Itâ€™s why he succeeded over the huge companies who focused on the balance sheet. Despite his founding Luminar in his garage, he still went to Stanford to study physics. Yet he jumped at the chance to bring his dream into reality when the opportunity came to take Peter Thiel as his mentor through the fellowship. He knew science but now he could nerd out on entrepreneurship. Channeling the founders of Silicon Valley Austin Rusell perfectly illustrates the advice that podcast legend Guy Raz gave in an interview with Tim Ferriss. Silicon Valley doesnâ€™t exist because of Amazon, Apple, or Facebook. The gold rush is what brought people to the area who came from all over the world. Few people actually got rich through striking gold but several entrepreneurs did. These were the people who saw an opportunity to service those blinded by the gold rush. Wells Fargo â€” Started as a business to transport gold for miners from California to the East Coast to sell. They needed to handle money as part of this which would go on to be a $122bn business. Levi Strauss â€” Created work pants for miners of high quality, this made him a millionaire. Later in his career, he made the first denim jeans and the rest is history. Right now tech has made several industries seem like gold rushes. The race to dominate the self-driving car market is fierce because it is estimated to be worth $221 bn in 2025. All the major competitors have almost unlimited pockets and could muscle Russell out if they wanted to. But Austin isnâ€™t competing with them, he is servicing them. Heâ€™s feeding the giants who have big appetites and already has production orders from Volvo, Daimler, and Mobileye. Luminar is worth around $8bn which sounds big but is little more than a pest if he tried to fight them head-on. Austin has kept his company laser-focused on lidar and if he achieves his dream, his company could be worth 10 times that in 10 years' time. â€œWhen this becomes a new, modern safety technology on vehicles thatâ€™s integrated on every vehicle globally produced, thatâ€™s when Iâ€™d firmly say that weâ€™ve accomplished the goals that we set.â€ Somehow he kept control Self-made tech founders rarely own a significant stake in their company by the time IPO rolls around. Most business insiders tell them to raise money through selling equity rather than relying on banks. Equity holders have a vested interest in the company succeeding whereas banks can seize the companyâ€™s assets to make their money back. It starts with seed or angel investors who take a chunk to get the company off the ground. Then another round with another venture capitalist and repeat. Itâ€™s not uncommon for this to go on for 5 rounds, leaving the founder with 10% at the end. Luminar had funding from Peter Thiel, Volvo Cars Tech Fund, Alec Goes, and Dean Metropoulos. Yet Austin still has a third of the equity! â€œIâ€™m still relatively young, but â€¦ a lot of blood, sweat and tears have gone into it. And I was fortunate enough to be able to retain a good enough stake.â€ He is far too modest. If his billionaire backers are to be believed, fortune played much less a part of it than he believes. Alec Gores thought Austin understood the listing process better than â€œsixty-year-old guys whoâ€™ve been in the business for 40 yearsâ€. Austin is the genius who has pushed lidar technology forward but itâ€™s even more impressive he kept the rights to the huge value he has created. The science nerd applied his brainpower to the startup world and has become an entrepreneur nerd. Iâ€™m sure young tech hopefuls will be studying him for decades to come. Love this? Learn more about me and subscribe here
Gaurav Dec 18Â·6 min read All autonomous vehicles (AV) use a collection of hardware sensors to identify the physical environment surrounding them. The hardware sensors include camera or a collection of cameras strategically placed around the body of the vehicle to capture 2D vision data, and some form of RADAR placed on top of the vehicle to capture 3D position data. There are a few vendors like Tesla who believe vision data is enough for a vehicle to identify its environment. Other vendors use LIDAR sensors to capture 3D position data for the objects surrounding the vehicle. Fusion of 2D vision data and 3D position data gives an AV system precise understanding of its surrounding environment. Developing precise understanding of its surrounding environment is the first component of an AV system. Image below shows all the important components of an AV system. Sensor Fusion Computer vision is a branch of computer science that uses camera or a combination of cameras to process 2D visual data. This allows computers to identify cars, trucks, cyclists, pedestrians, roads, lanes marking, traffic signals, building, horizon. Camera data is 2D in nature, and it does not provide distance of an object. Although focal length and aperture of a camera sensor can be used to approximate the depth of an object, it will not be precise because there is intrinsic loss of information when a 3D scene is captured by a camera sensor onto a 2D plane. Radar technology has been in use in places like air traffic management to locate flying objects. Radar can be used to estimate location and speed of an object. It cannot be used to classify an object as a car, person, traffic signal, or a building because it has low precision. Lidar is a hardware that uses laser technology to estimate the position and speed of objects in the surrounding. Lidar is able to generate point cloud of upto 2 million points per second. Due to higher accuracy Lidar can be used to measure shape and contour of an object. While RGB data from camera is lacking depth information, point cloud data generated by Lidar lacks texture and color information present in the RGB data. For example in a point cloud data, contour of a pedestrian 20 feet away might be a blob of points that can be identified as multiple different objects as shown in rendering of point cloud below. On the other hand a shadow ridden low quality partial visual information gives a hint that the object is a person as shown in image from a camera below. When fusion of visual data and point cloud data is performed, the result is a perception model of the surrounding environment that retains both the visual features and precise 3D positions. In addition of accuracy, it helps to provide redundancy in case of sensor failure. Fusion of camera sensor data and Lidar point cloud data involves 2D-to-3D and 3D-to-2D projection mapping. 3D-to-2D Projection Hardware Setup We start with the most comprehensive open source dataset made available by Motional: nuScenes dataset. It includes six cameras three in front and three in back. The capture frequency is 12 Hz. The pixel resolution is 1600x900. The image encoding is one byte per pixel as jpeg. The camera data is generated at 1.7MB/s per camera footage. One Lidar is placed on top of the car. The capture frequency for lidar is 20 Hz. It has 32 channels (beams). Its vertical field of view is -30 degrees to +10 degrees. Its range is 100 meters. Its accuracy is 2 cm. It can collect upto 1.4 million points per second. Itâ€™s output format is compressed .pcd. Lidarâ€™s output data rate is 26.7MB/s (20byte*1400000). Important Links Dataset Page: https://www.nuscenes.org/overview Paper URL: https://arxiv.org/pdf/1903.11027.pdf Devkit URL: https://github.com/nutonomy/nuscenes-devkit Understanding Reference Frames & Coordinate Systems In order to synchronize the sensors one has to define a world (global) coordinate system. Every sensor instrument has itâ€™s own reference frame & coordinate system. Lidar has itâ€™s own reference frame & coordinate system L1, Each camera has itâ€™s own reference frame & coordinate system C1, C2, C3, C4, C5, C6. IMU has itâ€™s own reference frame & coordinate system I1. For the purposes of this discussion here, ego vehicle reference frame is the same as lidar reference frame. Define world reference frame & coordinate system World reference frame (W1) is global reference frame. For example one can select lidarâ€™s first frame as center (0, 0, 0) of the world coordinate system. Subsequently every frame from lidar will be converted back to world coordinate system. Camera matrices M1, M2, M3, M4, M5, M6 will be formulated to convert from each camera coordinate system C1, C2, C3, C4, C5, C6 back to world coordinate system W1. Convert 3D Point Cloud Data to World Coordinate System Each frame in lidar reference frame (L1) will be converted back to world coordinate system by multiplication with ego frame translation & rotation matrices. Convert from World Coordinate System to Camera Coordinate System Next step is to convert data from world reference frame to camera reference frame by multiplication with camera rotation & translation matrices. Convert from 3D Camera Coordinate System to 2D Camera Frame Once the data is in camera reference frame it needs to be projected from 3D camera reference frame to 2D camera sensor plane. This is achieved by multiplication with camera intrinsic matrix. Result: Accurate Annotations As shown in the video below the fusion of lidar point cloud data, and camera data allows annotators to utilize both visual information & depth information to create more accurate annotations. 10X Speedup in labeling: Interpolation of annotation between frames One of the the most challenging task in development of autonomous vehicle systems is to manage humongous volumes of data used for training the neural networks. As the classification and detection accuracy improves the amount of new training data needed to further improve the performance grows exponentially. To increase speed and reduce cost of annotating new training data, annotation tool can provide automation. One example of automation is interpolation of annotations between frames in LIDAR point cloud tool. The sensor data being generated has high degree of accuracy. The lidar point cloud data is accurate to plus-or-minus 2 cms. The camera data is recorded at 1600 x 900 pixel resolution. High accuracy level allows annotation tool to provide semi-automatic techniques to reduce the manual effort required in data labeling. As an example consider annotation of 10 consecutive frames of point cloud data. Each lidar frame is accompanied by six camera frames. Human annotators use the annotation tool to fit a truck inside a cuboid in frame 1 and frame 10. Based on position of the cuboid in frame #1, and frame #10, the annotation tool can automatically interpolate position of the cuboid from frame 2 to frame 9. This significantly reduces the amount of work required from the human labellers. Such semi-automatic techniques can boost productivity, increase speed, and reduce the cost of building AI. https://www.trainingdata.io/blog/what-is-sensor-fusion-for-lidar-3d-point-cloud-data/
German Sharabok Sep 1Â·6 min read Almost every single company working on self-driving cars right now uses LIDAR. Uber, Waymo, and Toyota all use it, but not Tesla. I want to go over what the two competing technologies have to offer and what we should expect from self-driving cars in the future. Lidar VS Vision Lidar is a method of measuring distance by shooting lasers and detecting how much time they take to return. The idea is similar to Radar but instead of radio waves, we use lasers. The technology is extremely accurate at detecting objects even up to millimeters. Computer Vision is a field of Artificial Intelligence that trains computers to understand the visual world. This is basically reverse engineering human vision. Teslaâ€™s Vision Tesla has been heavily relying on Vision and going against LIDAR sensors. At the same time, all the other companies use Lidar and do not seem to care. Elon Musk even said: LIDAR is a foolâ€™s errandâ€¦ and anyone relying on LIDAR is doomed. â€” Elon Musk If you would like to see all Elonâ€™s thoughts on the technology choice, make sure to check his talk at Tesla Autonomy Day. Cost The most apparent reason for Tesla to take a different route is the cost. The cost of placing a single LIDAR device on a car is somewhere around $10,000. Google with its Waymo project has been able to slightly decrease the number by introducing mass production. However, the cost is still rather significant. Tesla is highly focused on costs and making sure the cars are affordable. Adding the prices of a LIDAR on top of the already expensive car is quite significant. Application to real roads One of the most important point and the one emphasized highly in the conference is the correlation with human vision. As humans, we do not throw lasers in every direction to be able to drive a car. Neither should the cars, as mentioned by Elon Musk. Everything we see on the road is full of visual information. All the signs, turns, intersections are there to help us navigate. All of these are stationary objects and it is great that LIDAR is so accurate in detecting them. Issues start to arise when moving objects appear on the road. Humans, dogs, flying plastic bags, are all objects we frequently encounter on the road. LIDAR is not able to detect how they are moving or even what those objects are. LIDAR cannot differentiate a road bump from a plastic bag. This is example was mentioned in the conference and is extremely important to take into consideration. If we are driving at high speeds on a highway and there is a plastic bag, we do not need to make a quick stop. It will not be much of an issue if we hit it. Now, if the car stops, that's where real dangers come in. Cars behind will probably not be able to react so quickly to our stop in the middle of the road. Such situations further demonstrate the attention to detail required when making self-driving cars. Tesla made it clear that their system of cameras and radar is able to detect what an object is. The radar looking forward is able to quickly tell if there is anything problematic ahead. Once an object comes into sight, cameras will decide what the object is and then the car can react to the situation. Adaptation Another essential takeaway from Autonomy Day and other interviews with Elon Musk is that the system is made to adapt. They talked a lot about the Neural Network used and how the system is able to use the data provided to make rational decisions. One of the big issues with Teslaâ€™s competitors is the lack of such adaptiveness. Most of these systems either rely heavily on high-accuracy maps with road lines or were never tested on real roads. Yes, we have seen Waymo drive around cities. However, only the large roads with highly efficient maps. The lighting, weather conditions, and traffic are ideal in those demos. Most people do not drive on such roads every single day. Smaller roads, with unexpected turns and lanes that change in size, are quite a bit more common. Plus, Tesla is an actual car that you can buy. People have driven over a billion miles on Tesla cars, while Waymo was only tested on around ten million miles. The amount of difficult and unpredictable road data that Tesla was able to accumulate is invaluable. That is how the system learns and keeps getting better. Such a concept is actually quite promising since customers actually see constant improvements. Accuracy Another interesting point can be deduced from a research paper published by Cornell University. The paper discussed how stereos cameras can be used to generate a 3D map nearly as accurate as a LIDAR map. We can conclude from the paper that other than spending $7,500 on a LIDAR device, you could get a few cameras that only cost $5 and get nearly the same accuracy. So when the guys at Tesla say that such hardware will become outdated soon, they could have a point. Final Thoughts With how much money is poured into the field of self-driving cars and the constant competition, we can be quite optimistic that such cars are coming. Whether Tesla will be the company that does this, we cannot know and might not even need to. Actually, there could potentially be several ways of developing self-driving cars. We might even end up seeing the combination of the two, which would not be that surprising. When it comes to Tesla, we could actually purchase such a car. They are driven all over the world and the progress could actually be observed in real-time. You cannot buy a Waymo or an Uber car. The reasons for Teslaâ€™s decision were covered here, however it does not mean we should all bet on Tesla. Maybe some other company manages to create a self-driving vehicle first. What we do know, is that some company will do it and we should pay close attention to everything happening in the field. References: [1] Wang, Y., Chao, W. L., Garg, D., Hariharan, B., Campbell, M., & Weinberger, K. Q. (2019). Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 8445â€“8453). [2] AutoPilot Review. (2019, April 27). Elon Musk on Cameras vs LiDAR for Self Driving and Autonomous Cars [Video]. YouTube. https://youtu.be/HM23sjhtk4Q [3] Hawkins, A. J. (2019). Waymo will sell LIDAR to customers who wonâ€™t compete with its robot taxi business. Retrieved September 07, 2020, from https://www.theverge.com/2019/3/6/18252561/waymo-sell-lidar-laser-sensor-av-customer-robot-taxi-competition
Nov 29Â·10 min read Object detection in crowded scenes is challenging. When objects gather, they tend to overlap largely with each other, leading to occlusions. Occlusion caused by objects of the same class is called intra-class occlusion, also referred to as crowd occlusion. Object detectors need to determine the locations of different objects in the crowd and accurately delineate their boundaries. Many cases are quite challenging even for human annotators. In autonomous driving, there are at least several scenarios where we have to deal with object detection in crowded scenes: vehicle detection in parking lots or city streets, and pedestrian detection in intersections. The challenges of crowd scenes Crowd occlusion is challenging for object detection for several reasons. When objects overlap heavily with each other, semantic features of different instances also interweave and make sectors difficult to discriminate instance boundaries. Even though detectors successfully differentiate and detect instances, they may still be suppressed by non-maximum suppression (NMS). The vanilla greedy NMS (and even its improved versions soft-NMS and matrix-NMS) follow one implicit hypothesis that detection boxes with high overlaps correspond to the same object and thus need to be grouped and reduced to one box. This assumption works reasonably well, however, it does not hold anymore in a crowded scene where objects heavily occlude and overlap with each other by definition. Many datasets containing or specialized in crowd detection (CrowdHuman, CityPersons, WiderPersons, etc) and real-world applications (autonomous driving) require amodal object detection. That means the detector predicts a box covering the entire object â€” even if it is not fully visible in the image, for example, due to partial occlusion by other objects. It is actually how humans perceive the environment. This further complicates object detection, as the overlap between amodal bounding boxes are usually much higher than visible bounding boxes. The Dilemma of NMS The most critical challenge of crowd detection is perhaps NMS. As we see below, almost all existing works in crowd object detection works around or directly on NMS. Although most parts in modern object detectors are end-to-end trainable, NMS remains one of the last human crafted components. NMS greedily selects the bounding box with the highest score and suppresses ones that have a high overlap with it. The overlap is measured by comparing Intersection-over-Union (IoU) threshold to a predefined threshold, usually ranging from 0.3 to 0.5. NMS is quite sensitive to the thresholdâ€”a higher threshold means less suppression power and may bring more FP, and a lower threshold means more aggressive suppression and may lead to missed detections. Modified loss for tighter boxes To reduce the sensitivity of detection results to the NMS threshold, some studies propose new losses to ensure tighter prediction. They propose additional penalties to produce more compact bounding boxes and become less sensitive to NMS. Implicitly they impose additional penalties to bbox which appear in the middle of the two pedestrians, addressing one of the issues for crowd object detection. RepLoss (Repulsion Loss: Detecting Pedestrians in a Crowd, CVPR 2018) proposes a novel bbox regression loss specifically designed for crowd scenes. This not only pushes each proposal to reach its designed target but also keep it away from other surrounding objects. The RepGT loss penalizes overlap with non-target GT object. RepBox loss encourages that the IoU region between two predicted boxes with different designated targets needs to be small. This means the predicted boxes with diff regression targets are less likely to be merged into one after NMS. AggLoss (Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd, ECCV 2018) proposes a new loss term to enforce proposals locate compactly to the designated ground truth object. Concretely, it enforces SL1 loss between the avg prediction of the anchors and the corresponding GT. I am actually a little bit puzzled by the formulation of Agg Loss in the original paper. To me, it would be more reasonable to propose a a consistency loss penalizing different predictions from different anchors matched to the same groudtruth, as shown in the diagram above. Both RepLoss and AggLoss encourages a tighter bbox through modification of loss function. However, sometimes even tighter detection results will not help in a highly crowded scenario where NMS sets an upper limit for the performance of an object detector. For example, in CrowdHuman dataset, nearly 10% of the groundtruth instances will be missed in detection if applying the standard NMS with IoU threshold of 0.5 (source). In other words, even a perfect detector (100% recall and precision with perfectly tight bounding boxes) will still fail to detect all the instances after NMS. Occlusion-aware NMS To achieve better performance of object detection in a crowded scene, the bottleneck of NMS needs to be addressed in a more principled way. Many papers strive to redesign NMS to handle occlusion cases more appropriately while not degrading performance for normal scenarios. Adaptive NMS (Refining Pedestrian Detection in a Crowd, CVPR 2019 oral) notes that the dilemma of NMS is caused by the forced selection of a single threshold. The adaptive NMS proposed by the paper applies a dynamic suppression strategy where the threshold rises as instances gather and occlude each other and decays when instances appear separately. It predicts the object density score (or crowdedness) online with a separate subnet and uses it as an adaptive threshold for NMS. For objects in a high object density area, use the dynamic threshold of max(fixed_threshold, crowdedness) to perform NMS. This adaptively adjusts up the threshold in crowded regions with a high crowdedness score. That said, crowdedness estimation is a challenging task and there are often inconsistencies between the round truth density and the IoU of the predicted bounding boxes. Double Anchor (Double Anchor R-CNN for Human Detection in a Crowd, Arxiv 2019) is developed to capture body and head parts in pairs. This paper addresses in particular human detection and the intuition behind the paper is simple: compared with the human body, the head usually has a smaller scale, less overlap, and a better view in real-world images, and thus is more robust to pose variations and crowd occlusions. The network is based on Faster RCNN framework and predicts a head box and a body box, each with a confidence score. Then a joint NMS method uses a weighted score from both head bbox score and body bbox score, and boxes with a lower score will be suppressed if either the body overlap or the head overlap exceeds a certain threshold. In my opinion, it may be a better idea to use head overlap only to perform the NMS, given the intuition of the paper that head parts are less prone to occlusion. Unfortunately the Double Anchor paper did not provide ablation studies on this. The intuition of Double Anchor is great but the notion of a head box and a body box is only limited to the context of pedestrian detection. Almost by definition, visible parts of an object suffer much less from occlusion. Can we make the method of Double Anchor more general by redefining the body box and head box as the amodal full box (enclosing the occluded extent) and the visible box? R2-NMS (CVPR 2020) and VG-NMS (NeurIPS 2019 workshop) did exactly that. These two roughly contemporary studies both predict both full bbox and visible region and use the visible region for NMS. R2-NMS focuses on crowded pedestrian detection, and uses a Faster-RCNN-like two-stage object detection framework, while VG-NMS focuses more on crowded vehicle detection in parking lots or urban scenes, and uses an SDD-like single-stage object detection framework. CrowdDet (CVPR 2020 oral) predicts multiple detections per anchor for crowd detection. The predicted boxes from the same anchor are expected to infer the same set of instances, rather than distinguishing individual instances as in the single prediction paradigm in most object detectors. A modified set NMS largely follows the normal NMS procedure but skips suppression for prediction coming from the same anchor. As each anchor now predicts a set of object instances without any particular order, the loss needs to be modified to measure the distance between two sets. EMD (earth moverâ€™s distance) loss is used to select the best matching one with the smallest loss for all permutations of matching. It also adds dummy boxes whose class label is regarded as background and mask out regression loss. These ideas actually closely resemble many of the paradigm-shifting DETR paper, which I will later write a summary about. The above contrived pathological scenario illustrates the fundamental limitation of single-prediction paradigm. In heavily crowded scens, it is intrinsically difficult to predict a single instance from a single anchor as as the proposals share very similar feature. Moreover, after vanilla NMS, it is very likely that only one prediction survives. Although in autonomous driving and many real-world application, the above contrived case is very unlikely to happen, this is indeed one corner case that modern object detectors are not designed cannot handle, be it one-stage or two-stage, anchor-based or anchor-free. Generally speaking, object detection is to tell where the object is, and how big it is. However, this case has both center location collision (cannot be handled via center heatmap) and size collision (cannot be handled via multi-scale feature maps) . NMS-free Now we know NMS is the necessary evil of object detectors, why not get rid of it? There has indeed been a recent wave of anchor-free and NMS-free object detectors, and among the most representative works are CenterNet (Arxiv 2019) and FCOS (ICCV 2019) for general object detection, and CSP (CVPR 2019) dedicated to pedestrian detection. Although anchor free methods can eliminate NMS in the traditional sense, a local maximum has to be selected in the predicted center heatmap. These anchor-free (or single-anchor, depends on how you see it) methods in a way alleviate the issues caused by NMS for the anchor-based approach. In my opinion, the performance of an anchor-free method still largely depends on how to formulate the detection problem. The correct way to go still seems to be that of the R2-NMS/VG-NMS method, or CrowdDet method. Densely packed scenes without occlusion In all the above studies, we assume crowded scenes with occlusions. A related but slightly different field is object detection in densely packed scenes, such as in a shelf display. In such retail scenes, many objects appear similar or identical and are often positioned in close proximity, but without too much occlusion. General object detectors would also fail miserably here. SKU110K (CVPR 2019) proposed an EM-merger algorithm to replace NMS to filter, merge and split overlapping detection clusters to resolve a single detection per object. Perhaps a hindsight, but it appears to me that object detection in non-occlusion crowded scene can be quite readily solved by a key-point based anchor-free method such as CenterNet. Similar to crowd occlusion cases, densely packed scenes also requires tighter bounding box, otherwise NMS will inaccurately suppress true positives and spares false positives.So methods that encourages a tigher prediction box such as RepLoss and AggLoss should also help. Takeaways NMS is an important building block for modern object detectors. NMS has a fundamental assumption that high overlapping predictions need to be suppressed but one. However, crowded scenes by definition challenge this assumption. Almost all existing works in crowd object detection works around or directly on NMS. RepLoss and AggLoss encourage tighter bounding box prediction, which alleviates the sensitivity to the NMS threshold. Adaptive NMS dynamically predicts the NMS threshold to be used in inference. Joint-NMS (Double Anchor), R2-NMS, VG-NMS predicts two boxes per instance and use the box less prone to occlusion during NMS. Set NMS (CrowdDet) fundamentally addresses corner cases with location and scale collision that most modern object detectors are not designed to handle. The idea of predicting a set of bounding boxes is similar to that of DETR and could be the future of object detection beyond the current paradigm of dense prediction. In practice, the method used by R2-NMS and VG-NMS seems to be the most practical way to go to handle occlusion and should handle most crowd occlusion cases in reality. Visualization of detection results before NMS also appears to be a powerful debugging tool. References RepLoss: Repulsion Loss: Detecting Pedestrians in a Crowd, CVPR 2018 AggLoss: Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd, ECCV 2018 Adaptive NMS: Refining Pedestrian Detection in a Crowd, CVPR 2019 oral Double Anchor R-CNN for Human Detection in a Crowd R2-NMS: NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing, CVPR 2020 VG-NMS: Visibility Guided NMS: Efficient Boosting of Amodal Object Detection in Crowded Traffic Scenes, NeurIPS 2019 workshop CrowdDet: Detection in Crowded Scenes: One Proposal, Multiple Predictions, CVPR 2020 oral SKU110K: Precise Detection in Densely Packed Scenes, CVPR 2019 CSP: High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection, CVPR 2019 Soft-NMS â€” Improving Object Detection With One Line of Code, ICCV 2017 CrowdHuman: A Benchmark for Detecting Human in a Crowd, Arxiv 2018 CenterNet: Objects as points, Arxiv 2019 FCOS: Fully Convolutional One-Stage Object Detection, ICCV 2019 DETR: End-to-End Object Detection with Transformers, ECCV 2020 oral
Yu Zhou 2 days agoÂ·5 min read Spark is a popular choice for data engineering work because Spark promises extremely fast processing power on a huge volume of distributed data. Like the game between cats and mice, ever-growing needs for deeper data processing can still overwhelm Spark, making Spark pipelines time consuming and painful to manage. My day job involves processing PTs of data, and our team maintains a nightly Spark pipeline in AWS EMR for that purpose. Every couple of months, we need Spark performance tuning. With hundreds of knobs to turn, it is always an uphill battle to squeeze more out of Spark pipelines. In this blog, I want to highlight three overlooked methods to optimize Spark pipelines: 1. tidy up pipeline output; 2. balance workload via randomization; 3. replace joins with window functions. 0. What does my Spark pipeline look like? Before I go over my three methods, I want to describe my Spark pipeline first. My Spark pipeline includes thousands of independent Spark applications. Each Spark application intakes a customerâ€™s data in S3, makes a long chain of transformations, and finally writes outputs into S3. These Spark applications are all in Pyspark in AWS EMR. We concurrently run a hundred Spark applications from these thousands. On bad days, it takes up to 24 hours for the whole work. Itâ€™s worthwhile to point out that one customerâ€™s data can much bigger than another customerâ€™s data, therefore these independent Spark applications actually have different resource footprint. 1. Tidy Up Pipeline Output The number one well-known strategy for performance optimization is reducing the size of data in your application: ingest only necessary raw data, and keep intermediate data frames compact. In one performance tuning sprint, I replaced one of our raw data source with another source that had lower granularity, I saw 50% performance improvement easily. If an alternative data source is not available, you can use input = spark.read.parquet("fs://path/file.parquet").select(...) to limit reading to only useful columns. Reading fewer data into memory will speed up your application. It should be equally obvious that writing less output into your destination directory also improves performance easily. Because the last step often gets less attention, the last step where you write outputs is likely not the most compact. In our data pipeline, we were very liberal towards our output. There were columns and rows that were only informative the best; we created many unnecessary data. In one performance tuning sprint, I reduced our output size (by 40%), and I saw a 2-hour performance improvement. If you consider the cost of data storage, and the cost for downstream processes to handle unnecessary rows, tidy up your pipeline output spreads performance benefits to the whole system and company. Donâ€™t forget an easy win there. 2. Balance Workload via Randomization The number of cores for executors, the size of memory for executors are the very basics for Spark memory management. There is a long list of parameters: overhead memory size, default partition number, and more. You can refer to these two documents for details: spark.org, AWS EMR. To find a good resource configuration for a Spark application, you need solid engineering knowledge and experience. One insight I learned from playing with these knobs is that networking kills performance. By reducing spark.sql.shuffle.partitions, I once saw 4-hour performance improvement. Although high partitions improve resilience, low partitions allow clusters to avoid sending data around, hence saving networking resource for in-memory calculation. Most writings on Spark performance tuning concerns one application, however, we usually do not care much about individual application performance. In our Spark pipeline, we have thousands of Spark applications, we want the total run time to stay low, we do not want to tune and assign every single application its own perfect resource configuration. Without per-application resource configuration, we create three buckets: small, medium, and large. Customers with large data volume get a large memory configuration, customers with small data volume get a small memory configuration. With different â€œsizesâ€ of applications in the pipeline, we ran into issues such as a long-tail pipeline (one last application takes an extra long time), larger applications blocking smaller applications, and other issues. Although we could use delicate engineering to address these issues, I found randomization a quicker way to go. In one performance tuning sprint, I simply randomized the thousands of applications and observed a 4-hour performance improvement. Randomization of the application not only made the workload evenly distributed (easier cluster-level resource provisioning) but also well mixed different sizes of applications together (less idleness). 3. Replace Joins with Window Functions Inside our Spark pipeline, we manipulate raw input data by creating and adding new columns. In this process, we inevitably make input data frames larger and create intermediate data frames. withColumn()is a common pyspark.sql function we use to create new columns, here links to its official Spark document. We create intermediate tables because we handle different business logic in each intermediate table, then in a later step, we would join the tables to get a final table. This pattern is all good and familiar. However, when the chain of data manipulation grows longer and longer, a simple join between intermediate tables later in the process can easily create memory error. Familiar techniques such as persist()to cache intermediate data does not even help. In one performance tuning sprint, I decided to avoid joins because of consistent memory problems. I instead used Window functions to create new columns that I would otherwise accomplish via joins. I not only avoid memory problem but also I saw a 33% performance improvement. Joins involved two tables, however, the Window function only involved a few columns. Needless to say, writing Window functions were less straightforward than joins, but replacing joins with Window functions certainly helped simplify the computation graph behind the scenes. Spark is a popular choice for data engineering work, but Spark performance tuning is the biggest pain point for serious data work. I hope Spark will handle more of its tuning automatically in the future, and it is always one step ahead of the growing data processing need. In the meantime, I hope my tricks are easy to understand and helpful for you to use.
I copied that, verbatim, from a book titled â€œBASIC for Kidsâ€ that came with the Apple IIe my father brought home in a box of parts. I couldnâ€™t read the book very well. I had to ask for help. My mother explained to me that the highlighted sections were what I was supposed to type into the computer. So I did. First, the program above. Then longer programs. Programs that built chess boards on the screen. Colored the screen with lines. Programs that made rudimentary choose your own adventure games. The programs got longer, and I got older, but I never lost that urge to create. When I turned 7, I wrote my first short story. I donâ€™t remember anything about it except for the fact my teacher said it was â€œReally greatâ€ and that I â€œshould be a writerâ€ when I grew up. Iâ€™m easily swayed My father was a mechanic. My mother drove a school bus. The closest to a computer programmer in my life was my aunt, who did secretarial work for a time. In short: I never really considered programming a career path. It was just that weird thing that I got interested in, like PokÃ©mon and that blonde that I never talked to. I did, however, have a lot of writers and aspiring writers in my life. I live in Maine, grew up here, and will likely die here. Maine is known for approximately two things: Lobster, and Stephen King. The second writer that I knew was my mother, who took my writing ability as a point of personal pride, as sheâ€™d always wanted to be a published author herself. I spent the rest of my time in school writing code, short stories, and poetry in equal amounts. Looking back, I would say writing prose gave me more help with my programming than most of my math classes â€” algebra excepted, of course. Algebra is just programming, after all. The Lessons Iâ€™ve Learned You and I learn in very similar ways. That statement is dripping with assumptions, so let me clarify. Iâ€™m assuming that if youâ€™re reading this to learn, you learn by reading. Same. It was about 7th grade when I decided that I was going to be a novelist when I grew up. There was no doubt in my mind. I would spend my mornings knocking out 1,000 words, then the rest of the day would be spent reading books, and playing video games. To prove how serious I was, I looked for every book that I could find on the subject. If you know what foreshadowing is, youâ€™ve probably caught on to the book Iâ€™m going to mention: On Writing by Stephen King. Thereâ€™s just something about reading how to be a writer by your local hero to get a teenager going. I read On Writing as closely as I would any novel. It fascinated me. The autobiography at the front, all the way to the brutally honest, if not very â€œlesson-yâ€ lessons, at the end. I read it. Then I read it again. In my life I have read that book likely 20 times. Approximately once per year since I first found it. So what were the lessons he taught, and how do you translate those to software development? Donâ€™t fear imperfection Iâ€™m convinced that fear is at the root of most bad writing. No writing is perfect, just like no code is perfect. Unlike with writing, however, once our code is published we can still go back through and edit it. People donâ€™t get upset with us if we dramatically alter the structure of our writing. Why? Because all they care about is if our code works to the best that it can. Start today Amateurs sit and wait for inspiration, the rest of us just get up and go to work. There is no amount of reading about code that will make you a coder. Just like there is no amount of reading about writing that will make you a novelist. Writers write. Coders code. If you want to be a developer, go forth and develop. If you think you need permission to do that, well, I give you permission. And Iâ€™m a professional, so that must mean something, right? Remove the non-essential When you write a story, youâ€™re telling yourself the story. When you rewrite, your main job is taking out all the things that are not the story. Every line of code that I write is a rough draft. It is meant to be altered. Updated. Improved. There is very little about my long term projects that will stay from version 0.0.1 to version 10.0.0. Variable names change. Classes are created, and deleted. Entire folder structures are wiped clean from the project. Eventually, we are left with just the essential code to get the job done. Some call this the elegant solution, but I think of it as just the next draft in a long line of drafts. Why? Because art is never finished, only abandoned. If you donâ€™t have time to read, you donâ€™t have time to write If you want to be a writer, you must do two things above all others: read a lot and write a lot. Thereâ€™s no way around these two things that Iâ€™m aware of, no shortcut. I read code. Not in the debugging sense, but in the â€œokay, letâ€™s see what this project does.â€ When I really started to learn programming, I didnâ€™t actually have a computer to use most of the time. I learned by printing code off at the library, bringing it home, and reading it like a chapter book. Complete with highlighter. I made notes in the margins, I refactored in a notebook. When I got back to the computer, I furiously typed away, making my changes, attempting to compile and run, and then printed it all off again. To this day, when I struggle with any really difficult refactor, I will print the code off and attack it with a highlighter. Only now I have entire conference rooms where I can lay the papers out, instead of my bedroom floor. You donâ€™t have to do any of this, but you should be going out and reading commits in your prefered language. I write PHP, so I read the commits for larger projects like PHPUnit, Laravel, WordPress, etc. Not just the commit messages, or pull requests, but the actual code changes. Read the code, ask why changes were made, get involved in conversations. This is how we become better developers. If you write, someone will say will try to make you feel lousy about it I have spent a good many years sinceâ€•too many, I thinkâ€•being ashamed about what I write. I think I was forty before I realized that almost every writer of fiction or poetry who has ever published a line has been accused by someone of wasting his or her God-given talent. If you write (or paint or dance or sculpt or sing, I suppose), someone will try to make you feel lousy about it, thatâ€™s all. I live and breath WordPress. And WordPress sucks, right? People will always find some way to try to make you feel bad about what youâ€™re doing. Maybe they even have legitimate reasons for it, but just have no idea how to express themselves without sounding like a jerk. Either way, ignore them. Hear their words, investigate anything that sounds like legitimate concern, but ignore everything else. I am a PHP developer that works in WordPress. Whenever I read up on Tech-Twitter, I feel attacked. But Iâ€™m okay with that. Why? Because Iâ€™m a damn good PHP developer. And you canâ€™t convince me otherwise. Life isnâ€™t a support system for art It starts with this: put your desk in the corner, and every time you sit down there to write, remind yourself why it isnâ€™t in the middle of the room. Life isnâ€™t a support system for art. Itâ€™s the other way around. This lesson took my fiance getting pregnant to really sink in for me. Code was always my hobby. I had never even considered it a possible career path. My entire life revolved around making enough money to survive, so I could spend all night writing code. It was fine. For me. Suddenly, it was more than just myself that I had to worry about. With my son on the way, I knew I couldnâ€™t get by on the wages I was making. Luckily, I had cultivated a skill, and the connections to sell it. I landed my first professional PHP role just 3 months before my son was born. Still, during that time, it seemed as though my entire life revolved around my code. My job was remote, which meant my job was in my living room, and so was I. Which meant I was always at work. I spent 18 hours a day on my computer, sometimes with my son on my lap, just typing away. It took an office job, and an hour long commute, for me to realize I didnâ€™t care as much about the code as I did being a dad. The work will be there when you come back. Your life disappears every second. If youâ€™re like me, you love what you do. The problem comes when we make what we do become who we are. We have to separate ourselves from our code. You might just surprise yourself and realize, it makes your code better. Be yourself One cannot imitate a writerâ€™s approach to a particular genre, no matter how simple what that writer is doing may seem. Lately Iâ€™ve been teaching myself JavaScript by writing an AI for a game called Screeps. I talk about it in an article I wrote. The thing that I love about writing this code is how many different ways there are to do it. With over 200,000 players on Slack, there are no two custom codebases the same. Iâ€™m writing a state machine pattern, which is not unique in and of itself. In fact, I followed a tutorial to implement the state machine, because though Iâ€™ve written them in C#, Iâ€™d never done one in JavaScript. But even with that, my code is different than the tutorial. Plus, nobody else has my exact states. Our code is an extension of ourselves. Which means we will never reach our full potential if we try to be someone else. Iâ€™m not Taylor Otwell, Iâ€™ll never be able to write code like he does. So, when I submitted my PR to the Laravel Framework, I didnâ€™t try to. I wrote my code, the way that I write code. I made the edits that he requested, because thatâ€™s what we do. Edits donâ€™t change who wrote the code, just what the code does. Be yourself, and donâ€™t be afraid, becauseâ€¦ Failure is okay Optimism is a perfectly legitimate response to failure. Iâ€™ll take this a step further: Failure is not just okay, it is necessary. If you never break anything, you never learn. I wrote millions of lines of code before I ever wrote a line I considered good. There was always something there, a thought poking out. One of the first large projects I tackled was writing my own ORM. Of course, I had no idea thatâ€™s what I was doing at the time. At the time, I just wanted some way to avoid having to write SQL queries all over my project. My ORM was absolute garbage, but it got the job done. It was a failure of proper architecture, but a success in teaching me that it was possible to abstract my code. I did all of this in my first year as a professional PHP developer. I now have 12 years of experience, and I donâ€™t think I could tackle an ORM like Eloquent yet. Not on my own, at least. But Iâ€™m still okay with that. Iâ€™m optimistic about failure, because everytime I fail, every error message that I see, is a step closer to success. Thatâ€™s why Iâ€™m such a huge proponent of test-driven development. If this article was interesting to you, youâ€™re going to love the stream of tech-consciousness that is my Twitter feed. Head over there and give me a follow. You wonâ€™t be disappointed.
Erik Engheim Nov 28Â·23 min read On YouTube, I watched a Mac user who had bought an iMac last year. It was maxed out with 40 GB of RAM costing him about $4,000. He watched in disbelief how his hyperexpensive iMac was being demolished by his new M1 Mac Mini, which he had paid a measly $700 for. In real-world test after test, the M1 Macs are not merely inching past top-of-the-line Intel Macs, they are destroying them. In disbelief, people have started asking how on earth this is possible? If you are one of those people, you have come to the right place. Here I plan to break it down into digestible pieces exactly what it is that Apple has done with the M1. Specifically the questions I think a lot of people have are: Sure you could try to Google this, but if you try to learn what Apple has done beyond the superficial explanations, you will quickly get buried in highly technical jargon such as M1 using very wide instruction decoders, enormous reorder buffer (ROB), etc. Unless you are a CPU hardware geek, a lot of this will simply be gobbledygook. To get the most out of this story I advise reading my earlier piece: â€œWhat Does RISC and CISC mean in 2020?â€ There I explain what a microprocessor (CPU) is as well as various important concepts such as: Instruction set architecture (ISA) Pipelining Load/store architecture Microcode vs. micro-operations But if you are impatient, I will do a quick version of the material you need to understand to grasp my explanation of the M1 chip. What is a microprocessor (CPU)? Normally when speaking of chips from Intel and AMD we talk about central processing units (CPUs) or microprocessors. As you can read more about in my RISC vs. CISC story, these pull in instructions from memory. Then each instruction is typically carried out in sequence. A CPU at its most basic level is a device with a number of named memory cells called registers and a number of computational units called arithmetic logic units (ALU). The ALUs perform things like addition, subtraction, and other basic math operations. However, these are only connected to the CPU registers. If you want to add up two numbers, you have to get those two numbers from memory and into two registers in the CPU. Here are some examples of typical instructions that a RISC CPU as found on the M1 carries out. Here r1 and r2 are the registers I talked about. Modern RISC CPUs cannot do operations on numbers that are not in a register like this. For example, it cannot add two numbers residing in RAM in two different locations. Instead, it has to pull these two numbers into a separate register. That is what we do in this simple example. We pull in the number at memory location 150 in the RAM and put it into register r1 in the CPU. Next, we put the contents of address 200 into register r2. Only then can the numbers be added with the add r1, r2 instruction. The concept of registers is old. For example, on this old mechanical calculator, the register is what holds the numbers you are adding. Likely the origin of the term cash register. The register is where you registered input numbers. The M1 is not a CPU! But here is a very important thing to understand about the M1: The M1 is not a CPU, it is a whole system of multiple chips put into one large silicon package. The CPU is just one of these chips. Basically, the M1 is one whole computer onto a chip. The M1 contains a CPU, graphical processing unit (GPU), memory, input and output controllers, and many more things making up a whole computer. This is what we call a system on a chip (SoC). Today if you buy a chip â€” whether from Intel or AMD â€” you actually get what amounts to multiple microprocessors in one package. In the past computers would have multiple physically separate chips on the motherboard of the computer. However because we are able to put so many transistors on a silicon die today, companies such as Intel and AMD began putting multiple microprocessors onto one chip. Today we refer to these chips as CPU cores. One core is basically a full independent chip that can read instructions from memory and perform calculations. This has for a long time been the name of the game in terms of increasing performance: Just add more general-purpose CPU cores. But there is a disturbance in the force. There is one player in the CPU market which is deviating from this trend. Appleâ€™s not so secret heterogeneous computing strategy Instead of adding ever more general-purpose CPU cores, Apple has followed another strategy: They have started adding ever more specialized chips doing a few specialized tasks. The benefit of this is that specialized chips tend to be able to perform their tasks significantly faster using much less electric current than a general-purpose CPU core. This is not entirely new knowledge. For many years already specialized chips such as the graphical processing units (GPUs) have been sitting in Nvidia and AMD graphics cards performing operations related to graphics much faster than general-purpose CPUs. What Apple has done is simply to take a more radical shift toward this direction. Rather than just having general-purpose cores and memory, the M1 contains a wide variety of specialized chips: Central processing unit (CPU) â€” the â€œbrainsâ€ of the SoC. Runs most of the code of the operating system and your apps. Graphics processing unit (GPU) â€” handles graphics-related tasks, such as visualizing an appâ€™s user interface and 2D/3D gaming. Image processing unit (ISP) â€” can be used to speed up common tasks done by image processing applications. Digital signal processor (DSP) â€” handles more mathematically intensive functions than a CPU. Includes decompressing music files. Neural processing unit (NPU) â€” used in high-end smartphones to accelerate machine learning (A.I.) tasks. These include voice recognition and camera processing. Video encoder/decoder â€” handles the power-efficient conversion of video files and formats. Secure Enclave â€” encryption, authentication, and security. Unified memory â€” allows the CPU, GPU, and other cores to quickly exchange information. This is part of the reason why a lot of people working on images and video editing with the M1 Macs are seeing such speed improvements. A lot of the tasks they do can run directly on specialized hardware. That is what allows a cheap M1 Mac Mini to encode a large video file without breaking a sweat while an expensive iMac has all its fans going full blast and still cannot keep up. Read more about heterogeneous computing: Apple M1 foreshadows Rise of RISC-V. What is Special About Appleâ€™s Unified Memory Architecture? Appleâ€™s â€œUnified Memory Architectureâ€ (UMA) is a bit tricky to wrap your head around (I got it wrong first time I wrote it down here). To explain why, we need to take a few steps back. For a long time cheap computer systems have had the CPU and GPU integrated into the same chip (same silicon die). These have been famously slow. In the past saying â€œintegrated graphicsâ€ was essentially the same as saying â€œslow graphics.â€ These where slow for severals reasons: Separate areas of this memory got reserved for the CPU and GPU. If the CPU had a chunk of data it wanted the GPU to use, it couldnâ€™t say â€œhere have some of my memory.â€ No, the CPU had to explicitly copy the whole chunk of data over the memory area controlled by the GPU. CPUs and GPUs donâ€™t want their memory served the same way. Let us do a silly food analogy: CPUs want their plate of data served very quickly by the waiter, but they are totally cool with small portion sizes. Imagine a fancy French restaurant with waiters on rollerblades to serve you really quickly. GPUs in contrast are cool with the waiter being slow to serve the data. But the GPUs want enormous servings. They gobble massive amounts of data because they are massive parallel machines, that can chew through lots of data in parallel. Imagine an American junk food place, where the food takes some time to arrive because they are pushing a whole trolley of food to your seating area. With such different needs, putting CPUs and GPUs on the same physical chip was not a great idea. The GPUs would sit there starving while given small French servings. The result was that there was no point in putting powerful GPUs on an SoC. The tiny portions of data served up, could easily be chewed up by a weak little GPU. The second problem was that large GPUs produce a lot of heat and thus you cannot integrate them with the CPU without getting problems ridding yourself of the heat produced. Thus discrete graphics cards tend to look like the one below: Large beasts with massive cooling fans. They have special dedicated memory designed to serve the greedy cards massive amounts of data. That is why these cards have high performance. But they have an achilles heel: Whenever they have to get data from the memory used by the CPU, this happens over a set of copper traces on the computer motherboard called a PCIe bus. Try chugging water through a super thin straw. It may get to your mouth fast, but the throughput is totally inadequate. Appleâ€™s Unified Memory Architecture tries to solve all these problems without having the disadvantages of old school shared memory. They achieve this in the following ways: Some will say unified memory is not entirely new. It is true that different systems have had it in the past. But then the difference in memory requirements may not have been as large. Secondly what Nvidia calls Unified Memory is not really the same thing. In the Nvidea world Unified Memory simply means that there is software and hardware which takes care of automatically copying data back and forth between the separate CPU and GPU memory. Thus from a programmers perspective Apple and Nvidia Unified Memory may look the same, but it is not the same in a physical sense. There is of course a tradeoff in this strategy. Getting this high bandwidth memory (big servings) require full integration which means you take away the opportunity from customers to upgrade their memory. But Apple seeks to minimize this problem by making the communication with the SSD disks so fast, that they essentially work like old fashion memory. If SoCs Are So Smart, Why Donâ€™t Intel and AMD Copy This Strategy? If what Apple is doing is so smart, why is not everybody doing it? To some extent they are. Other ARM chip makers are increasingly putting in specialized hardware. AMD has also started putting stronger GPUs on some of their chips and moving gradually toward some form of SoC with the accelerated processing units (APU) which are basically CPU cores and GPU cores placed on the same silicon die. Yet there are important reasons why they cannot do this. An SoC is essentially a whole computer on a chip. That makes it a more natural fit for an actual computer-maker, such as HP and Dell. Let me clarify with a silly car analogy: If your business model is to build and sell car engines, it would be an unusual leap to begin manufacturing and selling whole cars. For ARM, in contrast, this isnâ€™t an issue. Computer makers such as Dell or HP could simply license ARM intellectual property and buy IP for other chips, to add whatever specialized hardware they think their SoC should have. Next, they ship the finished design over to a semiconductor foundry such as GlobalFoundries or TSMC, which manufactures chips for AMD and Apple today. Here we get a big problem with the Intel and AMD business model. Their business models are based on selling general-purpose CPUs, which people just slot onto a large PC motherboard. Thus computer-makers can simply buy motherboards, memory, CPUs, and graphics cards from different vendors and integrate them into one solution. But we are quickly moving away from that world. In the new SoC world, you donâ€™t assemble physical components from different vendors. Instead, you assemble IP (intellectual property) from different vendors. You buy the design for graphics cards, CPUs, modems, IO controllers, and other things from different vendors and use that to design an SoC in-house. Then you get a foundry to manufacture this. Now you got a big problem, because neither Intel, AMD, or Nvidia are going to license their intellectual property to Dell or HP for them to make an SoC for their machines. Sure Intel and AMD may simply begin to sell whole finished SoCs. But what are these to contain? PC-makers may have different ideas of what they should contain. You potentially get a conflict between Intel, AMD, Microsoft, and PC-makers about what sort of specialized chips should be included because these will need software support. For Apple this is simple. They control the whole widget. They give you, for example, the Core ML library for developers to write machine learning stuff. Whether Core ML runs on Appleâ€™s CPU or the Neural Engine is an implementation detail developers donâ€™t have to care about. The fundamental challenge of making any CPU run fast So heterogeneous computing is part of the reason but not the sole reason. The fast general-purpose CPU cores on the M1, called Firestorm, are genuinely fast. This is a major deviation from ARM CPU cores in the past which tended to be very weak compared to AMD and Intel cores. Firestorm, in contrast, beats most Intel cores and almost beats the fastest AMD Ryzen cores. Conventional wisdom said that was not going to happen. Before talking about what makes Firestorm fast it helps to understand what the core idea of making a fast CPU is really about. In principle you accomplish in a combination of two strategies: Back in the â€™80s, it was easy. Just increase the clock frequency and the instructions would finish faster. Every clock cycle is when the computer does something. But this something can be quite little. Thus an instruction may require multiple clock cycles to finish because it is made up of several smaller tasks. However, today increasing the clock frequency is next to impossible. That is the whole â€œEnd of Mooreâ€™s Lawâ€ that people have been harping on for over a decade now. Thus it is really about executing as many instructions as possible in parallel. Multi-core or Out-of-Order processors? There are two approaches to this. Add more CPU cores. Each core works independent and in parallel. Make each CPU core execute multiple instructions in parallel. For a software developer, adding cores is like adding threads. Every CPU core is like a hardware thread. If you donâ€™t know what a thread is, then you can think of it as the process of carrying out a task. With two cores, a CPU can carry out two separate tasks concurrently: two threads. The tasks could be described as two separate programs stores in memory or it could actually be the same program performed twice. Each thread needs some bookkeeping, such as where in a sequence of program instructions the thread is currently at. Each thread may store temporary results which should be kept separate. In principle, a processor can have just one core and run multiple threads. In this case, it simply halts one thread and stores current progress before switching to another. Later it switches back. This doesnâ€™t bring much of a performance enhancement unless the thread has to frequently halt to: Wait for input from the user Data from a slow network connection, etc. Let us call these software threads. Hardware threads mean you have actual physical CPU cores at your disposal to speed up things. The problem with threads is that software developers have to write so called multi-threaded code. That is often difficult. In the past, this was some of the hardest code to write. However making server software multi-threaded tends to be easy. It is simply a matter of handling each user request on a separate thread. Thus in this case having lots of cores, is an obvious advantage. Especially for cloud services. That is the reason why you see ARM CPU-makers such as Ampere making CPUs such as the Altra Max which has a crazy 128 cores. This chip is specifically made for the cloud. You donâ€™t need crazy single-core performance, because in the cloud it is all about having as many threads as possible per watt to handle as many concurrent users as possible. Read more about ARM CPUs with many cores: Are Servers Next for Apple? Apple, in contrast, is on the complete opposite end of the spectrum. They make single-user devices. Lots of threads is not an advantage. Their devices are used for gaming, video editing, development, etc. They want desktops with beautiful responsive graphics and animations. Desktop software is generally not made to utilize lots of cores. For example, computer games will likely benefit from eight cores, but something like 128 cores would be a total waste. Instead, you would want fewer but more powerful cores. How Out-of-Order Execution Works To make a more powerful core we need it to execute more instructions in parallel. Out-of-Order execution (OoOE) is a way to execute more instructions in parallel but without exposing that capability as multiple threads. For an alternative solution read: Very Long Instruction Word Microprocessors Developers donâ€™t have to code their software specifically to take advantage of OoOE. Seen from the developerâ€™s perspective it just looks like each core runs faster. Please note it is not a direct alternative to hardware threads. You want to use both, depending on the particular problem you are solving. To understand how OoOE works, you need to understand some things about memory. Asking for data in one particular memory location is slow. But the CPU is capable of getting getting many bytes at the same time. Hence getting 1 specific byte in memory, takes no less time than getting 100 more bytes following that byte in memory. Here is an analogy: Consider pickers in a warehouse. Could be the little red robots in the picture above. Moving to multiple locations spread all over takes time. But picking up items from slots adjacent to each other is quick. Computer memory is very similar. You can quickly fetch content of memory cells which are adjecent. Data is sent across what we call a databus. You can think of it as a road or pipe between memory and different parts of the CPU where data gets pushed through. In reality, it is of course just some copper tracks conducting electricity. If the databus is wide enough you can get multiple bytes at the same time. Thus CPUs get a whole chunk of instructions at a time to execute. But they are written to be executed one after the other. Modern microprocessors do what we call Out-of-Order execution (OoOE). That means they are able to analyze a buffer of instructions quickly and see which ones depend on which. Look at the simple example below: Multiplication tends to be a slow process. So say it takes multiple clock cycles to perform. The second instruction will simply have to wait because its calculation depends on knowing the result that gets put into the r1 register. However, the third instruction at line 03 doesnâ€™t depend on calculations from previous instructions. Hence an Out-of-Order processor can begin calculating this instruction in parallel. However more realistically we are talking about hundreds of instructions. The CPU is able to figure out all the dependencies between these instructions. It analyses the instructions by looking at the inputs to each instruction. Do the inputs depend on output from one or more other instructions? By input and output, we mean registers containing results from previous calculations. For example, the add r4, r1, 5 instruction depends on input from r1 which is produced by mul r1, r2, r3 . We can chain together these relationships into long elaborate graphs that the CPU can work through. The nodes are the instructions and the edges are the registers connecting them. The CPU can analyze such a graph of nodes and determine which instructions it can perform in parallel and where it needs to wait for the results from multiple dependent calculations before carrying on. Many instructions will finish early but we cannot make their results official. We cannot commit them; otherwise, we supply the result in the wrong order. To the rest of the world, it has to look as if the instructions were carried out in the same sequence as they were issued. Like a stack, the CPU will keep popping done instructions from the top, until hitting an instruction that is not done. Basically you got two forms of parallelism: One that the developer must deal with explicitly when writing code and one that is entirely transparent. Of course the latter relies on lots of transistors on the CPU dedicated to Out-of-Order Execution magic. This is not a viable solution for small CPUs with few transistors. It is the superior Out-of-Order execution that is making the Firestorm cores on the M1 kick ass and take names. It is in fact much stronger than anything from Intel or AMD and they may never be able to catch up. To understand why, we need to get into some more technical details. ISA Instructions vs Micro-Operations Previously I skipped some details on how Out-of-Order Execution (OoOE) works. Programs loaded into memory are made up of machine code instructions designed for specific Instruction-Set Architectures (ISA) such as x86, ARM, PowerPC, 68K, MIPS, AVR etc. For instance the x86 instruction to fetch a number from memory location 24 into a register you may write: x86 have registers named ax, bx, cx and dx(remember these are the memory cells inside the CPU you perform operations on). However the equivalent ARM instruction would look like this: AMD and Intel processors understand the x86 ISA, while Apple Silicon chips, such as M1, understand the ARM Instruction-Set Architecture (ISA). However internally the CPU works on an entirely different instruction-set invisible to the programmer. We call these micro-operations (micro-ops or Î¼ops). These are the instructions the Out-of-Order hardware works with. But why canâ€™t the OoOE hardware work with regular machine code instructions? Because the CPU needs to attach lots of different information to the instructions to be able to run them in parallel. Thus while a normal ARM instruction may be 32-bit (32 digits of 0 and 1), a micro-op can be much longer. It contains information about its order. Consider if we run instruction 01: mul and 03: add in parallel. Both store their result in register r1 . If we write the result of instruction 03: add before 01: mul, then instruction 02: add will get the wrong input. Hence it is very important to keep track of instruction order. The order is stored with each micro-op. It also stores e.g. that instruction 02: add depends on output from 01: mul. That is why we cannot have programs written using micro-ops. They contain lots of details specific to the internals of each microprocessor. Two ARM processors could have very different micro-ops internally. Read more about CPUs with micro-ops like instructions: Very Long Instruction Word Microprocessors. Also, micro-ops are usually easier to work with for the CPU. Why? Because they each do one simple limited task. Regular ISA instructions can be more complex causing a bunch of stuff to happen and thus frequently translate to multiple micro-ops. Thus the name â€œmicroâ€ comes from the small task they do, not the length of the instruction in memory. For CISC CPUs there is usually no alternative but to use micro-ops otherwise the large complex CISC instructions would make pipelines and OoOE next to impossible to achieve. RISC CPUs have a choice. So, for example, smaller ARM CPUs donâ€™t use micro-ops at all. But that also means they cannot do things such as OoOE. Why is AMD and Intel Out-of-Order execution inferior to M1? But you wonder, why does any of this matter? Why is this detail important to know to understand why Apple has the upper hand on AMD and Intel? It is because the ability to run fast depends on how quickly you can fill up a buffer of micro-operations. If you got a large buffer then the OoOE hardware will have an easier time to locate two or more instructions which it can run in parallel. But there is no point in having a large instruction buffer if you cannot refill it fast enough after instructions get picked and executed. The ability to refill the instruction buffer quickly relies on the ability to quickly chop machine code instruction into micro-ops. The hardware units that does this are called decoders. And finally we get to the killer feature of the M1. The biggest and meanest Intel and AMD microprocessor have a total of four decoders busy cutting machine code instructions into micro-ops. But this is no match for the M1, which has an absolutely unheard of number of decoders: Eight. Significantly more than anybody else in the industry. That means it can fill up the instruction buffer much quicker. To deal with this the M1 also has an instruction buffer which is 3x times larger than what is normal in the industry. Why canâ€™t Intel and AMD add more instruction decoders? This is where we finally see the revenge of RISC, and where the fact that the M1 Firestorm core has an ARM RISC architecture begins to matter. You see, an x86 instruction can be anywhere from 1â€“15 bytes long. RISC instructions have fixed length. Every ARM instruction is 4 bytes long. Why is that relevant in this case? Because splitting up a stream of bytes into instructions to feed into eight different decoders in parallel becomes trivial if every instruction has the same length. However, on an x86 CPU, the decoders have no clue where the next instruction starts. It has to actually analyze each instruction in order to see how long it is. The brute force way Intel and AMD deal with this is by simply attempting to decode instructions at every possible starting point. That means x86 chips have to deal with lots of wrong guesses and mistakes which has to be discarded. This creates such a convoluted and complicated decoder stage that it is really hard to add more decoders. But for Apple, it is trivial in comparison to keep adding more. In fact, adding more causes so many other problems that four decoders according to AMD itself is basically an upper limit for them. This is what allows the M1 Firestorm cores to essentially process twice as many instructions as AMD and Intel CPUs at the same clock frequency. One could argue as a counterpoint that CISC instructions turn into more micro-ops. For instance if every x86 instruction turned into 2 micro-ops while every ARM instruction turned into 1 micro-op, then four x86 decoders would produce the same number of micro-ops per clock cycle as an ARM CPU with 8 decoders. Except this is not the case in the real world. Highly optimized x86 code rarely uses complex CISC instructions, which would translate into many micro-ops. In fact most will only translate into 1 micro-op. However all these simple x86 instructions donâ€™t help Intel or AMD. Because even if those 15 byte long instructions are rare, the decoders have to be made to handle them. This incurs complexity that blocks AMD and Intel from adding more decoders. But AMDs Zen3 cores are still faster right? As far as I remember from performance benchmarks, the newest AMD CPU cores, the ones called Zen3 are slightly faster than Firestorm cores. But here is the kicker: That only happens because the Zen3 cores are clocked at 5 GHz. Firestorm cores are clocked at 3.2 GHz. The Zen3 is just barely squeezing past Firestorm despite having almost 60% higher clock frequency. So why doesnâ€™t Apple increase the clock frequency too? Because higher clock frequency makes the chips run hotter. That is one of Appleâ€™s key selling points. Their computers â€” unlike Intel and AMD offerings â€” barely need cooling. In essence, one could say Firestorm cores really are superior to Zen3 cores. Zen3 only manages to stay in the game by drawing a lot more current and getting a lot hotter. Something Apple simply chooses not to do. If Apple wants higher performance they are simply going to add more cores. That lets them keep watt usage down while offering more performance. The future It seems AMD and Intel have painted themselves into a corner on two fronts: They donâ€™t have a business model that makes it easy to pursue heterogeneous computing and SoC designs. Their legacy x86 CISC instruction set is coming back to haunt them, making it hard to improve OoO performance. Read more: Intel, ARM and the Innovators Dilemma It doesnâ€™t mean game over. They can increase the clock frequency and use more cooling, throw in more cores, beef up the CPU caches, etc. But they are both at a disadvantage. Intel is in the worst situation, as their cores are already soundly beaten by Firestorm, and they have weak GPUs to integrate with an SoC solution. The problem with throwing in more cores is that for typical desktop workloads you reach diminishing returns with too many cores. Sure lots of cores are great for servers. However here companies such as Amazon and Ampere are attacking with monster CPUs with 128 cores. This is like fighting the western and eastern front at the same time. But fortunately for AMD and Intel, Apple doesnâ€™t sell their chips on the market. So PC users will simply have to put up with whatever they are offering. PC users may jump ship, but that is a slow process. You donâ€™t leave immediately a platform you are heavily invested in. But young professionals, with money to burn without too deep investments in any platform, may increasingly turn to Apple in the future, beefing up their hold on the premium market and consequently their share of the total profit in the PC market.
Given the latest chatter about the Apple Car, if nothing else, this will be a fascinating tweet/story to look back upon. Thereâ€™s undoubtedly more to the story, but just on face value, it seems like in 10 years we should have a pretty binary view on if this was great for Tesla and bad for Apple or great for Apple and bad for Teslaâ€¦
Jon Bell 2 days agoÂ·3 min read Well before Google, Facebook, Twitter, and Amazon existed, Bill Gates was still the CEO of Microsoft and Apple was about to go out of business. There were a lot of reasons for Microsoftâ€™s dominance and Appleâ€™s failings in the 90s, but a big one was that Appleâ€™s operating system was old. Really old. So the company needed to build or buy a new one to catch up. There were two obvious companies for Apple to buy, Be and NeXT. Be was fresh, fast, and had a cult following. This image might not look like much today, but at the time the OSâ€™s design was seen as revolutionary. In the other corner was NeXT. This screenshot looks pretty dull, especially compared with Be. But the technology powering NeXT was truly next-generation. If you use an iPhone or macOS computer, youâ€™re still using designs and technology built from the innovations that NeXT pioneered. (And the world wide web was invented on a NeXT computer.) The CEO of Apple at the time, Gil Amelio, needed to figure out which companyâ€™s technology to buy, and I love the story of how it happened. Itâ€™s recounted in Amelioâ€™s book â€œOn the Firing Line: My 500 Days at Apple.â€ Gil Amelio had been speaking with the CEOs of each company, and decided to have a shoot-out between them on December 10th, 1996. Steve Jobs showed up to pitch the virtues of NeXT and he came prepared with a presentation and his best engineer. Then Steve Jobs left, the board all nodded approvingly at each other, and the Be CEO went next. But he had no presentation. No pitch. His attitude amounted to â€œyou know weâ€™re the best, because we are the best.â€ In hindsight, we can say that Steve Jobs had the better technology, connections, or charisma. We could believe he was simply fated to return to the company he founded and was fired from. But none of that was necessarily true on December 10th, 1996. Itâ€™s a simple story: one person prepared, the other didnâ€™t. One person did their best to hear the concerns of the board, the other didnâ€™t. One person tried to be easy to work with, the other didnâ€™t. Thereâ€™s a lot to learn from here. This explain why NeXT beat Be, and why Apple didnâ€™t go out of business. From there, we can see how iPhone was invented and was successful because it was based on NeXT technology. And that whole timeline was set into motion on December 10, 1996. All because Steve Jobs knew that great technology and design canâ€™t speak for themselves. But listening and hard work can.
Hunter Walk 5 days agoÂ·4 min read Googleâ€™s internal management approach has sustained and scaled pretty impressively over the years. Quantitative goal-setting, setting stretch targets â€” these principles are as evident in the 2020s as they were when I arrived in 2003. Underpinning it all are OKRs â€” Objectives and Key Results â€” the framework by which individuals, teams and the entire company is managed. Xoogler Don Dodge did a comprehensive job of documenting OKRs in an earlier blog post, but the basics were always this: Each quarter individuals and teams document their objectives for the next 90 days and grade the goals they set 90 days earlier. In Q4 teams also set metagoals for the next year. Now we have software startups who have basically built their business around OKR-style planning! Or you could, of course, use this OKR template from Homebrew portfolio company Coda. My Nine Years at Google meant 36 Quarterly OKRs and the correlating number of annual planning exercises. My role at YouTube had me often working through our OKRs with Larry and Sergey (one of those stressful exercises that in hindsight was amazing). I believe OKRs were originally recommended to L&S by John Doerr of KPCB, and since that time, OKRs have spread through tech companies, sometimes carried by Google alumnus themselves. OKRs are sensible, straight forward and on a planning cycle managers understand. And thatâ€™s the problem. In 2009 Y Combinator founder Paul Graham wrote an essential essay called Makerâ€™s Schedule, Managerâ€™s Schedule. The post discusses how engineers need long periods of dedicated time to build and managers (or people whose work generally involves lots of meetings) can honor this by not scheduling interruptions in the middle of these periods. Itâ€™s great â€” you should read it. Manager time vs Maker time gave me a lens to not just evaluate day to day schedules, but the general cadence of how we plan and build at Google. We consider ourselves a company founded and driven by Makers (our engineers), but somehow we settled into a Manager planning rhythm, one which mimicked accounting cycles rather than how things actually get built. â€œQuarterly goals?â€ Why are three months the right duration for building features, why not two months or four months? And there was the amusing â€œlast week of quarterâ€ push to try and ship all the features youâ€™d committed to ~90 days earlier. Even more confusing were annual goals. By Q4, itâ€™s pretty clear whether youâ€™re going to hit the annual goals, the high level targets meant to inspire a year of work, but because you havenâ€™t started next yearâ€™s planning cycle, the team has no documented targets for what the next 12 months look like. (Obviously the best managers start with an evergreen vision and then break into planning cycles â€” this isnâ€™t about roadmapping within teams â€” but the Quarterly + Annual segmenting is still derived from financial planning, not hacking). What would I recommend for the Maker-side of early startup companies instead of Quarterly + Calendar Year Annual? These three: One Month â€” â€œWhat are we building this monthâ€ is the key question. Team leads get together the morning of the Monday prior to monthâ€™s end and document the next monthâ€™s feature releases. This is a bottom up process which includes items shipped completely intra-month and component work of projects which are greater than 30 days long (if you canâ€™t break a complex project into at least 30 days goals, then itâ€™s too big). Four weeks, a few weekends. Enough time to get a lot done. You donâ€™t need to micromanage â€” for example, if the team spends two days per month bug fixing, just hold that time aside in your calculations, donâ€™t document the bugs you intend to fix. â€œN+12 Monthsâ€ â€” â€œWhat will our product and business look like a year from now?â€ I like the idea of a rolling â€œone year outâ€ vision, processing new learnings and opportunities. At any given time the entire organization can have a true north for where we want to be a year from now. It evolves, it learns, it doesnâ€™t tick down to zero but rather always looks out over the horizon. Minimal Quarterly/Annual KPIs â€” Recognizing that quarterly and annual goals are important for financial reporting and goaling, you should keep a very narrow grasp on what you actually want to measure â€” just key drivers of business â€” and set quarterly targets. There can be a reality check â€” do these quarterly targets get achieved given what weâ€™re building? For me, Monthly Goals combined with N+12 Goals create the right short-term Maker cadence with longer term vision. I never got the chance to try it at Google, but hope to find companies using this sort of planning cycle to see how it works for them. [This post updates a version I wrote in 2013. Wanted to refresh and re-share because itâ€™s planning season!]
I donâ€™t like podcasts. For me, they are a drag. To sum up â€” itâ€™s when a couple of people take an hour to say what can be said in 10-mins. Only lazy people who like to think they are being productive listen to them. Thatâ€™s how I talked a year ago. But now I have a different opinion about this audio platform. I started my podcast in March 2020. As of December, I have converted $140,000 worth of clients from it. This conversion rate is second best to my Youtube Channel. Even that took 2-years to show this kind of return. If you think starting a podcast may be profitable for your business, this my basic-guide. The podcast industry is worth about $9.28 Billion and I now know I was wrong. People love to listen to their favourite personalities. Especially, when doing mechanical work like exercise, chores, or driving. With 144 Million listeners in America alone, itâ€™s a rich pool of potential audience. But, itâ€™s not as simple as pressing the record button and talking. You have to be strategic if you want people to listen and buy from you. Thatâ€™s the part I will cover in this story. Basic things like how to make a cover image or the recording process, I expect you to figure out on your own. A simple google search can solve that. This story is about what you canâ€™t find on Google â€” my strategy. 1. Podcast SEO: How To Rank Your Podcast I assume you have a business or niche you can talk about. But among 1.5 Million Podcasts, how will people find yours? Podcast Search Engine Optimization (SEO) is the answer. When a person searches for a topic, a search engine will decide whether your podcast shows up or not. Different podcast platforms (Spotify, Apple) have their own search engines. They are like Google but much simpler. They track two things; Keywords and Reviews. To optimize your podcast for keywords, set-up these three things. Note: You can find popular keywords for your industry from any keyword tool. Like the Keyword Planner offered by Google Ads. The general assumption is that whatâ€™s popular in web-searches tends to be popular in podcasts too. 2. Publish: How To Host Your Podcast Now, you can start to record your first episode. Once you have done that you need a platform to host your podcast like you need hosting for a website. If you want to find those platforms on your own, thatâ€™s okay. You can skip this part. I use Anchor.fm. It is a free tool that helps record and host my podcast. Once I upload the episode, it publishes it to all key platforms like Spotify, Apple or Google. Plus, I can also view the statistics from all the platforms in one place. But there are many other options available which you can choose yourself. 3. Growth: How to Get More Listeners Optimizing Keywords isnâ€™t enough. Everybody who starts a podcast does that. You need to do something more to reach your desired audience. With ten months of posting, I get 2k average listeners per episode with a total of 250k listens. Some episodes can reach as high as 8k listens. What did I do? Collaboration and interviews. You need to get into podcasts of people who already have the listeners you want. And itâ€™s not difficult. Like you, everybody has to create content. Bringing someone else from the industry eases that burden. But, you have to show your credibility in the industry. If you are a marketing agency, then showcase clients served or years in business. Plus, start posting episodes. Even if they get zero-listens. The more you post, the more credible and committed you will appear. When looking for podcasts to get featured on, look for the ones that are within your reach. Someone with 2k-5k followers is more approachable than the 100k one. Make a list and reach out to one a day. I skipped this process by hiring a podcasting agency. That means I pay to get featured on other peopleâ€™s podcasts. Itâ€™s not an issue because my funnel guarantees return. I have set-up my funnel in a way that If I spend $10k in ads, then I get $100k in clients. If you have money and the right funnel, I would suggest doing this. Otherwise, free is good enough to start. How to Perfect Your Marketing Funnel As a Small-Business Owner Hunting vs Farming: The only two types of marketing present. entrepreneurshandbook.co 4. Growth: How to Get More Reviews Spotify doesnâ€™t have a rating system. But iTunes or Apple Podcasts do. The more reviews you have the higher you will rank in Apple search engines. I currently have a total of 65 ratings on iTunes. I donâ€™t focus on ratings because my performing platform is Spotify. But, itâ€™s good to keep this tactic in your tool belt. You can make people leave a review by giving them an incentive. Example: â€œBe sure to leave a review. The best three will get my free-ebook onâ€¦â€ What to give away? This is where your existing business products and services will help you. If you are selling physical products then offer a discount code. If services, then offer a part of the service. I always offer a consultation no matter the platform. Doesnâ€™t it cost me? Not so much. When I give people a taste of my services/product for free, a percentage of them buys. By doing this on my Youtube Channel, I convert 50% of leads. This give-away tactic isnâ€™t new and has many names (freemium marketing). Itâ€™s something that Iâ€™ll be writing on soon. 5. Monetize: How to Convert Customers Your content should sell and convert for you. Thatâ€™s my mantra for getting sales from any platform. You donâ€™t need to turn your podcast into an Ad for your business. Sold people will on their own find a way to buy whatever you offer. Thatâ€™s when you need a call to action (CTA). You have to tell people to click the link to your landing page. Remember, this is different from selling. A Call-To-Action doesnâ€™t aim to conceive but remind people who are already convinced. In my podcast, I put a link in the description to my consultation page. Plus, between the topic I make an audio prompt to let people know, they can work with by going to my website. You can do this in the middle or end of your episode. With this, I convert about 1 client per 2000 listens. Not bad for an eight-month-old audience. I get 1 good lead for every 1000 listens. The average industry conversion rate is around 1.13%, something I aim to hit. A good CTA can boost your click-through by around 250%. But remember, itâ€™s only a door-way to your store. Your content will decide whether they come or not. 6. Takeaway: Donâ€™t Try to Be Perfect I will tell you a secret about my podcast. Itâ€™s not one. I donâ€™t record it. I take my Youtube videos and convert them into audio. So, what I release on youtube every week on, gets published on my Podcasts. With 10â€“15 mins audio clips and no extra effort, I was able to make six-figures. No content-marketing has made me money that quick. Not even Medium. I didnâ€™t tell you this earlier. Because uncommitted people would take it as an excuse to avoid effort. Iâ€™m assuming you are serious about this as you have read till now. Why did I tell you this now? To help you catch your breath. The process I shared above is simple. But you will make it complicated if you try for perfection. You donâ€™t need to worry about the perfect content or titles in the beginning. Follow the basic guidelines and be consistent. Your good enough is good enough to start. Quantity will eventually lead to quality and youâ€™ll see results. Grow your business with me, Iâ€™ll coach you.
The trick to getting things done is to address your pain points. Sean Kernan 3 days agoÂ·5 min read Productivity bottlenecks are common in business and factories, and can also be called â€˜limiting factorsâ€™, the one chain in the link that holds the rest of the process back. You see it when Coke bottles pile up on a factory belt because the liquid fillers are taking their sweet time. You see it in a company when a new line of code forwards too many helpdesk tickets to one person. Bottlenecks get messy, fast. Many of us have them within us: browsing social media, struggling to start, snacking, TV, animosity towards the task. They interrupt a normal workflow and slow down your production. For me, itâ€™s going down â€˜click holesâ€™ (internet black holes), which may start with me checking my Facebook, and end with me studying Giant African Snails for no reason. Productive people are masters of pushing past these bottlenecks and distractions. If you can control them, youâ€™ll unlock substantial gains to â€˜push productionâ€™. Use a Daredevilsâ€™ Method Being on the cast of MTVâ€™s Jackass came with the implicit mandate that each member pulls their weight. This translates to performing your share of crazy. Famously, in one scene, a cast member says to the director, â€œOkay, Iâ€™m going to do this â€” but this gets me out of something bad I have to do in the future.â€ The director agrees, â€œAbsolutely.â€ Then the cast member does a ridiculously logic-defying stunt. Steve-O, one of the shows bravest daredevils, is often asked how he works up the motivation and courage to do these tasks. He uses a simple tool that his fellow castmates use, which is actually highly useful for productivity: He counts down from five. This is actually a Pomodoro Technique and one I embraced a few years ago. Most Pomodoro techniques are focused on stretches of time for productivity and time until starting a task. For example, I also do five-second countdowns to start my tasks. You can also set five and ten-minute alarms if you need more time. I find that counting down from five, saying the numbers out loud causes my mind to drown out anti-productive thinking. Additionally, Iâ€™ll also focus on a small part of the task. If Iâ€™m going to the gym, Iâ€™ll focus on putting my shoes on first. After that, Iâ€™ll focus on getting in my car and going from there. Take it one step at a time. It lowers the mental weight of your tasks. Use Controlled Productivity Races Iâ€™ve been using David Allenâ€™s 2-Minute Rule to prevent my house from tumbling into chaos. It has a simple premise with an unexpected benefit. The premise: if it takes 2-minutes or less, do it now. The unexpected benefit: it causes you to rank order micro-tasks. The process of turning it into a race forces you to cut the fluff from your actions. Also, donâ€™t say, â€œI have to clean for 2 minutes.â€ Frame it as, â€œHow much can I clean in 2 minutes?â€ â€œProcrastination is the thief of time, collar him.â€ â€” Charles Dickens You can do quite a bit in a mere two minutes: wipe down furniture, sweep a room, or load the dishwasher. This drill often starts with me being a lazy bum and turns into flying through tasks and feeling revved up. Iâ€™ll often combine the 2-minute rule with the countdown rule. Most large projects can be broken down into smaller projects. Just as a complex problem is usually just a bunch of simple problems combined. The Power of Incentives I wonâ€™t say Iâ€™m addicted to coffee, but I spend hours fantasizing about having my next cup, and I may occasionally dream of swimming in giant lakes full of black coffee beans. Iâ€™m only a few years from 40 and Iâ€™m protecting this last vice with every ounce of my free will. Itâ€™s my last bridge. Then itâ€™s all crotchet and bingo from there. However, I do set rules for things like coffee. They make a big impact on my writing productivity (Iâ€™m a writer). I try to write in one and two-hour blocks and often have some sort of reward waiting for me at the end of that tunnel, â€œI can have some coffee after I blast out this article.â€ This strategy isnâ€™t as infantilizing as it sounds. It actually has two immediate perks. Create a reward. Put a productive roadblock in front of it. The Secret Sauce of What You Eat Years ago, I was stuck in this up and down of sugar highs and lows, hitting and crashing. I was constantly complaining about being tired and not having energy. Iâ€™d been a competitive college swimmer and got used to eating 7,000+ calories a day. Then, when I stopped swimming, I kept the insane diet going and was paying the price. Unhealthy eating is a leading cause of low energy and I was another statistic. When I finally stopped my binge eating and addiction to sweets, I felt ten years younger and was brimming with energy. I challenge you to try a week of eating healthy, drinking lots of water, with no fast food, no preservatives. Then watch the windfall of energy by the end of that week. I set four daily goals each day. Three are productivity-related. One is diet-focused. It can be as simple as â€œDrink 6 large cups of water.â€ Remember that energy fundamentally comes from food. Donâ€™t put dirty fuel in your engine. Lastly, Visualize Disaster Complacency is often a product of people underestimating the true danger of their predicament. â€œIâ€™ll do it later.â€ â€œIt wonâ€™t be a big deal.â€ â€œNobody will care.â€ Some of my biggest mistakes came from this attitude. Today, I often envision consequences unfolding to get myself moving. Put another way, Iâ€™m a firm believer in Murphy's Law: anything that can go wrong, will go wrong (and most things take longer than expected). For example, if Iâ€™m writing, Iâ€™ll think, â€œI should just review this draft one more time. Iâ€™ll kick myself if this piece doesnâ€™t get curated and I reread it and find some stupid, glaring typo.â€ The crazy thing? I often find a stupid glaring typo. I was discussing attention to detail and the importance of proofing things and my friend, Colin Jensen, COO of Garteiser Honea, said, â€œI use the phrase, â€˜Nobody will notice this, except my boss.â€™â€ Itâ€™s a perfect reminder for those feeling complacent on a task. Recap
Ten years ago, my partner and I were part of a pilot program for Yelpâ€™s premium reviewers program. They sent a select group of locals to a new restaurant every two weeks. The restaurant would host us in hopes of making a great impression. They rolled out the red carpet. We felt like kings. Incredibly, the food was always free. In turn, these premium Yelp members, my partner included, would write big sweeping reviews on Yelp that got lots of views. The entire concept of Yelp was neat and exciting at that point. It was a place designed to give a more sophisticated perspective of businesses, free from the chaos of the outside internet. Iâ€™d have never guessed the monster Yelp morphed into. Their sales reps are bullying businesses A damning report came out in early November via Business Insider. It exposed more than a dozen reports of reps being pushed to mislead businesses about their products. 21 separate employees stated they sold products to businesses while fully knowing owners didnâ€™t understand what they were buying. One employee, who remained anonymous, went so far as to say, â€œItâ€™s not uncommon to hear people leave out certain pieces of info and not uncommon for managers to encourage you not to explain the program fully.â€ A plant store owner, Melanie, was contacted by Yelp in March. The reps claimed they were trying to â€˜give backâ€™ and help small businesses during the pandemic. In this, they offered a $700 free advertising credit. She agreed. Then, the following month she was hit with a $52 charge. She protested and Yelp said the credit was part of a broader purchase. A second bill for $570 arrivedÂ . Yelp continued to refuse refunds. All until she publicly contacted the Better Business Bureau. Shortly thereafter, Yelp gave her the money back. This was just one snapshot of the coming problems. Restaurants get held hostage by customers In the years since the pilot program, the idealistic startup days have been overshadowed by the capitalistic demands of profiteering. Consequently, the dynamics between businesses and Yelp have become adversarial or symbiotic, with little room in between. A study by Northwestern University revealed that Yelp reviews tend to be harsher, being .7 stars lower than Google on average. ThisÂ .7 becomes more compounded at the ground level, where smaller restaurants have only a handful of reviews. That pilot program we were apart of has evolved into elite members. And these elite members now have powerful voices, with their reviews hitting hard against these smaller businesses. My friend owns an Italian restaurant here in Tampa. And when a premium comes in, they end up jumping through hoops just to make sure these members are happy. But worse, if there is so much as a minor kerfuffle, they usually end up comping the member out of fear of a bad review. Premium member power trips are only the beginning of the troubles though. Thereâ€™s a conflict of interest between Yelp and its mission Yelp CEO Danny Terran famously claimed that Yelp is the Wikipedia of reviews, open to any and all to participate. Yet the company actually has little in common with Wikipedia. Itâ€™s publicly traded and it reports to a board of shareholders. Its purpose is simple: maximize profit. This business model stems from revenue generated from the businesses that appear on the platform itself. Small businesses pay to play and appear first in search results. They also pay to get better reviews on the platform. But how do you get them to keep paying if they donâ€™t want to? Insert Botto Bistro in San Francisco. The owner agreed to do six months of advertising after being called 20 times per week by Yelp sales reps. Then, after he discontinued his membership, Yelp deleted his best reviews and a bunch of negative reviews mysteriously appeared. He sued and courts affirmed that Yelp has every right to do what it wanted with its reviews. He rebelled in fairly epic fashion: The reviews ended up being pretty hilarious: Yelpâ€™s practice of adding negative reviews and deleting positive ones is well documented throughout the country, particularly by small business owners. There are other tactics being used as well. There are other super shady tactics afoot When you click on a restaurant in the Yelp app, they recently added a new option: The top option, they secretly swapped out the restaurant's actual phone number and added in a Grubhub number. If you call that top number, they charge the restaurant a 15â€“20% referral fee. If you call the bottom number â€˜general questionsâ€™, it takes you to the actual restaurant. Worth noting, Grubhub and Yelp have a long-term partnership. Even worse, many restaurant owners report getting calls via these numbers and the customers hang up before saying anything. This causes them to get charged and itâ€™s widely alleged that Yelp employees make these calls to juke up referral revenue. Itâ€™s led to several huge lawsuits, including one by Munish Narula, who sued Grubhub for $5 million in 2018. The takeaway for us After reading testimonial after testimonial, it seems like the worst thing a business owner can do is answer a call from a Yelp Sales rep. The biggest driver of their revenue is pushing â€˜reputation managementâ€™ services and search results. If you field their call and donâ€™t buy-in, there can be instant repercussions. I spent seven years as a financial analyst so I couldnâ€™t help but dig into their financial reports. As suspected, right there on the annual financial report, listed under its risk section: Risk factor: â€œFavorable ratings and reviews could be perceived as obviating the need to advertiseâ€¦â€ Obviating was a remarkably thick word to use when â€˜removeâ€™ would have worked just fine. Notably, they also put this point in the least viewed section of the risk list (the third quartile). My point: they can easily boost revenue by deleting good reviews and mysteriously surfacing bad reviews. After all, what business wants bad reviews? So businesses, donâ€™t succumb to bullying tactics by them. If they contact you, Iâ€™d just hang up the moment they say they are from Yelp or come up with some reason to get off the phone. Hell, pretend you donâ€™t speak English. Most Yelp offices are regional and have high-pressure environments to hit sales goals. They have mechanisms in place to push owners to their side of the table fairly easily. Consumers â€” stick to Google reviews and Facebook reviews. Itâ€™s always a toss-up on reviews anyway. So take them as a grain of salt. Supporting Yelp is supporting a mafia-style business. I came into this article with an open mind, willing to see Yelp's side of it, but the evidence couldnâ€™t have felt more overwhelming â€” they are a shady enterprise. A â€˜for profitâ€™ review system is by design, destined to become adulterated by the needs of monetization.
June Kang 1 day agoÂ·6 min read In childrenâ€™s books, the sun is always colored yellow. Then, lines are uniformly drawn coming out of the sun to show that it is shining. Children who grow up reading these kinds of picture books all tend to draw the sun in the same way. Would this be any different for art students? The answer is â€œno.â€ They may be able to use better techniques, but we can observe a similar trend among them. Our education system instills in us the conventional wisdom of â€œthinking inside the box.â€ One thing might also be another thing, but we approach it through the lens of â€œthis is the only optimal way to solve this.â€ However, there are thousands of possible answers to the questions that exist in the real world. â€œProblem-finding is a creative process whereby individuals develop original ideas.â€ Frank LaBanca We live in a world of problem-solving. Problems are often intermingled in complicated ways, making it impossible for only one single answer to exist. We have lots of objectives focused on problem-solving, but we do not spend an equal amount of time carrying out proper â€œproblem finding.â€ The essence of adopting a problem-finding mindset is a willingness to question the status quo and more importantly, not to be afraid to truly think about the concept of ambiguity itself. Here is what Einstein has to say about problem finding. â€œThe formulation of a problem is far more often essential than its solution, which may be merely a matter of mathematical or experimental skill. To raise new questions, a new possibility, to regard old problems from a new angle requires creative imagination and marks a real advance in science.â€ Just open a dictionary, and you can easily find the definition of â€œimagination.â€ The dictionary defines imagination as â€œthe act or power of forming a mental image of something not present to the senses or never before wholly perceived in reality.â€ To understand the basic concept, consider the Chinese term for â€œelephantâ€ â€” è±¡ (xiÃ ng) â€” which also means â€œimage, imagination.â€ At the time, ancient Chinese people did not actually know what elephants look like. At the time, it may as well have been an imaginary animal in China, so they drew images of elephants without having ever seen them, using only the pictures in their heads; that is how the word â€œimaginationâ€ was born. In real life, unfortunately, the reality is often not what it seems and knowledge often limits our thoughts. When we know a lot, we are less likely to ask the questions â€œwhat ifâ€ and â€œwhy.â€ Asking â€œwhat ifâ€ and â€œwhyâ€ allows us to expand our perspective and reframe problems. Listening to individual stories and trying to recognize patterns can be another great source for problem finding. This allows us to build a deep understanding of peopleâ€™s emotions and underlying motivations, which in turn activates our problem-solving mindset. Other than simply trying to explore topics on a surface level, what are some under-discussed conversation topics that we could explore? People are focusing more on self-care these days and many people are experiencing a crisis of purpose fatigue. However, despite this, online searches for â€œhelping othersâ€ reached higher levels than ever before, showing that people still have a desire to channel their energy outward to help those around them. â€œWhen we can tap into empathy and create experiences that really do connect with people in a more resonant and emotive way, we have more successful end results.â€ Unknown When we feel comfortable with ambiguity and uncertainty, we are more likely to roll the dice and take action. This means that ambiguity and uncertainty can serve as sources of boldness. From my perspective, action through boldness starts with the right combination of curiosity-based observation and instinct from a personâ€™s unique life experience. There is a difference between â€œsolvingâ€ and â€œfindingâ€ I want to briefly discuss the difference between problem-solving and problem-finding. I think this can be boiled down to getting attention vs. paying attention. The reason for this is simple: If we do not intentionally make a distinction between these two concepts, we unintentionally forget about the distinctions throughout the process. When we are asked to identify problems, we often tend to propose a solution. The reason for this is because we all have preset, go-to solutions, and expertise in certain areas, so it is very natural to enter problem-solving mode using the skills we have traditionally excelled at. It is always tempting to transition into a problem-solving mode in order to produce a quick outcome. However, it is absolutely crucial to make sure we are not defining a problem based on our pre-existing biases and assumptions. We will not solve much of anything in the early stages, but we will undoubtedly end up with various types of problems we eager to solve. Let me borrow from Simon Sinekâ€™s infinite game mindset to further explain my philosophical theory here. In a finite game environment, like a new business pitch or an interview where there are fixed rules with known players and agreed objectives, selling expertise and our previous success might help us reach an immediate outcome and guarantee a similar level of success. However, the finite game mindset naturally narrows our vision to a fixed objective, so this may result in missing out on other creative ways to create new opportunities. â€œBeing busy can cure boredom, but it has a side effect: it shuts the door to serendipitous discoveries.â€ Adam Grant Consider the example of Michael Phelps. Michael Phelps became the greatest swimmer and the most successful Olympian in the world. What happened after he finally achieved his finite goals? He fell into a major state of depression because his goals were finite, and he wasnâ€™t able to identify what type of infinite game he was in other than being the best swimmer in the finite game. He later identified a purpose in his infinite game and became an ambassador for mental illness. Our business has many different types of finite components, but we are in an infinite game with no finish lines and the rules are always changing. We always want to build new and different types of relationships to grow our business and meet the new market expectations. In this game, everyone is responsible for identifying untold problems, rather than deferring to senior leadership, while keeping ourselves in siloed practitioner mode. I am also guilty of doing this. I often put myself in siloed practitioner mode and tend to come up with solutions before asking whether the problem is the one I should be solving. Problem finding is less about selling our expertise and more about paying attention to what is not told by helping audiences, customers, clients, or ourselves identify various types of problems we are eager to solve together over the long term. We really need to have an H-type mindset to be people who understand two different worlds. This mindset needs to be a combination of the right amount of optimism and pragmatism, not one or the other. It can neither be an optimistic mindset without vision or a pragmatic mindset without understanding the big picture. Depending on the types of problems we could find, that also means there could be different combinations of trials and experiments; it requires us to cross lanes as we swim forward â€” not limited to a business mindset, digital-first approach, design-driven solution, or a marketerâ€™s perspective. This is the secret to bold action in the face of problems we cannot fully comprehend. We need to be prepared to think differently about the tools we have and the infinite ways to approach problems, both from the perspective of solving problems and searching for them.
Of the 48 in all, these 6 matter most Itâ€™s one thing to write a best-seller on your first try. But what about writing a book so thorough and controversial it has to be banned from prisons as a safety precaution? With over 1.2 million copies sold in the U.S. since its first publication in 1998, Robert Greeneâ€™s debut book The 48 Laws Of Power has had a divisive response from day one. Sometimes referred to as â€œthe sociopathâ€™s bibleâ€, its success spawned a series of listicle-on-steroids sequels, including The Art Of Seduction, The Laws Of Human Nature, and The 33 Strategies Of War (Another publication banned in some prisons). In a 2012 interview with The Guardian, Greene stated his motivation for The 48 Laws Of Power was â€œto demystify the dirty tricks of the executives he encountered during a dispiriting period as a Hollywood screenwriter.â€ Clearly the bookâ€™s content struck a chord: Kanye West and Jay-Z have referenced the work in song lyrics; Calvin Harris has law #28 â€” â€œEnter with boldnessâ€ â€” tattooed on his arm; and An adaptation of the book into a series with Drake as executive producer was greenlit for Quibi earlier this year. Quibi later went up in flames. But itâ€™s probably safe to say if Drake wants to produce your book, youâ€™re onto something. The bestseller became so prevalent in rap circles in the mid-2000â€™s that The New Yorker coined Greene â€œHip hopâ€™s Machiavelliâ€. Chances are you donâ€™t plan to usurp a dynasty anytime soon, so the more diabolical laws â€” such as law 14, which encourages you to â€œPose as a friend, but work as a spyâ€ â€” will be entertaining at best and toxic at worst. As entrepreneurs, though, we must be privy to the dynamics of persuasion and influence. These 6 laws of power in particular are ones youâ€™ll want to know like the back of your hand. Law 9: â€œWin through actions, never through argumentâ€ â€œIâ€™ll let my racket do the talking.â€ â€” John McEnroe Instead of endlessly pitching yourself, your product, or results youâ€™ve never actually accomplished yourselfâ€¦ walk your talk instead. To land your message more deeply with an audience, explore ways to demonstrate and show your point rather than argue it.Our brains process visual information 90,000 times faster than text, and research from MIT found imagery registers in our minds after just 14 milliseconds. The literary device version of this law is â€œShow, donâ€™t tellâ€; itâ€™s sometimes related to a Chekhov quote: â€œDonâ€™t tell me the moon is shining. Show me the glint of light on broken glass.â€ Takeaway: Never tell that which you can show instead. Law 10: â€œInfection: Avoid the unhappy and unluckyâ€ As the saying goes: â€œYou are the average of the five people you surround yourself with the most.â€ Unhappy, complaining-centric people are like entrepreneur kryptonite and will sink your spirit rapidly. This is especially important to discern if you spend most of your day around non-entrepreneurs. Perks like being your own boss or creating your own income feel like impossible pipe dreams for some 9-to-5-ers. Occupy your mind with quality brain food from those who walk the talk. Nothing sidelines an aspiring entrepreneur faster than a peanut gallery of naysayers â€” distance yourself from misery and victim mentalities. Takeaway: Take care of the company you keep. Law 13: â€œAppeal to peopleâ€™s self-interest, never their mercy or gratitudeâ€ As you develop content, a product, or an offer, always be asking yourself â€œWhy would people care about this?â€ Why make your own bone broth instead of buying it from a store? Why set up an email list instead of dick around on Instagram? Why care? Outline for your consumers not only why they should care, but also whatâ€™s in it for them. Your readers arenâ€™t egomaniacs. Itâ€™s the human condition and we canâ€™t help ourselves; we want to read that which can help us feel fulfilled or move our personal interests forward. Add context around what people will get out of reading, watching, or listening to your content and youâ€™ll find it much easier to hold someoneâ€™s attention. Takeaway: Outline not only why readers should care, but also whatâ€™s in it for them. Law 18: â€œDo not build fortresses to protect yourselfâ€“isolation is dangerousâ€ Hereâ€™s something no one tells you about entrepreneur life: It gets lonely real quick. Itâ€™s a loneliness that is simultaneously kind of shaming, because you theoretically have freedom, creative autonomy, and the ability to create whatever income you chooseâ€¦ so you should be happy and fulfilled all the time, right? As you chug along with the grind, a trusted network will not only help you keep your sanity, but also provide a valuable sounding board. The mind is a dangerous place â€” donâ€™t go there alone. Itâ€™s rare â€” dare I say impossible â€” for an entrepreneur to get everything right on the very first try. Missteps are guaranteed, and having confidantes can help you get out of your head and back into action. Resilience is a virtue. Takeaway: The mind is a dangerous place â€” donâ€™t go there alone. Law 23: â€œConcentrate your forcesâ€ A lack of focus has killed off millions of brilliant business ideas over the years. Ensure that your efforts actually move you closer to your goals. An arm of my business Iâ€™ve always been curious aboutâ€“â€“low-cost online coursesâ€“â€“took off this year after years of stagnation and half-assery because I zeroed in on a 1â€“1â€“1 framework, a process I learned from business coach Rachel Rodgers: One promotional platform (If youâ€™re reading this, youâ€™re on it right now) One sales mechanism (Mine was previously sales calls, but is now moving into webinars) One product or sequence of products that results in transformation In switching to a 1â€“1â€“1 framework, I have fewer decisions to make and fewer excuses to choose from. My decisions actually move my business forward. Also, as it turns out, I actually like funnels. I just hate most funnel marketers. Takeaway: The more you clarify your focus, the less often distraction will derail your efforts. Law 41: â€œAvoid stepping into a great manâ€™s shoesâ€ Itâ€™s good to study other content creators and entrepreneurs. But if you spend all your time attempting to mimic the success of other people, your North star will become more about trail following than trailblazing. How can what you offer be better or more relevant? How is working with you different or unique? What problems does your market have that go on unsolved? As Sally Hogshead says: â€œDifferent is better than betterâ€. Tons of people sell what I sell and write about what I write about. If I try to be just like themâ€¦ itâ€™s game over. When Iâ€™m myself, the rules can shift in my favor. Clarify how your approach is different or unique and youâ€™ll have an easier time slicing through the online noise. Takeaway: Follow trails to learn the ropes. Then go on to blaze your own. Final takeaways Robert Greeneâ€™s The 48 Laws Of Power is not a light read. But its lessons from history give critical insights on influence and persuasion, which are imperative if youâ€™re an aspiring entrepreneur. Dial in your power generators, make the necessary adjustments, and youâ€™ll find yourself better positioned for success both now and in the future. ðŸ‘‹ðŸ¼ P.S. â€” Wanna know do I write 5+ articles/week for websites like Entrepreneur and Fast Company? One word: Templates. Iâ€™ve put my best ones and top tips into a free downloadable toolkit. âž¡ï¸ Grab your free toolkit right here.
As a punk millennial kid, I grew up with Facebook. In 2005 I went to a nightclub with a Harvard student visiting Australia. I said, â€œHey, can you send me the pictures from tonight?â€ She said, â€œYou can find them on The Facebook.â€ I was pissed. I didnâ€™t understand what a book full of faces had to do with me getting the pictures from the night to humblebrag to all the people who didnâ€™t have drinks with the DJ and kiss his ass. My Harvard friend sent me a link to join Facebook. It was my second step (after myspace) into the social networking world. Then the movie The Social Network came out. It told the story of Facebook and the smart ass kid who created it. This was the first time I was introduced to the Winklevoss twins. They were portrayed in the film as rich elites who would crush a toddlersâ€™ toy collection into tiny little pieces and leave the dust on the kidsâ€™ bedroom floor next to the red fire truck. I saw the Winklevoss twins as the worst human beings in history. They tried to steal Facebook off Zucks, and take a sketchy idea off him and call it their own. I despised them and their elitism. Years later Ben Mezrich wrote a book about the twins. I completely changed my mind. The inaccurate portrayal of the Winklevoss twins was done for show. In the heroâ€™s journey format of Hollywood storytelling, Zucks was the hero and the Winklevoss twins were evil. There had to be an evil enemy in the movie and the Winklevoss twins were an easy target any audience could learn to hate. Nobody really knows what happened in the early days of Facebook and who created it. It doesnâ€™t seem to matter. Everybody who was involved with Facebook made plenty of cashola. Founding a tech company doesnâ€™t make you a god or goddess. Progressing humanity is the real magic trick. After suing Mark Zuckerberg and winning, the twins went on to accidentally become part of a different social network. The twins said this in a recent interview: Money is the greatest social network of all. The twins were referring to their work in rebuilding financial services for the internet. The current system we use to move money around online and across country borders is a system of IOUs. You think youâ€™ve transacted with a business but the actual value from your transaction shifts from one account to the other much later. The balance summary of your favorite money app is an IOU. Mark Zuckerberg thought he had won the social media battle with his platform, Facebook. He actually lost. Centralized social media platforms like Facebook are going through many challenges. A lot of internet users are calling the end of Facebook. I agree. Privacy is too important to ignore. Manipulating usersâ€™ minds with dopamine tools such as likes is old and outdated. Social media needs a massive upgrade. Facebook will die. Even our grandparents will stop using it to see what we did at the beach with friends on the weekend. I have changed my view on the Winklevoss twins. I now believe they are two of the most important humans in history. Why? They are reinventing money. They are also recreating how digital assets like writing and photographs are bought, sold â€” and most importantly â€” OWNED, via their platform nifty gateway. Ownership on the internet is broken. Trust is too easy to be misused by money-hungry startups addicted to VC money and their own vanity metrics. The Winklevoss twins are slowly unveiling their future of the internet. It started by making internet money like Bitcoin and Ethereum easier to buy. Then the twins found the greatest problem with internet money: it needs regulation, and large institutions to survive. The twins have spent the last few years gaining the trust of regulators, banks, hedge funds, and sophisticated investors, using the idea of internet money they found on a post-2008 recession holiday. This time the twins didnâ€™t care whether they invented the idea they were backing. Coming up with a unique idea like they attempted to do with social media was the least of their worries. This time the twins took an existing idea and made it better. The result has been the large scale adoption of internet money with companies like Visa and PayPal all getting on board â€” as well as Wall Street tycoons like Paul Tudor Jones. The Winklevoss twins have thrown their time at a different problem. Itâ€™s the same problem Jack Dorsey is trying to solve: the unbanked. 31% of the world doesnâ€™t have access to banking services. The concept of internet money can take currency from being racist, and turn it into the peoplesâ€™ exchange of value once again. Why should billions of people be excluded from the banking system? They shouldnâ€™t. Banking is a basic human right. Many think the Winklevoss twins are solving this problem for financial gain. I completely disagree. The twins have already become billionaires thanks to their early investment in bitcoin. Many critics of the twins believe they are promoting internet money to inflate their investment in bitcoin. I disagree, again. All you have to do is watch the twins doing daily interviews (like I have) and you can see the sincerity written all over their faces. And in the way they deliver their mission and put it into eloquently articulated words. The Winklevoss twins arenâ€™t assholes at all. That statement is shortsighted. Theyâ€™re trying to solve a huge problem because they can â€” itâ€™s as simple as that. Not everybodyâ€™s intentions are evil. There are still decent human beings on the planet. Not every billionaire should be shamed and called a cheater. The Ultimate Demonstration of Humility Zucks realized late in the game that money was the real social network he missed out on. He acted on his FOMO by creating Facebookâ€™s Libra project. It was promptly declined by regulators and many of the founding partners removed themselves from the project. Zucks and the Winklevoss twins made up. What is impressive about the twins is their humility. Unlike the movie portrayal of their characters, they now regularly acknowledge what Zucks has built. They could be bitter and they could shame Zucks, yet they donâ€™t. I said in the headline that the Winklevoss twins are gods. I donâ€™t mean this in a bro hug kind of way, or as though we should worship them. The twins are gods because theyâ€™ve figured out the point of their work in tech: to solve a problem bigger than themselves. They donâ€™t need the money; theyâ€™re already billionaires. Itâ€™s clear theyâ€™re building something much more important. The Winklevoss twins have already helped change history with their work to legitimize digital money. Their next task is to make the internet and the currency we transact in democratic again. I believe the twins will succeed. Their ability to unite and inspire humanity around a problem is infectious. They went from the spoilt Facebook twins to an invisible force for good. The Winklevoss twins are the ones to watch. If they can make their mark using humility then why canâ€™t you? Itâ€™s not whether you win, or what you get credit for. Itâ€™s the problem you solve and why it truly matters. Join my email list with 40K+ people for more helpful insights.
As hard as I tried, I couldnâ€™t shake off the thought of another stream of income meandering into my cash pool. The excitement bubbling up within my mind swallowed all the doubts I had. High on excitement and big on promise, I dove head-first into the side hustle hole. That was in mid-2017 when the spark of the flash lured me to the photo studio. I wanted to be like a colleague who shot weddings every other weekend. He persuaded me I could even divert into architectural photography, which, for a real estate surveyor, was music to my ears. In 2019, a colleague suggested we co-authored a peer-reviewed article. I didnâ€™t know any better than to offload rounds of rapid-fire follow-up emails to the editorial team. Judging by the echoing silence I got in response, I guess most of them went into the trash folder. Dejected, I realized I could channel the frustration into writing articles. Between starting a WordPress blog and writing on Medium, the pen is proving to be a powerful weapon, if not quite mightier than the sword just yet. As the year trudges to the finish line, I sat down to count my blessings and recount the lessons Iâ€™ve learned combining two side-hustles with my career in real estate. They were bountiful, thanks. But here are only three. The more you have on your plate, the more time you can create When I thought of starting a photography business on the side of my grueling work schedule as an Estates Assistant, part of me wondered whether I would have any time left to brush my teeth. But the siren call of more money was enough to convince me to at least give photography a shot. Even though I smiled at the first paycheck, my second career threatened to take up too much time if I didnâ€™t zoom in and focus on the fine details of my time allocation. So I did a time audit. It turned out the mindless scrolling on Twitter and Facebook, and my mini-addiction to watching every WhatsApp status took between two and four hours of my day. I knew if I focused less on my friendsâ€™ rants and strangersâ€™ stunts online, I could take back some of those hours. Adding another side hustle in writing meant I had to keep an even tighter rein on my time. Challenge accepted. Now, outside of my morning routines and breakfast, I try to write about two hours Monday to Wednesday and read for another hour before midday. My lunch break, mail time, and social media take over until around 3 p.m. A 20â€“30 minute nap follows. In the evenings, I keep a fluid schedule that mixes taking writing courses, doing research work, and reading articles online with spending time with the family. I try to schedule photo sessions and property inspections for Fridays, but depending on the clientsâ€™ demands, these could fall on any day. Iâ€˜ve scheduled my laundry for Saturdays, mostly while I listen to the Guardian Football Weekly podcast or a few NBA-related pods from ESPN. I am free to go to church and do a whole lot of non-work-related activities on Sundays. But most importantly, I do not starve myself of the recommended seven to nine hours of sleep. I stay on schedule about 60% of the time, but thatâ€™s a decent mark considering the chopping and changing I have had to make as my commitments change. Itâ€™s hard to speak for everyone because we are all different. For example, I donâ€™t have pets, kids, or extended family to cater for. And I have a partner who respects my schedules. But I have realized that the more work I have had on my hands, the more creative I have become with my time, leaving me with more time to spare. More work will make you better than you could imagine I always try to be the best in every endeavor I take up. This means I have had to read more pages, endure harder practice sessions, battle tough failures, and learn from those failures. But it turns out thatâ€™s the best route to a bountiful harvest from different fields. YouTube keeps feeding me with lots and lots of photography tutorials. And thanks to platforms like Edx and Coursera, Iâ€˜m taking courses to learn how to write better, how to communicate clearly. I am now aware of some basic rules of writing and communication, and I can see when a newbie breaks them. When my photography side business took off, I soon realized there was more to it than posing people and clicking shutters. I had to learn skills like sales, marketing, customer relations, and bookkeeping on the fly. Iâ€™ve also learned to network and support other writers â€” well more than what I thought writing was about. But when I look back, I realize I would not have learned most of these skills so fast if I was only on my first career as a real estate surveyor. I am happy with the progress Iâ€™m making. Similarly, when I look at a few of my colleagues who have also added different dimensions to the careers school prepared them for, I see how theyâ€™ve gotten better. Some have mastered public speaking, teaching, disk jockeying, etc. Yes, I can say I have gotten better and learned more skills from adding two side hustles to the career school prepared me for. Youâ€™d best ignore a lot of the grind and hustle advice If I had listened to some of the advice on hustling and grinding, I wouldâ€™ve ground myself into powder. â€œGrind, work hard, sleep less, work, even more, keep your nose to the grindstone and grind harder,â€ youâ€™ll hear some evangelists preach. A colleague even advised me to wake up an hour earlier, scrap the nap and sleep an hour later than my usual bedtime so I can steal two and a half hours from nature. I bought it, not knowing any better then. But it didnâ€™t take long for me to realize I was harming myself more than I was benefiting from the marginal gains I enjoyed. That was even before I read about some of the effects of the lack of adequate sleep. People have gone after Gary Vee for leading a grind and hustle culture that threatens to turn people into mules glistening with sweat. But when I listened to him, I realized that was not what he meant. In this YouTube video, he said hustling means going all-in and maximizing the 15 hours a day when heâ€™s awake. In another video, he says he takes between four and seven weeks of vacation in a year. Yes, some startup founders have had to work harder, sleep less, and take even fewer vacations, depending on their roles and goals. Iâ€™m no organizational psychologist, but I have also realized everyone doesnâ€™t require that level of work to be successful. Thankfully, Iâ€™ve not had to survive on four hours of sleep a night, as some ultra-hard working people do. Occasionally, a tight deadline pushes me past my 10 p.m. bedtime or takes up most of my day, but thatâ€™s normal. When I see the threat of overworking looming, I occasionally delegate roles like location scouting and photo editing. This frees up even more time for me to recharge my batteries. In short, trash a lot of the grind and hustle advice that says to work yourself into bare bones before you can taste the juicy fruits of your labor. Conclusion I may not be the archetypal entrepreneur who came up with a disruptive idea that turned the world upside down to make it better. People like that are the exception. But for me, when the side-hustle train â€” with lots of people hanging from the sides â€” curled into view, I hopped on and joined the party. No, none of them rakes in a million dollars in revenue yet, but that was never the goal. Along the way, I have learned a lot of invaluable lessons, the three most important being: 1. The more you do, the more time you can create. 2. The more you do, the better youâ€™d become. 3. People misunderstand a lot of the grind and hustle advice. Yes, you can run a small business or two on the side of your career and still have your body in one piece. Itâ€™s hard, and you may have to be super creative and innovative. But since when didnâ€™t you have to possess those qualities in the 21st century?
Patrick Sisson Dec 9Â·18 min read â€œNone of us knows how long this crisis will last,â€ pleaded Robert Reffkin in a letter to Nancy Pelosi and her Republican counterpart Kevin McCarthy in March. Reffkin, CEO of real estate startup Compass, was urging Congress to include independent contractors like real estate agents â€” some 2 million of them in the United States, according to the National Association of Realtors â€” in its economic stimulus package. In his plea, Reffkin cleared up any misconceptions about the professionals: They were entrepreneurs and small business owners who represent the backbone of the U.S. economy, personify the American dream, yet typically only earn less than $41,800 per year. â€œWe do know that for real estate agents, the economic pain will last even longer than it will for those in many other professions.â€ The heartfelt missive on the plight of his industry during a pandemic â€” he even mentioned his mother, who works as an agent for his firm â€” came during a particularly bleak moment for Compass and its more than 11,500 independent contractor agents who depend on it. With the backing of $1.6 billion in venture capital, including a $450 million infusion from SoftBank in December 2017, Compass had been on a growth streak, grabbing market share in the nationâ€™s most expensive housing markets and becoming a major player in high-end residential real estate. But as everything, including real estate sales, ground to a halt in late March, it left a commission-dependent workforce desperate for signs of life. Reffkin sent his letter on a Thursday. The following Monday, he laid off 375 of his full-time staffers, characterizing the moment as an â€œeconomic standstillâ€ and forecasting a 50% revenue decline over the next half a year. By mid-April, after stay-at-home orders and shutdowns froze the economy, new weekly home listings in New York City, one of the companyâ€™s largest markets, were down 89% year over year. Worries of hollowed out, abandoned urban centers began to take hold, a particularly grim scenario for a luxury urban real estate company. By this summer, Compass hadnâ€™t just recovered, it was posting record-breaking monthly revenue every month from June to October. While the urban exodus story was largely a myth, wealthy families would begin relocating to bigger, more spacious homes, or purchasing second homes. All of this made Reffkinâ€™s letter seem as dated as a sepia-toned dispatch in a Ken Burns documentary. By late spring, home buying had started to swing back with a force no one had anticipated. Since then, real estate sales, particularly expensive single-family homes, have been booming, hitting a 14-year high in October, per the National Association of Realtors, the same month the number of homes on the market priced over $1 million doubled and the median home price jumped 15%, setting a record high. America Runs on Dunkinâ€™, But Dunkinâ€™ Runs on Private Equity How an iconic company became a lucrative vessel for dealmakers marker.medium.com By this summer, Compass hadnâ€™t just recovered, it was posting record-breaking monthly revenue every month from June to October. In June, July, and August, agents netted 50% more revenue than the same period last year. Since April, it has brought on 3,500 more agents to meet demand. Compass competitor Redfin found that luxury home sales, defined as homes in the top 5% in the market, are up 42% year over year in the third quarter of 2020, and showings, both virtual and in-person, have skyrocketed in recent months, up 64% year over year in September. The surge has been most pronounced at the highest end. â€œNew listings for what we define as affordable homes is up just 2.8% in Q3,â€ says Daryl Fairweather, chief economist at Redfin. â€œFor luxury, inventory is up 45%.â€ Now, rapidly expanding during a historic upswing in home buying, Compass is using its billion-dollar war chest to build on that momentum with new technology and its army of agents. Compass, first launched as an apartment rental site called Urban Compass in 2012 co-founded by Reffkin and serial tech entrepreneur Ori Allon, is aiming to build a brokerage with the valuation of a tech giant. With the SoftBank backing and massive valuation, there have also been no shortage of Compass comparisons to WeWork. While well-funded upstarts taking aim at large established industries is nothing new, few asset classes boast the value of U.S. homes, worth more than $30 trillion dollars. Compassâ€™ strategy has been to develop what its chief technology officer Joseph Sirosh told Marker is an â€œoperating systemâ€ for the antiquated real estate industry. Things like bespoke customer relationship management tools that use predictive A.I. to tell agents who and when to target; computer vision and machine learning to examine pictures of your home and tell you what upgrades you need to make to increase the sales value; and a slick smartphone app that lets an agent create customized video clips for social media to help one client sell their home, send a bottle of champagne to another happy customer, and generate and send itineraries for tomorrowâ€™s home tour, all in minutes. The aspiration is that with every agent interaction tracked, it will eventually be able to recommend which strategies work best. â€œThe nature of our platform is that itâ€™s like Shopify or Salesforce,â€ says Sirosh. The narrative of the tech-first disruptor is a well-tread path. Salad giant Sweetgreen, which has raised nearly half a billion dollars, has shown the value of casting itself as a technology firm that just happens to sell lettuce. While Compass is currently valued at $6.4 billion, itâ€™s been met with skepticism, often withering, especially from established industry players. Its valuation is 10 times that of Realogy, the giant conglomerate of marquee firms such as Sothebyâ€™s and Coldwell Banker that dominates the U.S. real estate market and has almost six times more annual revenue and 20 times more agents. When asked by business journalist Andrew Ross Sorkin why it appears that his company is actually seeing growth from rolling up small and large real estate firms as opposed to tech investment, Reffkin responded with the pithy â€œIs Amazon a retailer or tech company? Is Uber a transportation or tech company?â€ With the SoftBank backing and massive valuation, there have also been no shortage of Compass comparisons to WeWork. (While Reffkin and Allon declined to speak to Marker, Compass fiercely resists those comparisons, underscoring that unlike the co-working giant, it has no debt.) According to The Real Deal, Compass has acquired more than a dozen brokerages, including a San Francisco-based firm with $14 billion in annual sales. It expanded from 37 to 122 markets in 2018, reportedly seducing brokers with unsustainably high commissions. A real estate executive in New York City who doesnâ€™t compete with the firm says the startupâ€™s greatest strength isnâ€™t technology â€” but good old-fashioned brute force. â€œTheyâ€™re a disruptor by capital, not innovation,â€ he says. â€œItâ€™s amazing itâ€™s gotten this far. Theyâ€™re a brokerage that doesnâ€™t offer anything different; theyâ€™re just better at selling an idea.â€ Real estate has largely defined the history of America, from land grabs and westward expansion to the growth of cities, suburban sprawl, and todayâ€™s McMansions. But the actual realty profession is little more than 100 years old, born, according to Jeffrey Hornstein, author of A Nation of Realtors, of the early 20th century progressive-era drive to encourage entrepreneurship and escape the yoke of corporate servitude. Beginning in the late 19th century, real estate salesmen created professional groups as a reaction to the dubiousness of the then relatively unregulated career path. Prospective sales agents, nicknamed curbstoners, would compete by placing multiple signs and placards in front of a for-sale property brimming with modern conveniences, the public left hoping to choose an agent with scruples. It was an â€œopen listingâ€ market filled with speculators; a seller could work with as many agents as they wanted to, only paying the one who brought them a buyer. Realtors saw professionalization as a route to more sales and less consumer skepticism, and pushed a narrative of moral salesmanship to the public. The home, according to Hornstein, was fast becoming a focal point of consumerism, and consumers needed expert guidance to find the right one. To help combat the reputation of agents as bamboozling sharks, realtors pushed for state licensing requirements and formed multiple listing services; sellers would offer agents exclusive rights to their property and pay them commission for sales. FedEx Gears Up for All-Out War With Amazon The pandemic turbo-charged e-commerce â€” now the two giants are battling over the future of shipping marker.medium.com Throughout the economic booms and busts, the hot market of the 1920s, and the vast post-WWII homebuilding spree, realtors would continue evolving their business practices and organizational structure, taking advantage of the governmentâ€™s explicit backing of homeownership as an economic good (one that was, and still is, severely restricted for people of color). Before the Great Depression, a down payment for a home may have been as much as half the homeâ€™s value. But the New Deal and postwar boom in government loan programs, revolution in credit availability, and dramatic increase in supply due to suburbanization and a booming economy meant that consumers saw their paychecks rise while home prices stayed relatively flat between 1950 to 1970. By 1928, one in every 80 Californians had a real estate license. Over the past 50 years, the two biggest factors upending real estate was the cultural obsession with it â€” and the internet. While marketing methods would change, a formalized industry began to take shape. In 1925, a broker in Fort Wayne, Indiana, had a â€œbrand-new sales ideaâ€ to show completely furnished homes. In the â€™30s and â€™40s agents created sales networks so they could show prospective buyers multiple options. (Before, it was typically one agent moored to a single home.) In 1952, a realtor in Dallas began using the model house concept, to sell the future vision of a home. Century 21, founded by a pair of Orange County agents in 1971, set out to create the franchise â€œMcDonaldâ€™s of real estateâ€ model. Alongside firms like Coldwell Banker, initially founded in 1906 in San Francisco, Century 21 would rapidly expand in the â€™70s and â€™80s, corporatizing and scaling the franchise model, comprised of independent contractors as agents, to improve sales and salesmanship. In the â€™80s, brokers would provide access to MLS books, huge, telephone-directory-like collections of local listings agents would thumb through for listings, often making photocopies to take into the field. Over the past 50 years, the two biggest factors upending real estate have been our increasing cultural obsession with it â€” and the internet. Beginning in 1999, Home and Garden Television began airing House Hunters, the first in a long series of shows featuring what writer Kate Wagner, who founded McMansion Hell, called â€œsledgehammer-driven makeovers,â€ and the underlying idea that smart renovations can inflate values and transform property. (The home-flipping boom would soon follow.) Zillow, which launched in 2006, became a portal of real estate information and an aspirational time-suck for millions, one in a wave of websites and services that would democratize access to real estate data. Where realtors previously were experts with rarified knowledge and insider information, by the 2010s, they were mostly engaged in customer service and facilitation, less oracles and more operators and advisers connecting buyers and sellers (though they still make relatively the same commission amount they did decades ago, according to a 2019 Brookings study, a nice cut when average home prices have skyrocketed). During a 2006 interview, Zillow founder Richard Barton said, â€œRealtors currently sit at the middle of the transaction. I think in the future they will sit more on the outside offering specific services.â€ But the process â€” endless paperwork, applying for mortgages, working with inspectors and escrow, and, yes, the old-fashioned signs â€” make it seem behind the times, lending a car salesman vibe. This sidelining has only been compounded by what real estate tech investor Clelia Peters has coined â€œthe white T-shirt problemâ€ â€” a consumer is more heavily tracked and analyzed, and experiences a more technologically savvy checkout process, when they purchase a plain shirt, she argues, than when they make the most expensive and potentially consequential purchase of their lives: a home. And then thereâ€™s the diminishing reputation of the profession. Top residential real estate brokers are skilled professionals, juggling million-dollar-plus deals in certain markets. But the process â€” endless paperwork, applying for mortgages, working with inspectors and escrow, and, yes, the old-fashioned signs â€” make it seem behind the times, lending a car salesman vibe. It makes sense that in 2020, young brokers, given the choice between an old-school established real estate firm and a hot tech company, will probably choose the latter. All of this made the industry a prime target for Ori Allon, an Israeli-Australian serial entrepreneur and computer scientist. Allon had sold companies heâ€™d founded to both Google and Twitter, each boasting proprietary technology that became core parts of how both tech giants operated. (In 2005, when he was 25 pitching to Google in San Francisco, he started by saying, â€œIâ€™ll show you the future of the world of search.â€) He used some of the proceeds to buy his hometown basketball team â€” Hapoel Jerusalem â€” a perennial also-ran, and turn it into a championship contender. Along with Reffkin, a former chief of staff for the president of Goldman Sachs and White House Fellow who ran nearly 50 marathons, who he met during dinner at an American Academy of Achievement conference, Allon launched Urban Compass in 2012. Allon initially saw the real potential in real estate in the data. There are more than 600 versions of the Multiple Listing Service, the shared data platform that lists homes for sale, across the country. Allon believed that by bringing all these data sources under one roof, creating a system that could parse and analyze, seeing trends faster than a human agent, he could create a more efficient, and ultimately more lucrative, sales process. In 2014, he told the New York Times that he loves to â€œfix things with technology, and real estate needed to be fixed.â€ It was a challenge â€œway more interesting to me than what Iâ€™ve done in the past,â€ and his target demographic was â€œevery person that can operate an iPhone or website.â€ Allonâ€™s vision for a real estate tech company did tap into a colossal problem faced by the industry. The work of agents is very inefficient. As M. Ryan Gorman, CEO of Coldwell Bankers (part of Realogy), puts it, they are the â€œquarterback of every transaction,â€ and responsible for so much marketing and prospecting work that happens out of the eyes of a consumer. Coordinating with escrow companies, prepping homes for showings, creating marketing material, scheduling tours, and even setting up renovations can add hours of work before factoring in face-to-face conversations with clients. But Gorman says both consumers and agents had been resistant to taking the human touch out of the largest transaction most of them will ever make. In 2018, the firm, flush with SoftBank funding, launched a massive acquisition campaign in cities across the country, poaching agents with the goal of grabbing 20% of 20 markets by 2020. Other tech companies had already digitized pieces of the process. Whereas Zillow made home values more accessible and transparent, and Redfin created a digital-first brokerage, Compass, bolstered by technology, would aim to make the sales process more efficient. To get there, Allon and Reffkin swung in a bunch of directions. Initially, the firm touted its agent-side technology. Then, in 2016, it launched an app for consumers that provided agents and buyers and sellers constantly updated market information. The company would continue to ping-pong back and forth between being an agent-focused and consumer-centric platform, eventually settling on trying to be all things to all people. Now, according to Rory Golod, president of Compassâ€™ New York region, the platform is â€œB-to-B-to-Câ€ (business to business to consumer), with the agent at the center. Compass began with an $8 million seed round in 2012, with investments from Goldman Sachs and Founders Fund, among others, and by 2016, had raised $208 million with a valuation of $1 billion. In 2018, the firm, flush with SoftBank funding, launched a massive acquisition campaign in cities across the country, poaching agents with the goal of grabbing 20% of 20 markets by 2020. Candy Evans, a Dallas-based publisher who runs a local real estate news site CandysDirt, says that â€œhigh-end brokers were quaking in their boots, and everyone perked upâ€ when Compass came to town. Agents tend to prefer firms with more expensive property and better commission splits, which enable them to make more money on each sale. This was especially important amid increased competition: The National Association of Realtors found that while the number of brokers nearly doubled from 760,000 in 2000 to 1,359,000 in 2018, the number of total transactions actually dropped, from 5.99 million to 5.96 million. Across the country, agents spoke about getting generous fee splits from Compass, or signing bonuses some rumored to be in the seven figures. Compass said they do not poach; Realogy, which has a pending lawsuit in New York accusing the company of just such an action, disagrees. Matt Spangler, Compassâ€™ chief marketing solutions officer, says Compassâ€™ retention rate is the best in the industry, and theyâ€™re â€œnot paying people any more money than anybody else.â€ (â€œYouâ€™re not poaching anyone; youâ€™re attracting someone,â€ he clarifies.) Investor Peters, who sits on the board of trustees of Side, a VC-backed brokerage in San Francisco, says that an explicit part of their strategy is to lock up as many good agents as they can for as long as they can, so the ecosystem of other options for them will be limited. â€œIs that innovation?â€ she says. â€œTo me, that seems more scorched earth.â€ But agents like Evans say that while Compassâ€™ aggressive splits and signing bonuses work in the moment, agents are mobile by nature, and when one- or two-year contracts are up, they often return to their old firms. She believes the spree of acquisitions, bonuses, and favorable terms is about market share, more than anything else. â€œYou can keep your revenue split at 90 or 95%, what agent wouldnâ€™t want to go there?â€ she says. In Dallas, Compass took on legacy firms such as Briggs Freeman and Ebby Halliday, and now, according to Evans, are on nearly equal footing. â€œThey did find the agents,â€ she says, â€œbut hereâ€™s the rub: How are you going to turn a profit when youâ€™ve given them almost all of their commission? They have a beautiful office. Their marketing has expensive signs. But how are you going to turn a profit?â€ At the end of 2018, Compass made a splashy hire with its new CTO, Sirosh, Microsoftâ€™s former CTO of artificial intelligence who had also created a fraud detection system for Amazon. Heâ€™s helped introduce Compass Lens, which determines which upgrades will raise the sale price the most, and the custom CRM software that tells agents when itâ€™s better to get in touch with contacts. Heâ€™s opened up Compass tech campuses in Seattle and India and hired hundreds of coders and A.I. experts to help improve Compassâ€™ technology, all while the company has gone on an acquisition tear, buying up Contactually (customer management), Detectica (A.I.), and Modus (digital title and escrow services, core parts of a real estate transaction). Over Zoom in mid-October, Sirosh explained that Compassâ€™ technology could handle all the grunt work, with agents doing more transactions with less cost, and reach more customers with Amazon-level service quality. As co-founder Allon â€” who has no day-to-day role at Compass, but currently serves as executive chairman â€” once said, â€œbringing the science to what has for too long been only an art.â€ Tony Accardo has been a realtor for the last 12 years, working the Los Angeles beachfront and Palos Verdes. He saw Compass as the only player thatâ€™s â€œlooking forward, not backwardsâ€ in an often stodgy business. When the company came to L.A., he saw them growing at such a fast pace, he told his wife, â€œIf I donâ€™t do this, Iâ€™m going to miss the train.â€ He joined in 2018 and now 90% of his day is spent behind the Compass dashboard, looking at market data, seeing which properties his clients click on. Itâ€™s a â€œportal that provides everything, cohesively branded and well thought out.â€ He says it makes him much more efficient with his time, and his 2020 sales are triple what they were last year. Victor Lund, a real estate tech consultant, says that as Compass continually adds more data points to better understand the market and its clients, itâ€™ll create a longer-lasting relationship with the high-end buyers and sellers making up an increasingly large part of the market. â€œIn terms of raving fans, Compass agents we speak to often refer to their CRM and marketing tools as the best they have encountered,â€ he says. â€œI tend to agree. Compass has already passed their peers who have fumbled quite significantly in the core tools provided to agents, but the next iterations of Compass tech that leverages data as an asset to improve agent effectiveness and client services will reveal an entirely new landscape for the industry.â€ Austin Was Destined to Replace Silicon Valley. Then the Pandemic Hit Can the hottest boomtown off the coasts survive a recessionâ€”and a Covid surge? marker.medium.com Lund believes other tech solutions for real estate â€” Keller Williamsâ€™ A.I.-powered virtual assistant Kelle, for example, which is focused on teams and associates, and various iBuyer options, which use an algorithm to place a competitive bid on a property â€” just arenâ€™t as focused, and loyal, to the agent as the one being built by Compass. But when pressed for specifics and stats to confirm the effectiveness of its technology, Sirosh wonâ€™t go beyond vague. When asked if he had data on how the agent tools impacted sales and performance, he noted that the top third of agents are growing at amazing rates year over year, but â€œitâ€™s hard to say whether itâ€™s correlation or causation, but either way, itâ€™s a good story.â€ Could he provide more details, or hard numbers or statistics that proved the efficacy of Compass technology? Sirosh suggested the company was growing too fast, and changing so quickly, it was hard to quantify. â€œItâ€™s all good and positive and big numbers, but with hockey stick growth in the number of agents, thereâ€™s limited history around any specific thing. Meanwhile, a multilayered study by real estate tech expert Mike DelPrete from this spring showed that on an array of different measures, from production to transactions, Compass agents lagged behind the competition. If there were a ubiquitous symbol of real estate in need of a high-tech reimagining, Compass believed it was the for-sale sign sitting on someoneâ€™s front lawn. J Maggio, a top-producing agent with Conlon, a Chicago boutique firm that operated in the city and nearby suburbs, was courted by Compass in 2017, says he got the â€œhorse-and-pony show,â€ but turned them down, only to become a Compass agent in 2018 when they bought out his firm. â€œIâ€™ll say this as politely as I can: Compass had a cool software presentation, but none of it saved me time, made me extra money, or made my life easier,â€ he says. The tech he used until he left the firm in March of this year was worse than what heâ€™d used at other brokerages. â€œCompass has a young, swaggy vibe, but the tech wasnâ€™t fully baked for what I needed it to be.â€ If there were a ubiquitous symbol of real estate in need of a high-tech reimagining, Compass believed it was the for-sale sign sitting on someoneâ€™s front lawn. â€œThe real estate sign, for years, was very stagnant, a missed opportunity,â€ says Johan Liden, an industrial designer at Aruliden, a global design firm hired by Compass in 2017 for the makeover. â€œIt needed to be a true icon that people can interact with.â€ When Compass unveiled the reinvented sign in the summer of 2018, it seemed like the perfect totem of the rapidly expanding brokerage: A sleek, black extruding aluminum wand, backlit by LED lights and wired with an accelerometer, temperature sensor, and Bluetooth connectivity, it announced, like an upside-down exclamation point, that the house was a must-see â€” walk right up, scan a QR code, and take a virtual tour with your phone. Fast Company hailed Compass for the innovation, which was set to hit the front yards of its properties that fall. But it never made it past the first batch of 1,000 signs. Which was probably a good thing. â€œI thought a $1,000 digital sign was a waste of money,â€ says Maggio, the Chicago broker who left Compass this past March after two years. â€œThis was like the show Silicon Valley where an idea really doesnâ€™t need to be improved upon; itâ€™s for the wow factor.â€ As Compass tries to make the financial case for its technology-juiced business model â€” with the eventual goal of taking the company public, which Reffkin indicated in September â€” it needs to prove itâ€™s more than just flashy packaging. â€œWhatâ€™s happening now with the acquisitions and the scrambles is theyâ€™re attempting to reposition themselves as a tech company,â€ says investor Peters. â€œIs the market going to buy that story?â€ The first wave of post-pandemic home buying was more impulsive, with families jumping to the suburbs looking for more space. But as time goes on, and the pandemic continues, and decisions about remote work and lifestyles may harden, Redfinâ€™s Fairweather expects more and more buyers to make a move. She points to Sacramento as a key example; itâ€™s long been attractive to Bay Area residents looking for a cheaper, more spacious home, and saw a massive spike in high-end sales, 86% year over year across the metro area. As people look for more long-term housing solutions, she thinks smaller, secondary markets like this may be more popular, such as Portland, Oregon, or West Palm Beach, Florida. Brokers say the suburbs are on fire. In the New York region, the Hudson Valley, Long Island, Westchester, and the Hamptons are seeing homes disappear as soon as they go on the market. Maggio in Chicago says this luxury buying spree means a lot of the larger, older suburban homes â€” McMansion-like properties that may have gone out of style in the rush toward downtown â€” are suddenly in demand. â€œThis is as liquid as the market is going to be,â€ he says. â€œThereâ€™s a big push by agents to get people to sell because buyers are coming from the city, and homes need to be on the market now.â€ Meanwhile, Reffkin, who purchased his new home in New York City during the pandemic, is as bullish as ever about his cityâ€™s housing market. While Manhattanâ€™s office space continues to empty out â€” it hasnâ€™t had this much available since 2003 â€” Reffkin believes that as soon as the vaccine arrives, companies and workers will change their tune. Plus, he says Compassâ€™ elite slice of the market is continuing to boom â€” at least for now. â€œIn the $20 million-plus listing market weâ€™re seeing more activity driven by wealthy and savvy investors looking for opportunity, and in the sub-$2 million market, weâ€™re seeing a font of new first-time buyers being driven by record low interest rates,â€ he recently told CNN. â€œAnd they want to take advantage of it while it lasts.â€